{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2712436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_p(r, a):\n",
    "    one = r*torch.cos(a)+.25\n",
    "    two = r*torch.sin(a)\n",
    "    return torch.stack([one, two])\n",
    "\n",
    "def get_x(r, a, theta):\n",
    "    p_val = get_p(r, a)\n",
    "    new1 = -1*torch.sum(theta, -1).abs()/math.sqrt(2)\n",
    "    new2 = (-1*theta[..., 0]+theta[..., 1])/math.sqrt(2)\n",
    "    new = torch.stack([new1, new2])\n",
    "    return (p_val+new).T\n",
    "\n",
    "def generate_data(n_pts, return_theta=False):\n",
    "    prior = D.Uniform(torch.tensor([-1., -1.]), torch.tensor([1., 1.]))\n",
    "    a_dist = D.Uniform(-math.pi/2, math.pi/2)\n",
    "    r_dist = D.Normal(0.1, .01)\n",
    "\n",
    "    theta, a, r = prior.sample((n_pts,)), a_dist.sample((n_pts,)), r_dist.sample((n_pts,))\n",
    "    x = get_x(r, a, theta)\n",
    "    if return_theta: \n",
    "        return theta, x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca0e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknos.nflows import flows, transforms\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "from warnings import warn\n",
    "\n",
    "from pyknos.nflows import distributions as distributions_\n",
    "from pyknos.nflows import flows, transforms\n",
    "from pyknos.nflows.nn import nets\n",
    "from pyknos.nflows.transforms.splines import rational_quadratic\n",
    "from torch import Tensor, nn, relu, tanh, tensor, uint8\n",
    "\n",
    "from sbi.utils.sbiutils import (\n",
    "    standardizing_net,\n",
    "    standardizing_transform,\n",
    "    z_score_parser,\n",
    ")\n",
    "from sbi.utils.torchutils import create_alternating_binary_mask\n",
    "from sbi.utils.user_input_checks import check_data_device, check_embedding_net_device\n",
    "\n",
    "class ContextSplineMap(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network from `context` to the spline parameters.\n",
    "    We cannot use the resnet as conditioner to learn each dimension conditioned\n",
    "    on the other dimensions (because there is only one). Instead, we learn the\n",
    "    spline parameters directly. In the case of conditinal density estimation,\n",
    "    we make the spline parameters conditional on the context. This is\n",
    "    implemented in this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_features: int,\n",
    "        context_features: int,\n",
    "        hidden_layers: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize neural network that learns to predict spline parameters.\n",
    "        Args:\n",
    "            in_features: Unused since there is no `conditioner` in 1D.\n",
    "            out_features: Number of spline parameters.\n",
    "            hidden_features: Number of hidden units.\n",
    "            context_features: Number of context features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # `self.hidden_features` is only defined such that nflows can infer\n",
    "        # a scaling factor for initializations.\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        # Use a non-linearity because otherwise, there will be a linear\n",
    "        # mapping from context features onto distribution parameters.\n",
    "\n",
    "        # Initialize with input layer.\n",
    "        layer_list = [nn.Linear(context_features, hidden_features), nn.ReLU()]\n",
    "        # Add hidden layers.\n",
    "        layer_list += [\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "        ] * hidden_layers\n",
    "        # Add output layer.\n",
    "        layer_list += [nn.Linear(hidden_features, out_features)]\n",
    "        self.spline_predictor = nn.Sequential(*layer_list)\n",
    "\n",
    "    def __call__(self, inputs: Tensor, context: Tensor, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Return parameters of the spline given the context.\n",
    "        Args:\n",
    "            inputs: Unused. It would usually be the other dimensions, but in\n",
    "                1D, there are no other dimensions.\n",
    "            context: Context features.\n",
    "        Returns:\n",
    "            Spline parameters.\n",
    "        \"\"\"\n",
    "        return self.spline_predictor(context)\n",
    "\n",
    "# Declan: this code from SBI library\n",
    "def build_nsf(\n",
    "    batch_x: Tensor,\n",
    "    batch_y: Tensor,\n",
    "    z_score_x: Optional[str] = \"independent\",\n",
    "    z_score_y: Optional[str] = \"independent\",\n",
    "    hidden_features: int = 50,\n",
    "    num_transforms: int = 5,\n",
    "    num_bins: int = 10,\n",
    "    embedding_net: nn.Module = nn.Identity(),\n",
    "    tail_bound: float = 3.0,\n",
    "    hidden_layers_spline_context: int = 1,\n",
    "    num_blocks: int = 2,\n",
    "    dropout_probability: float = 0.0,\n",
    "    use_batch_norm: bool = False,\n",
    "    **kwargs,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Builds NSF p(x|y).\n",
    "    Args:\n",
    "        batch_x: Batch of xs, used to infer dimensionality and (optional) z-scoring.\n",
    "        batch_y: Batch of ys, used to infer dimensionality and (optional) z-scoring.\n",
    "        z_score_x: Whether to z-score xs passing into the network, can be one of:\n",
    "            - `none`, or None: do not z-score.\n",
    "            - `independent`: z-score each dimension independently.\n",
    "            - `structured`: treat dimensions as related, therefore compute mean and std\n",
    "            over the entire batch, instead of per-dimension. Should be used when each\n",
    "            sample is, for example, a time series or an image.\n",
    "        z_score_y: Whether to z-score ys passing into the network, same options as\n",
    "            z_score_x.\n",
    "        hidden_features: Number of hidden features.\n",
    "        num_transforms: Number of transforms.\n",
    "        num_bins: Number of bins used for the splines.\n",
    "        embedding_net: Optional embedding network for y.\n",
    "        tail_bound: tail bound for each spline.\n",
    "        hidden_layers_spline_context: number of hidden layers of the spline context net\n",
    "            for one-dimensional x.\n",
    "        num_blocks: number of blocks used for residual net for context embedding.\n",
    "        dropout_probability: dropout probability for regularization in residual net.\n",
    "        use_batch_norm: whether to use batch norm in residual net.\n",
    "        kwargs: Additional arguments that are passed by the build function but are not\n",
    "            relevant for maf and are therefore ignored.\n",
    "    Returns:\n",
    "        Neural network.\n",
    "    \"\"\"\n",
    "    x_numel = batch_x[0].numel()\n",
    "    # Infer the output dimensionality of the embedding_net by making a forward pass.\n",
    "    check_data_device(batch_x, batch_y)\n",
    "    check_embedding_net_device(embedding_net=embedding_net, datum=batch_y)\n",
    "    y_numel = embedding_net(batch_y[:1]).numel()\n",
    "\n",
    "    # Define mask function to alternate between predicted x-dimensions.\n",
    "    def mask_in_layer(i):\n",
    "        return create_alternating_binary_mask(features=x_numel, even=(i % 2 == 0))\n",
    "\n",
    "    # If x is just a scalar then use a dummy mask and learn spline parameters using the\n",
    "    # conditioning variables only.\n",
    "    if x_numel == 1:\n",
    "        # Conditioner ignores the data and uses the conditioning variables only.\n",
    "        conditioner = partial(\n",
    "            ContextSplineMap,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=y_numel,\n",
    "            hidden_layers=hidden_layers_spline_context,\n",
    "        )\n",
    "    else:\n",
    "        # Use conditional resnet as spline conditioner.\n",
    "        conditioner = partial(\n",
    "            nets.ResidualNet,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=y_numel,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "        )\n",
    "\n",
    "    # Stack spline transforms.\n",
    "    transform_list = []\n",
    "    for i in range(num_transforms):\n",
    "        block = [\n",
    "            transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "                mask=mask_in_layer(i) if x_numel > 1 else tensor([1], dtype=uint8),\n",
    "                transform_net_create_fn=conditioner,\n",
    "                num_bins=num_bins,\n",
    "                tails=\"linear\",\n",
    "                tail_bound=tail_bound,\n",
    "                apply_unconditional_transform=False,\n",
    "            )\n",
    "        ]\n",
    "        # Add LU transform only for high D x. Permutation makes sense only for more than\n",
    "        # one feature.\n",
    "        if x_numel > 1:\n",
    "            block.append(\n",
    "                transforms.LULinear(x_numel, identity_init=True),\n",
    "            )\n",
    "        transform_list += block\n",
    "\n",
    "    z_score_x_bool, structured_x = z_score_parser(z_score_x)\n",
    "    if z_score_x_bool:\n",
    "        # Prepend standardizing transform to nsf transforms.\n",
    "        transform_list = [\n",
    "            standardizing_transform(batch_x, structured_x)\n",
    "        ] + transform_list\n",
    "\n",
    "    z_score_y_bool, structured_y = z_score_parser(z_score_y)\n",
    "    if z_score_y_bool:\n",
    "        # Prepend standardizing transform to y-embedding.\n",
    "        embedding_net = nn.Sequential(\n",
    "            standardizing_net(batch_y, structured_y), embedding_net\n",
    "        )\n",
    "\n",
    "    distribution = distributions_.StandardNormal((x_numel,))\n",
    "\n",
    "    # Combine transforms.\n",
    "    transform = transforms.CompositeTransform(transform_list)\n",
    "    neural_net = flows.Flow(transform, distribution, embedding_net)\n",
    "\n",
    "    return neural_net\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.context_dim = dim\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Assumes context x is of shape (batch_size, self.context_dim)\n",
    "        '''\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a406aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, x = generate_data(100, return_theta=True) \n",
    "\n",
    "prior = D.Uniform(torch.tensor([-1., -1.]), torch.tensor([1., 1.]))\n",
    "a_dist = D.Uniform(-math.pi/2, math.pi/2)\n",
    "r_dist = D.Normal(0.1, .01)\n",
    "mb_size=100\n",
    "device='cpu'\n",
    "kwargs = {'prior': prior,\n",
    "        'a_dist': a_dist,\n",
    "        'r_dist': r_dist}\n",
    "\n",
    "# EXAMPLE BATCH FOR SHAPES\n",
    "z_dim = prior.sample().shape[-1]\n",
    "x_dim = x.shape[-1]\n",
    "num_obs_flow = mb_size\n",
    "fake_zs = torch.randn((mb_size, z_dim))\n",
    "fake_xs = torch.randn((mb_size, x_dim))\n",
    "encoder = build_nsf(fake_zs, fake_xs, z_score_x='none', z_score_y='none')\n",
    "\n",
    "encoder.to(device)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a725616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 2.227240800857544\n",
      "Iteration 1: loss 2.3150510787963867\n",
      "Iteration 2: loss 2.0759389400482178\n",
      "Iteration 3: loss 2.197991132736206\n",
      "Iteration 4: loss 2.063671350479126\n",
      "Iteration 5: loss 2.077873945236206\n",
      "Iteration 6: loss 2.072089433670044\n",
      "Iteration 7: loss 1.9783339500427246\n",
      "Iteration 8: loss 1.9403975009918213\n",
      "Iteration 9: loss 1.9125277996063232\n",
      "Iteration 10: loss 1.9253953695297241\n",
      "Iteration 11: loss 1.853196620941162\n",
      "Iteration 12: loss 1.7862945795059204\n",
      "Iteration 13: loss 1.8069168329238892\n",
      "Iteration 14: loss 1.6985688209533691\n",
      "Iteration 15: loss 1.6327482461929321\n",
      "Iteration 16: loss 1.7114253044128418\n",
      "Iteration 17: loss 1.635581612586975\n",
      "Iteration 18: loss 1.595872402191162\n",
      "Iteration 19: loss 1.582434058189392\n",
      "Iteration 20: loss 1.528934359550476\n",
      "Iteration 21: loss 1.535510540008545\n",
      "Iteration 22: loss 1.4221372604370117\n",
      "Iteration 23: loss 1.3233698606491089\n",
      "Iteration 24: loss 1.4563674926757812\n",
      "Iteration 25: loss 1.3338828086853027\n",
      "Iteration 26: loss 1.2140458822250366\n",
      "Iteration 27: loss 1.238122582435608\n",
      "Iteration 28: loss 1.1345481872558594\n",
      "Iteration 29: loss 1.1579787731170654\n",
      "Iteration 30: loss 1.0768624544143677\n",
      "Iteration 31: loss 1.0731956958770752\n",
      "Iteration 32: loss 1.0802119970321655\n",
      "Iteration 33: loss 0.9272070527076721\n",
      "Iteration 34: loss 0.9253929257392883\n",
      "Iteration 35: loss 0.8299822807312012\n",
      "Iteration 36: loss 0.8357316851615906\n",
      "Iteration 37: loss 0.7745870351791382\n",
      "Iteration 38: loss 0.7046991586685181\n",
      "Iteration 39: loss 0.6174219250679016\n",
      "Iteration 40: loss 0.4841609299182892\n",
      "Iteration 41: loss 0.48449063301086426\n",
      "Iteration 42: loss 0.3367502689361572\n",
      "Iteration 43: loss 0.28645268082618713\n",
      "Iteration 44: loss 0.28421857953071594\n",
      "Iteration 45: loss 0.2652599811553955\n",
      "Iteration 46: loss 0.14367833733558655\n",
      "Iteration 47: loss 0.01949789933860302\n",
      "Iteration 48: loss -0.08321315795183182\n",
      "Iteration 49: loss -0.1363079994916916\n",
      "Iteration 50: loss -0.02523508109152317\n",
      "Iteration 51: loss -0.17972692847251892\n",
      "Iteration 52: loss -0.22179242968559265\n",
      "Iteration 53: loss -0.20575419068336487\n",
      "Iteration 54: loss -0.2923637330532074\n",
      "Iteration 55: loss -0.29288679361343384\n",
      "Iteration 56: loss -0.44186729192733765\n",
      "Iteration 57: loss -0.241225928068161\n",
      "Iteration 58: loss -0.3298379182815552\n",
      "Iteration 59: loss -0.5186078548431396\n",
      "Iteration 60: loss -0.4184436798095703\n",
      "Iteration 61: loss -0.3252812325954437\n",
      "Iteration 62: loss -0.48655104637145996\n",
      "Iteration 63: loss -0.2964807450771332\n",
      "Iteration 64: loss -0.3244762420654297\n",
      "Iteration 65: loss -0.503330409526825\n",
      "Iteration 66: loss -0.37911370396614075\n",
      "Iteration 67: loss -0.35065189003944397\n",
      "Iteration 68: loss -0.5302554965019226\n",
      "Iteration 69: loss -0.4495243430137634\n",
      "Iteration 70: loss -0.45412251353263855\n",
      "Iteration 71: loss -0.482186883687973\n",
      "Iteration 72: loss -0.5115004181861877\n",
      "Iteration 73: loss -0.3952327072620392\n",
      "Iteration 74: loss -0.4166739583015442\n",
      "Iteration 75: loss -0.45907461643218994\n",
      "Iteration 76: loss -0.6181306838989258\n",
      "Iteration 77: loss -0.6589080691337585\n",
      "Iteration 78: loss -0.7623907327651978\n",
      "Iteration 79: loss -0.5165489315986633\n",
      "Iteration 80: loss -0.569644570350647\n",
      "Iteration 81: loss -0.6871013641357422\n",
      "Iteration 82: loss -0.6552202105522156\n",
      "Iteration 83: loss -0.4294760823249817\n",
      "Iteration 84: loss -0.6291954517364502\n",
      "Iteration 85: loss -0.6426438689231873\n",
      "Iteration 86: loss -0.5465077757835388\n",
      "Iteration 87: loss -0.620380163192749\n",
      "Iteration 88: loss -0.6731317639350891\n",
      "Iteration 89: loss -0.7901761531829834\n",
      "Iteration 90: loss -0.5775578022003174\n",
      "Iteration 91: loss -0.742317795753479\n",
      "Iteration 92: loss -0.7941709756851196\n",
      "Iteration 93: loss -0.8255590200424194\n",
      "Iteration 94: loss -0.7576732635498047\n",
      "Iteration 95: loss -0.7675116658210754\n",
      "Iteration 96: loss -0.7787989974021912\n",
      "Iteration 97: loss -0.845493495464325\n",
      "Iteration 98: loss -0.7466146349906921\n",
      "Iteration 99: loss -0.7080742120742798\n",
      "Iteration 100: loss -0.8355679512023926\n",
      "Iteration 101: loss -0.8829449415206909\n",
      "Iteration 102: loss -0.9579293727874756\n",
      "Iteration 103: loss -0.921623706817627\n",
      "Iteration 104: loss -0.9156407713890076\n",
      "Iteration 105: loss -1.0075217485427856\n",
      "Iteration 106: loss -0.8847882151603699\n",
      "Iteration 107: loss -0.9414887428283691\n",
      "Iteration 108: loss -0.9941326379776001\n",
      "Iteration 109: loss -1.0958629846572876\n",
      "Iteration 110: loss -1.055262565612793\n",
      "Iteration 111: loss -1.0698102712631226\n",
      "Iteration 112: loss -0.9676597118377686\n",
      "Iteration 113: loss -1.0659899711608887\n",
      "Iteration 114: loss -1.0226428508758545\n",
      "Iteration 115: loss -1.267411231994629\n",
      "Iteration 116: loss -1.0879974365234375\n",
      "Iteration 117: loss -0.9724717736244202\n",
      "Iteration 118: loss -1.0587856769561768\n",
      "Iteration 119: loss -1.207323431968689\n",
      "Iteration 120: loss -1.1177563667297363\n",
      "Iteration 121: loss -1.1699323654174805\n",
      "Iteration 122: loss -1.0772534608840942\n",
      "Iteration 123: loss -1.1338618993759155\n",
      "Iteration 124: loss -1.1733070611953735\n",
      "Iteration 125: loss -1.1193499565124512\n",
      "Iteration 126: loss -1.062464714050293\n",
      "Iteration 127: loss -1.09110426902771\n",
      "Iteration 128: loss -1.203094482421875\n",
      "Iteration 129: loss -1.0978562831878662\n",
      "Iteration 130: loss -1.327438235282898\n",
      "Iteration 131: loss -1.209700107574463\n",
      "Iteration 132: loss -1.1503263711929321\n",
      "Iteration 133: loss -1.1054261922836304\n",
      "Iteration 134: loss -1.2754019498825073\n",
      "Iteration 135: loss -1.2825514078140259\n",
      "Iteration 136: loss -1.3862701654434204\n",
      "Iteration 137: loss -1.3786189556121826\n",
      "Iteration 138: loss -1.1667981147766113\n",
      "Iteration 139: loss -1.3783962726593018\n",
      "Iteration 140: loss -1.2524694204330444\n",
      "Iteration 141: loss -1.26279878616333\n",
      "Iteration 142: loss -1.3384408950805664\n",
      "Iteration 143: loss -1.273989200592041\n",
      "Iteration 144: loss -1.3096554279327393\n",
      "Iteration 145: loss -1.332555890083313\n",
      "Iteration 146: loss -1.2190887928009033\n",
      "Iteration 147: loss -1.2463020086288452\n",
      "Iteration 148: loss -1.2885212898254395\n",
      "Iteration 149: loss -1.196770191192627\n",
      "Iteration 150: loss -1.3546512126922607\n",
      "Iteration 151: loss -1.3115477561950684\n",
      "Iteration 152: loss -1.3000489473342896\n",
      "Iteration 153: loss -1.370534062385559\n",
      "Iteration 154: loss -1.4005168676376343\n",
      "Iteration 155: loss -1.3467531204223633\n",
      "Iteration 156: loss -1.2836912870407104\n",
      "Iteration 157: loss -1.1734631061553955\n",
      "Iteration 158: loss -1.4370887279510498\n",
      "Iteration 159: loss -1.491726279258728\n",
      "Iteration 160: loss -1.5307371616363525\n",
      "Iteration 161: loss -1.4183049201965332\n",
      "Iteration 162: loss -1.397222638130188\n",
      "Iteration 163: loss -1.4071245193481445\n",
      "Iteration 164: loss -1.4832943677902222\n",
      "Iteration 165: loss -1.499688744544983\n",
      "Iteration 166: loss -1.531336784362793\n",
      "Iteration 167: loss -1.4690542221069336\n",
      "Iteration 168: loss -1.5591462850570679\n",
      "Iteration 169: loss -1.594658613204956\n",
      "Iteration 170: loss -1.653204321861267\n",
      "Iteration 171: loss -1.5830525159835815\n",
      "Iteration 172: loss -1.6304008960723877\n",
      "Iteration 173: loss -1.5484577417373657\n",
      "Iteration 174: loss -1.4497298002243042\n",
      "Iteration 175: loss -1.4840712547302246\n",
      "Iteration 176: loss -1.548689842224121\n",
      "Iteration 177: loss -1.6336289644241333\n",
      "Iteration 178: loss -1.6874467134475708\n",
      "Iteration 179: loss -1.8013073205947876\n",
      "Iteration 180: loss -1.5322473049163818\n",
      "Iteration 181: loss -1.62468683719635\n",
      "Iteration 182: loss -1.5889620780944824\n",
      "Iteration 183: loss -1.6372030973434448\n",
      "Iteration 184: loss -1.6761724948883057\n",
      "Iteration 185: loss -1.5377089977264404\n",
      "Iteration 186: loss -1.3457658290863037\n",
      "Iteration 187: loss -1.6408915519714355\n",
      "Iteration 188: loss -1.4056254625320435\n",
      "Iteration 189: loss -1.5386841297149658\n",
      "Iteration 190: loss -1.5835973024368286\n",
      "Iteration 191: loss -1.7908384799957275\n",
      "Iteration 192: loss -1.640328049659729\n",
      "Iteration 193: loss -1.655673861503601\n",
      "Iteration 194: loss -1.701967477798462\n",
      "Iteration 195: loss -1.7022916078567505\n",
      "Iteration 196: loss -1.689939260482788\n",
      "Iteration 197: loss -1.6518152952194214\n",
      "Iteration 198: loss -1.7580569982528687\n",
      "Iteration 199: loss -1.7615649700164795\n",
      "Iteration 200: loss -1.8313543796539307\n",
      "Iteration 201: loss -1.804653525352478\n",
      "Iteration 202: loss -1.7534534931182861\n",
      "Iteration 203: loss -1.8440842628479004\n",
      "Iteration 204: loss -1.8642734289169312\n",
      "Iteration 205: loss -1.745862603187561\n",
      "Iteration 206: loss -1.7389209270477295\n",
      "Iteration 207: loss -1.7715363502502441\n",
      "Iteration 208: loss -1.8416557312011719\n",
      "Iteration 209: loss -1.7244311571121216\n",
      "Iteration 210: loss -1.866043210029602\n",
      "Iteration 211: loss -1.7940163612365723\n",
      "Iteration 212: loss -1.687320590019226\n",
      "Iteration 213: loss -1.8976130485534668\n",
      "Iteration 214: loss -1.759352684020996\n",
      "Iteration 215: loss -2.0072667598724365\n",
      "Iteration 216: loss -1.7745290994644165\n",
      "Iteration 217: loss -1.8555182218551636\n",
      "Iteration 218: loss -1.8719860315322876\n",
      "Iteration 219: loss -1.8361483812332153\n",
      "Iteration 220: loss -1.8537641763687134\n",
      "Iteration 221: loss -1.873130202293396\n",
      "Iteration 222: loss -1.9463601112365723\n",
      "Iteration 223: loss -2.051438093185425\n",
      "Iteration 224: loss -1.7638229131698608\n",
      "Iteration 225: loss -1.946040153503418\n",
      "Iteration 226: loss -1.8754451274871826\n",
      "Iteration 227: loss -1.710543155670166\n",
      "Iteration 228: loss -1.6507090330123901\n",
      "Iteration 229: loss -1.8697224855422974\n",
      "Iteration 230: loss -2.0084142684936523\n",
      "Iteration 231: loss -1.7953981161117554\n",
      "Iteration 232: loss -2.0311667919158936\n",
      "Iteration 233: loss -1.9605854749679565\n",
      "Iteration 234: loss -2.0259974002838135\n",
      "Iteration 235: loss -1.9180351495742798\n",
      "Iteration 236: loss -1.9123350381851196\n",
      "Iteration 237: loss -2.015437126159668\n",
      "Iteration 238: loss -1.9843477010726929\n",
      "Iteration 239: loss -2.0018486976623535\n",
      "Iteration 240: loss -1.9927868843078613\n",
      "Iteration 241: loss -2.0686655044555664\n",
      "Iteration 242: loss -1.8276660442352295\n",
      "Iteration 243: loss -1.7580736875534058\n",
      "Iteration 244: loss -1.8446139097213745\n",
      "Iteration 245: loss -1.7211428880691528\n",
      "Iteration 246: loss -1.8793989419937134\n",
      "Iteration 247: loss -1.7422279119491577\n",
      "Iteration 248: loss -2.0425021648406982\n",
      "Iteration 249: loss -2.085993528366089\n",
      "Iteration 250: loss -1.909703016281128\n",
      "Iteration 251: loss -1.840334415435791\n",
      "Iteration 252: loss -1.8913564682006836\n",
      "Iteration 253: loss -1.8755453824996948\n",
      "Iteration 254: loss -1.7559834718704224\n",
      "Iteration 255: loss -1.5742524862289429\n",
      "Iteration 256: loss -1.7403570413589478\n",
      "Iteration 257: loss -1.818628191947937\n",
      "Iteration 258: loss -2.1444475650787354\n",
      "Iteration 259: loss -2.104837656021118\n",
      "Iteration 260: loss -1.9576845169067383\n",
      "Iteration 261: loss -1.8513977527618408\n",
      "Iteration 262: loss -1.7013109922409058\n",
      "Iteration 263: loss -2.015126943588257\n",
      "Iteration 264: loss -1.5321877002716064\n",
      "Iteration 265: loss -1.7088990211486816\n",
      "Iteration 266: loss -1.5615735054016113\n",
      "Iteration 267: loss -1.6267584562301636\n",
      "Iteration 268: loss -1.7143325805664062\n",
      "Iteration 269: loss -1.7325439453125\n",
      "Iteration 270: loss -2.0848312377929688\n",
      "Iteration 271: loss -1.9127470254898071\n",
      "Iteration 272: loss -1.9151591062545776\n",
      "Iteration 273: loss -1.9739410877227783\n",
      "Iteration 274: loss -1.967820167541504\n",
      "Iteration 275: loss -2.171870470046997\n",
      "Iteration 276: loss -2.1727800369262695\n",
      "Iteration 277: loss -2.0929551124572754\n",
      "Iteration 278: loss -1.8969203233718872\n",
      "Iteration 279: loss -1.9947788715362549\n",
      "Iteration 280: loss -2.0057342052459717\n",
      "Iteration 281: loss -2.1413400173187256\n",
      "Iteration 282: loss -2.1317756175994873\n",
      "Iteration 283: loss -2.140254020690918\n",
      "Iteration 284: loss -2.0464401245117188\n",
      "Iteration 285: loss -2.0716304779052734\n",
      "Iteration 286: loss -2.1405515670776367\n",
      "Iteration 287: loss -2.027679204940796\n",
      "Iteration 288: loss -1.9535390138626099\n",
      "Iteration 289: loss -2.0458056926727295\n",
      "Iteration 290: loss -2.170923948287964\n",
      "Iteration 291: loss -2.00232195854187\n",
      "Iteration 292: loss -2.0655324459075928\n",
      "Iteration 293: loss -2.064354658126831\n",
      "Iteration 294: loss -1.97246515750885\n",
      "Iteration 295: loss -2.048212766647339\n",
      "Iteration 296: loss -2.1550159454345703\n",
      "Iteration 297: loss -2.052809476852417\n",
      "Iteration 298: loss -2.1510324478149414\n",
      "Iteration 299: loss -2.0558958053588867\n",
      "Iteration 300: loss -1.967785358428955\n",
      "Iteration 301: loss -2.221452236175537\n",
      "Iteration 302: loss -2.095160722732544\n",
      "Iteration 303: loss -2.087162971496582\n",
      "Iteration 304: loss -2.17972469329834\n",
      "Iteration 305: loss -2.1582610607147217\n",
      "Iteration 306: loss -2.121025800704956\n",
      "Iteration 307: loss -2.136056900024414\n",
      "Iteration 308: loss -2.133357286453247\n",
      "Iteration 309: loss -2.1749684810638428\n",
      "Iteration 310: loss -2.3313395977020264\n",
      "Iteration 311: loss -2.239861488342285\n",
      "Iteration 312: loss -1.8666576147079468\n",
      "Iteration 313: loss -2.1372036933898926\n",
      "Iteration 314: loss -2.246452569961548\n",
      "Iteration 315: loss -2.0991241931915283\n",
      "Iteration 316: loss -2.045775890350342\n",
      "Iteration 317: loss -1.9818472862243652\n",
      "Iteration 318: loss -2.1366958618164062\n",
      "Iteration 319: loss -2.236337900161743\n",
      "Iteration 320: loss -2.021636962890625\n",
      "Iteration 321: loss -2.263228416442871\n",
      "Iteration 322: loss -1.885502576828003\n",
      "Iteration 323: loss -2.2143709659576416\n",
      "Iteration 324: loss -1.9729846715927124\n",
      "Iteration 325: loss -1.8683781623840332\n",
      "Iteration 326: loss -1.7907311916351318\n",
      "Iteration 327: loss -2.025087356567383\n",
      "Iteration 328: loss -2.1415910720825195\n",
      "Iteration 329: loss -2.0486366748809814\n",
      "Iteration 330: loss -1.8983068466186523\n",
      "Iteration 331: loss -1.8016390800476074\n",
      "Iteration 332: loss -2.0301103591918945\n",
      "Iteration 333: loss -2.0659945011138916\n",
      "Iteration 334: loss -1.9672386646270752\n",
      "Iteration 335: loss -2.0154902935028076\n",
      "Iteration 336: loss -1.786195993423462\n",
      "Iteration 337: loss -1.5017088651657104\n",
      "Iteration 338: loss -2.010934352874756\n",
      "Iteration 339: loss -2.1969072818756104\n",
      "Iteration 340: loss -2.243483781814575\n",
      "Iteration 341: loss -2.1770524978637695\n",
      "Iteration 342: loss -2.245746374130249\n",
      "Iteration 343: loss -2.129101514816284\n",
      "Iteration 344: loss -2.2035915851593018\n",
      "Iteration 345: loss -2.1540236473083496\n",
      "Iteration 346: loss -2.1694092750549316\n",
      "Iteration 347: loss -1.9900262355804443\n",
      "Iteration 348: loss -2.205152988433838\n",
      "Iteration 349: loss -2.2251594066619873\n",
      "Iteration 350: loss -1.9341243505477905\n",
      "Iteration 351: loss -2.124802827835083\n",
      "Iteration 352: loss -2.0735416412353516\n",
      "Iteration 353: loss -1.956152081489563\n",
      "Iteration 354: loss -2.2413315773010254\n",
      "Iteration 355: loss -2.4267008304595947\n",
      "Iteration 356: loss -2.0299336910247803\n",
      "Iteration 357: loss -2.1235527992248535\n",
      "Iteration 358: loss -2.335507392883301\n",
      "Iteration 359: loss -2.192570686340332\n",
      "Iteration 360: loss -2.2648282051086426\n",
      "Iteration 361: loss -2.368176221847534\n",
      "Iteration 362: loss -2.0160186290740967\n",
      "Iteration 363: loss -2.3900279998779297\n",
      "Iteration 364: loss -2.074303150177002\n",
      "Iteration 365: loss -2.299057960510254\n",
      "Iteration 366: loss -2.2167863845825195\n",
      "Iteration 367: loss -1.902725338935852\n",
      "Iteration 368: loss -2.2984766960144043\n",
      "Iteration 369: loss -2.221996545791626\n",
      "Iteration 370: loss -2.292119026184082\n",
      "Iteration 371: loss -2.1457221508026123\n",
      "Iteration 372: loss -2.2763898372650146\n",
      "Iteration 373: loss -2.234992027282715\n",
      "Iteration 374: loss -2.361840009689331\n",
      "Iteration 375: loss -2.282038450241089\n",
      "Iteration 376: loss -2.305241584777832\n",
      "Iteration 377: loss -2.3358588218688965\n",
      "Iteration 378: loss -2.2301836013793945\n",
      "Iteration 379: loss -2.2484323978424072\n",
      "Iteration 380: loss -2.2203290462493896\n",
      "Iteration 381: loss -2.3397762775421143\n",
      "Iteration 382: loss -2.3316149711608887\n",
      "Iteration 383: loss -2.244986057281494\n",
      "Iteration 384: loss -2.420088291168213\n",
      "Iteration 385: loss -2.2082326412200928\n",
      "Iteration 386: loss -2.25285267829895\n",
      "Iteration 387: loss -2.1892220973968506\n",
      "Iteration 388: loss -2.3113741874694824\n",
      "Iteration 389: loss -2.395599603652954\n",
      "Iteration 390: loss -2.4645960330963135\n",
      "Iteration 391: loss -2.2045767307281494\n",
      "Iteration 392: loss -2.214517831802368\n",
      "Iteration 393: loss -2.1519782543182373\n",
      "Iteration 394: loss -2.4558660984039307\n",
      "Iteration 395: loss -2.300624370574951\n",
      "Iteration 396: loss -2.2516930103302\n",
      "Iteration 397: loss -2.3017489910125732\n",
      "Iteration 398: loss -2.3360230922698975\n",
      "Iteration 399: loss -2.254181385040283\n",
      "Iteration 400: loss -2.4400246143341064\n",
      "Iteration 401: loss -2.3014345169067383\n",
      "Iteration 402: loss -2.3687994480133057\n",
      "Iteration 403: loss -2.3737032413482666\n",
      "Iteration 404: loss -2.3681576251983643\n",
      "Iteration 405: loss -2.245572566986084\n",
      "Iteration 406: loss -2.5399630069732666\n",
      "Iteration 407: loss -2.298281669616699\n",
      "Iteration 408: loss -2.3754723072052\n",
      "Iteration 409: loss -2.467646837234497\n",
      "Iteration 410: loss -2.099820852279663\n",
      "Iteration 411: loss -2.507735252380371\n",
      "Iteration 412: loss -2.250049591064453\n",
      "Iteration 413: loss -2.4511024951934814\n",
      "Iteration 414: loss -2.3174538612365723\n",
      "Iteration 415: loss -2.0435330867767334\n",
      "Iteration 416: loss -2.3543105125427246\n",
      "Iteration 417: loss -1.9953315258026123\n",
      "Iteration 418: loss -2.1345949172973633\n",
      "Iteration 419: loss -2.369586706161499\n",
      "Iteration 420: loss -2.055889129638672\n",
      "Iteration 421: loss -2.1728029251098633\n",
      "Iteration 422: loss -2.226541519165039\n",
      "Iteration 423: loss -2.3527777194976807\n",
      "Iteration 424: loss -2.0679259300231934\n",
      "Iteration 425: loss -2.4873645305633545\n",
      "Iteration 426: loss -2.344547986984253\n",
      "Iteration 427: loss -2.543179988861084\n",
      "Iteration 428: loss -2.25981068611145\n",
      "Iteration 429: loss -2.3401918411254883\n",
      "Iteration 430: loss -2.469285488128662\n",
      "Iteration 431: loss -2.2670514583587646\n",
      "Iteration 432: loss -2.3770546913146973\n",
      "Iteration 433: loss -2.5060012340545654\n",
      "Iteration 434: loss -2.443591594696045\n",
      "Iteration 435: loss -2.3407254219055176\n",
      "Iteration 436: loss -2.4327502250671387\n",
      "Iteration 437: loss -2.4156060218811035\n",
      "Iteration 438: loss -2.42279052734375\n",
      "Iteration 439: loss -2.514187812805176\n",
      "Iteration 440: loss -2.471888780593872\n",
      "Iteration 441: loss -2.4026503562927246\n",
      "Iteration 442: loss -2.4191346168518066\n",
      "Iteration 443: loss -2.5132107734680176\n",
      "Iteration 444: loss -2.4628100395202637\n",
      "Iteration 445: loss -2.494688034057617\n",
      "Iteration 446: loss -2.451681613922119\n",
      "Iteration 447: loss -2.4076671600341797\n",
      "Iteration 448: loss -2.3680694103240967\n",
      "Iteration 449: loss -2.4389266967773438\n",
      "Iteration 450: loss -2.3795197010040283\n",
      "Iteration 451: loss -2.403308153152466\n",
      "Iteration 452: loss -2.547776460647583\n",
      "Iteration 453: loss -2.6158978939056396\n",
      "Iteration 454: loss -2.3326544761657715\n",
      "Iteration 455: loss -2.392953872680664\n",
      "Iteration 456: loss -2.420673131942749\n",
      "Iteration 457: loss -2.7013099193573\n",
      "Iteration 458: loss -2.575307846069336\n",
      "Iteration 459: loss -2.642688274383545\n",
      "Iteration 460: loss -2.459272861480713\n",
      "Iteration 461: loss -2.524005174636841\n",
      "Iteration 462: loss -2.5047202110290527\n",
      "Iteration 463: loss -2.4712772369384766\n",
      "Iteration 464: loss -2.569582939147949\n",
      "Iteration 465: loss -2.5983710289001465\n",
      "Iteration 466: loss -2.455458879470825\n",
      "Iteration 467: loss -2.61983060836792\n",
      "Iteration 468: loss -2.721752643585205\n",
      "Iteration 469: loss -2.7486824989318848\n",
      "Iteration 470: loss -2.471989870071411\n",
      "Iteration 471: loss -2.6423888206481934\n",
      "Iteration 472: loss -2.534595251083374\n",
      "Iteration 473: loss -2.617887020111084\n",
      "Iteration 474: loss -2.694077968597412\n",
      "Iteration 475: loss -2.496659517288208\n",
      "Iteration 476: loss -2.4296112060546875\n",
      "Iteration 477: loss -2.7357823848724365\n",
      "Iteration 478: loss -2.623199224472046\n",
      "Iteration 479: loss -2.55495548248291\n",
      "Iteration 480: loss -2.219510793685913\n",
      "Iteration 481: loss -2.2925620079040527\n",
      "Iteration 482: loss -2.0571846961975098\n",
      "Iteration 483: loss -2.284454107284546\n",
      "Iteration 484: loss -1.9924752712249756\n",
      "Iteration 485: loss -2.003171443939209\n",
      "Iteration 486: loss -2.249181032180786\n",
      "Iteration 487: loss -2.153456926345825\n",
      "Iteration 488: loss -2.2521698474884033\n",
      "Iteration 489: loss -2.331998825073242\n",
      "Iteration 490: loss -2.308795928955078\n",
      "Iteration 491: loss -2.4786388874053955\n",
      "Iteration 492: loss -2.6933953762054443\n",
      "Iteration 493: loss -2.474121332168579\n",
      "Iteration 494: loss -2.2289488315582275\n",
      "Iteration 495: loss -2.4852967262268066\n",
      "Iteration 496: loss -2.264355421066284\n",
      "Iteration 497: loss -2.3803887367248535\n",
      "Iteration 498: loss -2.1546597480773926\n",
      "Iteration 499: loss -2.3703112602233887\n",
      "Iteration 500: loss -2.429276943206787\n",
      "Iteration 501: loss -2.3260579109191895\n",
      "Iteration 502: loss -2.4797561168670654\n",
      "Iteration 503: loss -2.409397840499878\n",
      "Iteration 504: loss -2.470404624938965\n",
      "Iteration 505: loss -2.51604962348938\n",
      "Iteration 506: loss -2.472172260284424\n",
      "Iteration 507: loss -2.551111936569214\n",
      "Iteration 508: loss -2.7616000175476074\n",
      "Iteration 509: loss -2.6270155906677246\n",
      "Iteration 510: loss -2.3159523010253906\n",
      "Iteration 511: loss -2.6097586154937744\n",
      "Iteration 512: loss -2.645235538482666\n",
      "Iteration 513: loss -2.6466848850250244\n",
      "Iteration 514: loss -2.7525038719177246\n",
      "Iteration 515: loss -2.6389825344085693\n",
      "Iteration 516: loss -2.6396069526672363\n",
      "Iteration 517: loss -2.182638168334961\n",
      "Iteration 518: loss -2.0138089656829834\n",
      "Iteration 519: loss -2.1187057495117188\n",
      "Iteration 520: loss -1.935884952545166\n",
      "Iteration 521: loss -2.093790292739868\n",
      "Iteration 522: loss -1.7819446325302124\n",
      "Iteration 523: loss -2.15018367767334\n",
      "Iteration 524: loss -1.8165075778961182\n",
      "Iteration 525: loss -1.922000527381897\n",
      "Iteration 526: loss -2.1598329544067383\n",
      "Iteration 527: loss -1.9759255647659302\n",
      "Iteration 528: loss -1.7448108196258545\n",
      "Iteration 529: loss -2.039917469024658\n",
      "Iteration 530: loss -2.326608657836914\n",
      "Iteration 531: loss -2.227609634399414\n",
      "Iteration 532: loss -2.269717216491699\n",
      "Iteration 533: loss -2.006169080734253\n",
      "Iteration 534: loss -2.1801750659942627\n",
      "Iteration 535: loss -2.24061918258667\n",
      "Iteration 536: loss -2.104119062423706\n",
      "Iteration 537: loss -2.383305311203003\n",
      "Iteration 538: loss -2.144814968109131\n",
      "Iteration 539: loss -2.112839698791504\n",
      "Iteration 540: loss -2.2636806964874268\n",
      "Iteration 541: loss -2.338078260421753\n",
      "Iteration 542: loss -2.4236724376678467\n",
      "Iteration 543: loss -2.4637417793273926\n",
      "Iteration 544: loss -2.2685539722442627\n",
      "Iteration 545: loss -2.3375940322875977\n",
      "Iteration 546: loss -2.487774133682251\n",
      "Iteration 547: loss -2.4884157180786133\n",
      "Iteration 548: loss -2.415544033050537\n",
      "Iteration 549: loss -2.540358066558838\n",
      "Iteration 550: loss -2.6380014419555664\n",
      "Iteration 551: loss -2.439368486404419\n",
      "Iteration 552: loss -2.5106935501098633\n",
      "Iteration 553: loss -2.622821092605591\n",
      "Iteration 554: loss -2.6031084060668945\n",
      "Iteration 555: loss -2.506716728210449\n",
      "Iteration 556: loss -2.448011875152588\n",
      "Iteration 557: loss -2.570176124572754\n",
      "Iteration 558: loss -2.5508272647857666\n",
      "Iteration 559: loss -2.501838445663452\n",
      "Iteration 560: loss -2.7596940994262695\n",
      "Iteration 561: loss -2.4769127368927\n",
      "Iteration 562: loss -2.592503309249878\n",
      "Iteration 563: loss -2.6114606857299805\n",
      "Iteration 564: loss -2.5268280506134033\n",
      "Iteration 565: loss -2.7168591022491455\n",
      "Iteration 566: loss -2.4582526683807373\n",
      "Iteration 567: loss -2.6628036499023438\n",
      "Iteration 568: loss -2.6181747913360596\n",
      "Iteration 569: loss -2.7473723888397217\n",
      "Iteration 570: loss -2.633673906326294\n",
      "Iteration 571: loss -2.525137424468994\n",
      "Iteration 572: loss -2.6368181705474854\n",
      "Iteration 573: loss -2.5043609142303467\n",
      "Iteration 574: loss -2.666163682937622\n",
      "Iteration 575: loss -2.7014119625091553\n",
      "Iteration 576: loss -2.6568148136138916\n",
      "Iteration 577: loss -2.593493700027466\n",
      "Iteration 578: loss -2.6220877170562744\n",
      "Iteration 579: loss -2.7370243072509766\n",
      "Iteration 580: loss -2.6723873615264893\n",
      "Iteration 581: loss -2.6992201805114746\n",
      "Iteration 582: loss -2.66154408454895\n",
      "Iteration 583: loss -2.5677762031555176\n",
      "Iteration 584: loss -2.64768123626709\n",
      "Iteration 585: loss -2.7389657497406006\n",
      "Iteration 586: loss -2.634018659591675\n",
      "Iteration 587: loss -2.476705312728882\n",
      "Iteration 588: loss -2.702069044113159\n",
      "Iteration 589: loss -2.2798736095428467\n",
      "Iteration 590: loss -2.789607048034668\n",
      "Iteration 591: loss -2.312556028366089\n",
      "Iteration 592: loss -2.4833672046661377\n",
      "Iteration 593: loss -2.479840040206909\n",
      "Iteration 594: loss -2.030958890914917\n",
      "Iteration 595: loss -2.6193675994873047\n",
      "Iteration 596: loss -2.465247869491577\n",
      "Iteration 597: loss -2.4895877838134766\n",
      "Iteration 598: loss -2.55588698387146\n",
      "Iteration 599: loss -2.636784553527832\n",
      "Iteration 600: loss -2.3082435131073\n",
      "Iteration 601: loss -2.3871090412139893\n",
      "Iteration 602: loss -2.473344326019287\n",
      "Iteration 603: loss -2.5283091068267822\n",
      "Iteration 604: loss -2.3644096851348877\n",
      "Iteration 605: loss -2.58758544921875\n",
      "Iteration 606: loss -2.4590930938720703\n",
      "Iteration 607: loss -2.6372549533843994\n",
      "Iteration 608: loss -2.4169719219207764\n",
      "Iteration 609: loss -2.504422903060913\n",
      "Iteration 610: loss -2.693542718887329\n",
      "Iteration 611: loss -2.668795108795166\n",
      "Iteration 612: loss -2.6452553272247314\n",
      "Iteration 613: loss -2.5692989826202393\n",
      "Iteration 614: loss -2.547276735305786\n",
      "Iteration 615: loss -2.750761032104492\n",
      "Iteration 616: loss -2.7788121700286865\n",
      "Iteration 617: loss -2.6830151081085205\n",
      "Iteration 618: loss -2.6097958087921143\n",
      "Iteration 619: loss -2.7044405937194824\n",
      "Iteration 620: loss -2.6645007133483887\n",
      "Iteration 621: loss -2.591857671737671\n",
      "Iteration 622: loss -2.716935157775879\n",
      "Iteration 623: loss -2.6890430450439453\n",
      "Iteration 624: loss -2.712170362472534\n",
      "Iteration 625: loss -2.7331132888793945\n",
      "Iteration 626: loss -2.7680110931396484\n",
      "Iteration 627: loss -2.7767531871795654\n",
      "Iteration 628: loss -2.758378028869629\n",
      "Iteration 629: loss -2.562507390975952\n",
      "Iteration 630: loss -2.6323025226593018\n",
      "Iteration 631: loss -2.824373722076416\n",
      "Iteration 632: loss -2.8978488445281982\n",
      "Iteration 633: loss -2.6502907276153564\n",
      "Iteration 634: loss -2.8987669944763184\n",
      "Iteration 635: loss -2.7236130237579346\n",
      "Iteration 636: loss -2.7367775440216064\n",
      "Iteration 637: loss -2.972327947616577\n",
      "Iteration 638: loss -2.6883251667022705\n",
      "Iteration 639: loss -2.7597920894622803\n",
      "Iteration 640: loss -2.634274959564209\n",
      "Iteration 641: loss -2.734135627746582\n",
      "Iteration 642: loss -2.6736207008361816\n",
      "Iteration 643: loss -2.764174461364746\n",
      "Iteration 644: loss -2.82257080078125\n",
      "Iteration 645: loss -2.7299070358276367\n",
      "Iteration 646: loss -2.8894660472869873\n",
      "Iteration 647: loss -2.8972506523132324\n",
      "Iteration 648: loss -2.709764003753662\n",
      "Iteration 649: loss -2.8916988372802734\n",
      "Iteration 650: loss -2.7834930419921875\n",
      "Iteration 651: loss -2.635310411453247\n",
      "Iteration 652: loss -2.8482885360717773\n",
      "Iteration 653: loss -2.5835161209106445\n",
      "Iteration 654: loss -2.7999608516693115\n",
      "Iteration 655: loss -2.7205333709716797\n",
      "Iteration 656: loss -2.712697744369507\n",
      "Iteration 657: loss -2.9293506145477295\n",
      "Iteration 658: loss -2.7752435207366943\n",
      "Iteration 659: loss -2.8788881301879883\n",
      "Iteration 660: loss -2.6771252155303955\n",
      "Iteration 661: loss -2.7579221725463867\n",
      "Iteration 662: loss -2.8134875297546387\n",
      "Iteration 663: loss -2.9955859184265137\n",
      "Iteration 664: loss -2.7932708263397217\n",
      "Iteration 665: loss -2.8168752193450928\n",
      "Iteration 666: loss -2.7805330753326416\n",
      "Iteration 667: loss -2.945614814758301\n",
      "Iteration 668: loss -2.9332103729248047\n",
      "Iteration 669: loss -2.988320827484131\n",
      "Iteration 670: loss -2.7112905979156494\n",
      "Iteration 671: loss -2.8606600761413574\n",
      "Iteration 672: loss -2.6564371585845947\n",
      "Iteration 673: loss -2.9412314891815186\n",
      "Iteration 674: loss -2.468226909637451\n",
      "Iteration 675: loss -2.867753267288208\n",
      "Iteration 676: loss -2.9135773181915283\n",
      "Iteration 677: loss -2.8464722633361816\n",
      "Iteration 678: loss -2.7727668285369873\n",
      "Iteration 679: loss -2.595736026763916\n",
      "Iteration 680: loss -2.9429407119750977\n",
      "Iteration 681: loss -2.7121102809906006\n",
      "Iteration 682: loss -2.868396520614624\n",
      "Iteration 683: loss -2.6955597400665283\n",
      "Iteration 684: loss -2.7831552028656006\n",
      "Iteration 685: loss -2.780893564224243\n",
      "Iteration 686: loss -2.3398334980010986\n",
      "Iteration 687: loss -2.9334053993225098\n",
      "Iteration 688: loss -2.2798044681549072\n",
      "Iteration 689: loss -2.6348090171813965\n",
      "Iteration 690: loss -2.8346240520477295\n",
      "Iteration 691: loss -2.6039938926696777\n",
      "Iteration 692: loss -2.909895896911621\n",
      "Iteration 693: loss -2.6638805866241455\n",
      "Iteration 694: loss -2.8819613456726074\n",
      "Iteration 695: loss -2.9689114093780518\n",
      "Iteration 696: loss -2.7813820838928223\n",
      "Iteration 697: loss -2.7800185680389404\n",
      "Iteration 698: loss -2.8626279830932617\n",
      "Iteration 699: loss -2.9308526515960693\n",
      "Iteration 700: loss -2.8548691272735596\n",
      "Iteration 701: loss -2.8331825733184814\n",
      "Iteration 702: loss -2.9178948402404785\n",
      "Iteration 703: loss -2.956380605697632\n",
      "Iteration 704: loss -2.74387788772583\n",
      "Iteration 705: loss -2.771392583847046\n",
      "Iteration 706: loss -2.825051784515381\n",
      "Iteration 707: loss -2.6471729278564453\n",
      "Iteration 708: loss -2.757438898086548\n",
      "Iteration 709: loss -2.8840527534484863\n",
      "Iteration 710: loss -2.872865915298462\n",
      "Iteration 711: loss -3.0189242362976074\n",
      "Iteration 712: loss -2.889052152633667\n",
      "Iteration 713: loss -2.7565271854400635\n",
      "Iteration 714: loss -2.797917366027832\n",
      "Iteration 715: loss -2.7634918689727783\n",
      "Iteration 716: loss -2.963693857192993\n",
      "Iteration 717: loss -2.8812451362609863\n",
      "Iteration 718: loss -2.8086330890655518\n",
      "Iteration 719: loss -2.7700164318084717\n",
      "Iteration 720: loss -2.873837947845459\n",
      "Iteration 721: loss -2.9019577503204346\n",
      "Iteration 722: loss -2.9121406078338623\n",
      "Iteration 723: loss -2.7189700603485107\n",
      "Iteration 724: loss -2.816145658493042\n",
      "Iteration 725: loss -2.9030568599700928\n",
      "Iteration 726: loss -2.866903305053711\n",
      "Iteration 727: loss -3.041705846786499\n",
      "Iteration 728: loss -2.972867965698242\n",
      "Iteration 729: loss -2.85390043258667\n",
      "Iteration 730: loss -3.0161120891571045\n",
      "Iteration 731: loss -2.9086802005767822\n",
      "Iteration 732: loss -2.9779491424560547\n",
      "Iteration 733: loss -2.917844533920288\n",
      "Iteration 734: loss -3.0077099800109863\n",
      "Iteration 735: loss -2.8553502559661865\n",
      "Iteration 736: loss -3.0857045650482178\n",
      "Iteration 737: loss -2.9349255561828613\n",
      "Iteration 738: loss -3.1384963989257812\n",
      "Iteration 739: loss -2.9320485591888428\n",
      "Iteration 740: loss -3.0623083114624023\n",
      "Iteration 741: loss -3.100454330444336\n",
      "Iteration 742: loss -3.0527117252349854\n",
      "Iteration 743: loss -3.0073633193969727\n",
      "Iteration 744: loss -2.723585605621338\n",
      "Iteration 745: loss -2.593507766723633\n",
      "Iteration 746: loss -3.0759856700897217\n",
      "Iteration 747: loss -2.8376410007476807\n",
      "Iteration 748: loss -2.836810827255249\n",
      "Iteration 749: loss -2.984657049179077\n",
      "Iteration 750: loss -3.08504581451416\n",
      "Iteration 751: loss -2.771266222000122\n",
      "Iteration 752: loss -2.8793563842773438\n",
      "Iteration 753: loss -2.766350746154785\n",
      "Iteration 754: loss -2.8092894554138184\n",
      "Iteration 755: loss -2.7862589359283447\n",
      "Iteration 756: loss -2.9616339206695557\n",
      "Iteration 757: loss -2.9782004356384277\n",
      "Iteration 758: loss -3.0172464847564697\n",
      "Iteration 759: loss -3.046292781829834\n",
      "Iteration 760: loss -2.9863171577453613\n",
      "Iteration 761: loss -2.94047212600708\n",
      "Iteration 762: loss -2.903116464614868\n",
      "Iteration 763: loss -2.885237216949463\n",
      "Iteration 764: loss -3.0199973583221436\n",
      "Iteration 765: loss -2.792006731033325\n",
      "Iteration 766: loss -3.054379940032959\n",
      "Iteration 767: loss -2.815582036972046\n",
      "Iteration 768: loss -2.8390579223632812\n",
      "Iteration 769: loss -2.93452787399292\n",
      "Iteration 770: loss -2.6275298595428467\n",
      "Iteration 771: loss -3.1509432792663574\n",
      "Iteration 772: loss -2.982236862182617\n",
      "Iteration 773: loss -2.7520651817321777\n",
      "Iteration 774: loss -2.919175386428833\n",
      "Iteration 775: loss -2.7517483234405518\n",
      "Iteration 776: loss -2.542722702026367\n",
      "Iteration 777: loss -3.0755269527435303\n",
      "Iteration 778: loss -2.863823175430298\n",
      "Iteration 779: loss -2.9271926879882812\n",
      "Iteration 780: loss -2.8311243057250977\n",
      "Iteration 781: loss -3.005427837371826\n",
      "Iteration 782: loss -2.821927070617676\n",
      "Iteration 783: loss -2.634244918823242\n",
      "Iteration 784: loss -3.0342438220977783\n",
      "Iteration 785: loss -2.8400309085845947\n",
      "Iteration 786: loss -2.8984241485595703\n",
      "Iteration 787: loss -2.934049606323242\n",
      "Iteration 788: loss -2.920985460281372\n",
      "Iteration 789: loss -2.9683616161346436\n",
      "Iteration 790: loss -2.9872498512268066\n",
      "Iteration 791: loss -3.005297899246216\n",
      "Iteration 792: loss -2.694129228591919\n",
      "Iteration 793: loss -2.8126142024993896\n",
      "Iteration 794: loss -2.7528984546661377\n",
      "Iteration 795: loss -2.9854180812835693\n",
      "Iteration 796: loss -2.9846808910369873\n",
      "Iteration 797: loss -3.023761510848999\n",
      "Iteration 798: loss -2.877504587173462\n",
      "Iteration 799: loss -2.925870418548584\n",
      "Iteration 800: loss -3.0276482105255127\n",
      "Iteration 801: loss -2.868711471557617\n",
      "Iteration 802: loss -2.7945363521575928\n",
      "Iteration 803: loss -3.0024075508117676\n",
      "Iteration 804: loss -2.858152389526367\n",
      "Iteration 805: loss -2.7395713329315186\n",
      "Iteration 806: loss -2.595219373703003\n",
      "Iteration 807: loss -2.853255033493042\n",
      "Iteration 808: loss -2.879260301589966\n",
      "Iteration 809: loss -2.601458787918091\n",
      "Iteration 810: loss -2.7383315563201904\n",
      "Iteration 811: loss -2.7961411476135254\n",
      "Iteration 812: loss -2.836068630218506\n",
      "Iteration 813: loss -2.800746202468872\n",
      "Iteration 814: loss -2.8234829902648926\n",
      "Iteration 815: loss -2.8148276805877686\n",
      "Iteration 816: loss -2.746201276779175\n",
      "Iteration 817: loss -2.7045154571533203\n",
      "Iteration 818: loss -3.00253963470459\n",
      "Iteration 819: loss -2.949030876159668\n",
      "Iteration 820: loss -2.89155650138855\n",
      "Iteration 821: loss -2.8983068466186523\n",
      "Iteration 822: loss -3.047057867050171\n",
      "Iteration 823: loss -2.9042630195617676\n",
      "Iteration 824: loss -3.0102198123931885\n",
      "Iteration 825: loss -3.168726921081543\n",
      "Iteration 826: loss -2.90263032913208\n",
      "Iteration 827: loss -2.9332590103149414\n",
      "Iteration 828: loss -3.0923125743865967\n",
      "Iteration 829: loss -2.942392349243164\n",
      "Iteration 830: loss -2.9630424976348877\n",
      "Iteration 831: loss -2.8499650955200195\n",
      "Iteration 832: loss -2.9905474185943604\n",
      "Iteration 833: loss -2.97360897064209\n",
      "Iteration 834: loss -2.8268518447875977\n",
      "Iteration 835: loss -3.0386013984680176\n",
      "Iteration 836: loss -3.086975336074829\n",
      "Iteration 837: loss -2.9336578845977783\n",
      "Iteration 838: loss -2.990269660949707\n",
      "Iteration 839: loss -3.03515625\n",
      "Iteration 840: loss -2.9541335105895996\n",
      "Iteration 841: loss -3.127293586730957\n",
      "Iteration 842: loss -3.054455518722534\n",
      "Iteration 843: loss -2.970215082168579\n",
      "Iteration 844: loss -3.119723320007324\n",
      "Iteration 845: loss -2.6920077800750732\n",
      "Iteration 846: loss -3.0916521549224854\n",
      "Iteration 847: loss -2.6607611179351807\n",
      "Iteration 848: loss -3.161165475845337\n",
      "Iteration 849: loss -2.7432804107666016\n",
      "Iteration 850: loss -2.8104162216186523\n",
      "Iteration 851: loss -3.018995761871338\n",
      "Iteration 852: loss -2.7164461612701416\n",
      "Iteration 853: loss -2.9894299507141113\n",
      "Iteration 854: loss -2.6894619464874268\n",
      "Iteration 855: loss -2.853548526763916\n",
      "Iteration 856: loss -2.9901223182678223\n",
      "Iteration 857: loss -2.836981773376465\n",
      "Iteration 858: loss -2.951150417327881\n",
      "Iteration 859: loss -3.0082640647888184\n",
      "Iteration 860: loss -3.064246892929077\n",
      "Iteration 861: loss -3.100424289703369\n",
      "Iteration 862: loss -3.082949161529541\n",
      "Iteration 863: loss -2.991783380508423\n",
      "Iteration 864: loss -3.0663437843322754\n",
      "Iteration 865: loss -3.1369190216064453\n",
      "Iteration 866: loss -2.9496960639953613\n",
      "Iteration 867: loss -2.8995885848999023\n",
      "Iteration 868: loss -3.171254873275757\n",
      "Iteration 869: loss -3.0346791744232178\n",
      "Iteration 870: loss -3.127389430999756\n",
      "Iteration 871: loss -2.8824212551116943\n",
      "Iteration 872: loss -2.8736703395843506\n",
      "Iteration 873: loss -2.7889575958251953\n",
      "Iteration 874: loss -3.0284366607666016\n",
      "Iteration 875: loss -2.970878839492798\n",
      "Iteration 876: loss -2.817812442779541\n",
      "Iteration 877: loss -3.056626796722412\n",
      "Iteration 878: loss -3.1617014408111572\n",
      "Iteration 879: loss -3.0625011920928955\n",
      "Iteration 880: loss -3.07768177986145\n",
      "Iteration 881: loss -3.0050628185272217\n",
      "Iteration 882: loss -2.967877149581909\n",
      "Iteration 883: loss -2.944746494293213\n",
      "Iteration 884: loss -2.90301513671875\n",
      "Iteration 885: loss -2.895944833755493\n",
      "Iteration 886: loss -2.9950919151306152\n",
      "Iteration 887: loss -3.0186288356781006\n",
      "Iteration 888: loss -2.857908248901367\n",
      "Iteration 889: loss -2.9190239906311035\n",
      "Iteration 890: loss -3.152134418487549\n",
      "Iteration 891: loss -3.0067381858825684\n",
      "Iteration 892: loss -3.026475191116333\n",
      "Iteration 893: loss -3.0939033031463623\n",
      "Iteration 894: loss -3.0961289405822754\n",
      "Iteration 895: loss -3.028324604034424\n",
      "Iteration 896: loss -3.0507967472076416\n",
      "Iteration 897: loss -3.0926389694213867\n",
      "Iteration 898: loss -2.933878183364868\n",
      "Iteration 899: loss -2.9264864921569824\n",
      "Iteration 900: loss -3.0926647186279297\n",
      "Iteration 901: loss -3.055772304534912\n",
      "Iteration 902: loss -3.0831644535064697\n",
      "Iteration 903: loss -2.9675989151000977\n",
      "Iteration 904: loss -2.9059267044067383\n",
      "Iteration 905: loss -2.8897016048431396\n",
      "Iteration 906: loss -2.9090280532836914\n",
      "Iteration 907: loss -2.975071668624878\n",
      "Iteration 908: loss -2.8819704055786133\n",
      "Iteration 909: loss -2.896984815597534\n",
      "Iteration 910: loss -2.8778226375579834\n",
      "Iteration 911: loss -3.0406363010406494\n",
      "Iteration 912: loss -2.8273203372955322\n",
      "Iteration 913: loss -3.026075839996338\n",
      "Iteration 914: loss -2.733374834060669\n",
      "Iteration 915: loss -3.015939235687256\n",
      "Iteration 916: loss -3.068626642227173\n",
      "Iteration 917: loss -2.891505241394043\n",
      "Iteration 918: loss -3.056490898132324\n",
      "Iteration 919: loss -3.072702646255493\n",
      "Iteration 920: loss -2.8610591888427734\n",
      "Iteration 921: loss -3.0110461711883545\n",
      "Iteration 922: loss -3.070021390914917\n",
      "Iteration 923: loss -2.9809305667877197\n",
      "Iteration 924: loss -3.0738179683685303\n",
      "Iteration 925: loss -3.0831713676452637\n",
      "Iteration 926: loss -3.097498893737793\n",
      "Iteration 927: loss -3.16487455368042\n",
      "Iteration 928: loss -3.1201071739196777\n",
      "Iteration 929: loss -3.128267765045166\n",
      "Iteration 930: loss -3.223578453063965\n",
      "Iteration 931: loss -2.917965888977051\n",
      "Iteration 932: loss -3.0458061695098877\n",
      "Iteration 933: loss -3.087111711502075\n",
      "Iteration 934: loss -3.138387680053711\n",
      "Iteration 935: loss -3.0673270225524902\n",
      "Iteration 936: loss -3.1560614109039307\n",
      "Iteration 937: loss -3.2048401832580566\n",
      "Iteration 938: loss -3.1865813732147217\n",
      "Iteration 939: loss -2.776291847229004\n",
      "Iteration 940: loss -3.0304152965545654\n",
      "Iteration 941: loss -3.0962531566619873\n",
      "Iteration 942: loss -2.916489839553833\n",
      "Iteration 943: loss -3.2116992473602295\n",
      "Iteration 944: loss -3.156148672103882\n",
      "Iteration 945: loss -3.0686447620391846\n",
      "Iteration 946: loss -3.0096983909606934\n",
      "Iteration 947: loss -2.7644948959350586\n",
      "Iteration 948: loss -3.0495214462280273\n",
      "Iteration 949: loss -3.214437484741211\n",
      "Iteration 950: loss -3.057577133178711\n",
      "Iteration 951: loss -2.9415946006774902\n",
      "Iteration 952: loss -3.1749343872070312\n",
      "Iteration 953: loss -2.9697954654693604\n",
      "Iteration 954: loss -2.8955905437469482\n",
      "Iteration 955: loss -2.8945016860961914\n",
      "Iteration 956: loss -2.9077107906341553\n",
      "Iteration 957: loss -2.748659610748291\n",
      "Iteration 958: loss -2.8467884063720703\n",
      "Iteration 959: loss -2.873589515686035\n",
      "Iteration 960: loss -2.9803824424743652\n",
      "Iteration 961: loss -2.8015291690826416\n",
      "Iteration 962: loss -2.86733341217041\n",
      "Iteration 963: loss -2.974834680557251\n",
      "Iteration 964: loss -2.974320650100708\n",
      "Iteration 965: loss -2.6710782051086426\n",
      "Iteration 966: loss -2.8317344188690186\n",
      "Iteration 967: loss -2.8392157554626465\n",
      "Iteration 968: loss -2.8201980590820312\n",
      "Iteration 969: loss -2.512119770050049\n",
      "Iteration 970: loss -2.867558240890503\n",
      "Iteration 971: loss -2.809671640396118\n",
      "Iteration 972: loss -2.800626516342163\n",
      "Iteration 973: loss -2.903146266937256\n",
      "Iteration 974: loss -2.9182863235473633\n",
      "Iteration 975: loss -2.7187998294830322\n",
      "Iteration 976: loss -2.9889039993286133\n",
      "Iteration 977: loss -2.91884708404541\n",
      "Iteration 978: loss -2.7937872409820557\n",
      "Iteration 979: loss -3.1071839332580566\n",
      "Iteration 980: loss -2.9907965660095215\n",
      "Iteration 981: loss -3.005690574645996\n",
      "Iteration 982: loss -3.0558464527130127\n",
      "Iteration 983: loss -3.0117485523223877\n",
      "Iteration 984: loss -3.168679714202881\n",
      "Iteration 985: loss -3.1436548233032227\n",
      "Iteration 986: loss -2.9841065406799316\n",
      "Iteration 987: loss -3.0882251262664795\n",
      "Iteration 988: loss -2.9588403701782227\n",
      "Iteration 989: loss -2.984161615371704\n",
      "Iteration 990: loss -2.914581298828125\n",
      "Iteration 991: loss -2.91522216796875\n",
      "Iteration 992: loss -3.0696523189544678\n",
      "Iteration 993: loss -2.855739116668701\n",
      "Iteration 994: loss -2.9061875343322754\n",
      "Iteration 995: loss -3.0043017864227295\n",
      "Iteration 996: loss -2.712986469268799\n",
      "Iteration 997: loss -2.941316604614258\n",
      "Iteration 998: loss -2.940863847732544\n",
      "Iteration 999: loss -2.9676618576049805\n",
      "Iteration 1000: loss -2.98344087600708\n",
      "Iteration 1001: loss -2.9596810340881348\n",
      "Iteration 1002: loss -2.761007308959961\n",
      "Iteration 1003: loss -2.621560573577881\n",
      "Iteration 1004: loss -2.6607797145843506\n",
      "Iteration 1005: loss -2.6838278770446777\n",
      "Iteration 1006: loss -2.8592517375946045\n",
      "Iteration 1007: loss -3.028226613998413\n",
      "Iteration 1008: loss -2.8285181522369385\n",
      "Iteration 1009: loss -2.8609814643859863\n",
      "Iteration 1010: loss -2.930771589279175\n",
      "Iteration 1011: loss -2.891326904296875\n",
      "Iteration 1012: loss -2.7077503204345703\n",
      "Iteration 1013: loss -2.9200234413146973\n",
      "Iteration 1014: loss -2.9036026000976562\n",
      "Iteration 1015: loss -2.9508092403411865\n",
      "Iteration 1016: loss -2.9060280323028564\n",
      "Iteration 1017: loss -2.9705147743225098\n",
      "Iteration 1018: loss -3.044706344604492\n",
      "Iteration 1019: loss -3.0499892234802246\n",
      "Iteration 1020: loss -3.078488826751709\n",
      "Iteration 1021: loss -3.1583902835845947\n",
      "Iteration 1022: loss -3.1518077850341797\n",
      "Iteration 1023: loss -3.0375053882598877\n",
      "Iteration 1024: loss -3.069249153137207\n",
      "Iteration 1025: loss -2.944423198699951\n",
      "Iteration 1026: loss -3.1263484954833984\n",
      "Iteration 1027: loss -3.030061960220337\n",
      "Iteration 1028: loss -2.9598493576049805\n",
      "Iteration 1029: loss -3.127413272857666\n",
      "Iteration 1030: loss -2.8791632652282715\n",
      "Iteration 1031: loss -3.1689226627349854\n",
      "Iteration 1032: loss -3.2229690551757812\n",
      "Iteration 1033: loss -3.0030362606048584\n",
      "Iteration 1034: loss -3.0389461517333984\n",
      "Iteration 1035: loss -3.1309871673583984\n",
      "Iteration 1036: loss -3.022789716720581\n",
      "Iteration 1037: loss -3.109098434448242\n",
      "Iteration 1038: loss -3.0773839950561523\n",
      "Iteration 1039: loss -2.9394702911376953\n",
      "Iteration 1040: loss -3.1122193336486816\n",
      "Iteration 1041: loss -2.9860808849334717\n",
      "Iteration 1042: loss -2.9247422218322754\n",
      "Iteration 1043: loss -2.959193229675293\n",
      "Iteration 1044: loss -3.134488582611084\n",
      "Iteration 1045: loss -3.1708016395568848\n",
      "Iteration 1046: loss -3.036051034927368\n",
      "Iteration 1047: loss -2.8885498046875\n",
      "Iteration 1048: loss -3.0581798553466797\n",
      "Iteration 1049: loss -3.029848575592041\n",
      "Iteration 1050: loss -3.159318447113037\n",
      "Iteration 1051: loss -3.1214144229888916\n",
      "Iteration 1052: loss -3.0078341960906982\n",
      "Iteration 1053: loss -3.1932668685913086\n",
      "Iteration 1054: loss -2.8940932750701904\n",
      "Iteration 1055: loss -3.1042919158935547\n",
      "Iteration 1056: loss -3.092695951461792\n",
      "Iteration 1057: loss -3.090437412261963\n",
      "Iteration 1058: loss -3.1621758937835693\n",
      "Iteration 1059: loss -3.199897050857544\n",
      "Iteration 1060: loss -2.9736056327819824\n",
      "Iteration 1061: loss -3.027761936187744\n",
      "Iteration 1062: loss -3.0069475173950195\n",
      "Iteration 1063: loss -2.858173131942749\n",
      "Iteration 1064: loss -3.147225856781006\n",
      "Iteration 1065: loss -3.023102045059204\n",
      "Iteration 1066: loss -3.052187442779541\n",
      "Iteration 1067: loss -2.9057576656341553\n",
      "Iteration 1068: loss -3.0176422595977783\n",
      "Iteration 1069: loss -3.1219184398651123\n",
      "Iteration 1070: loss -2.8013603687286377\n",
      "Iteration 1071: loss -3.061124563217163\n",
      "Iteration 1072: loss -3.2691779136657715\n",
      "Iteration 1073: loss -2.897212266921997\n",
      "Iteration 1074: loss -3.0472283363342285\n",
      "Iteration 1075: loss -2.9834084510803223\n",
      "Iteration 1076: loss -2.866673231124878\n",
      "Iteration 1077: loss -3.185866594314575\n",
      "Iteration 1078: loss -2.9351742267608643\n",
      "Iteration 1079: loss -3.006122350692749\n",
      "Iteration 1080: loss -3.1736178398132324\n",
      "Iteration 1081: loss -2.9182281494140625\n",
      "Iteration 1082: loss -3.0078284740448\n",
      "Iteration 1083: loss -3.202223539352417\n",
      "Iteration 1084: loss -3.0304155349731445\n",
      "Iteration 1085: loss -3.040081739425659\n",
      "Iteration 1086: loss -3.1340856552124023\n",
      "Iteration 1087: loss -3.1105990409851074\n",
      "Iteration 1088: loss -2.9801242351531982\n",
      "Iteration 1089: loss -3.179056167602539\n",
      "Iteration 1090: loss -2.932734966278076\n",
      "Iteration 1091: loss -2.9292898178100586\n",
      "Iteration 1092: loss -3.2061891555786133\n",
      "Iteration 1093: loss -3.1801464557647705\n",
      "Iteration 1094: loss -3.136592149734497\n",
      "Iteration 1095: loss -2.967339515686035\n",
      "Iteration 1096: loss -3.0594277381896973\n",
      "Iteration 1097: loss -2.9365668296813965\n",
      "Iteration 1098: loss -3.0924935340881348\n",
      "Iteration 1099: loss -2.962888717651367\n",
      "Iteration 1100: loss -2.9080240726470947\n",
      "Iteration 1101: loss -3.0823416709899902\n",
      "Iteration 1102: loss -3.0484397411346436\n",
      "Iteration 1103: loss -3.184022903442383\n",
      "Iteration 1104: loss -2.862293004989624\n",
      "Iteration 1105: loss -3.0674197673797607\n",
      "Iteration 1106: loss -2.88077449798584\n",
      "Iteration 1107: loss -2.794934034347534\n",
      "Iteration 1108: loss -3.1479642391204834\n",
      "Iteration 1109: loss -3.269862174987793\n",
      "Iteration 1110: loss -2.9521186351776123\n",
      "Iteration 1111: loss -3.0161914825439453\n",
      "Iteration 1112: loss -3.194061040878296\n",
      "Iteration 1113: loss -3.1134395599365234\n",
      "Iteration 1114: loss -3.0618791580200195\n",
      "Iteration 1115: loss -3.1238653659820557\n",
      "Iteration 1116: loss -2.9604101181030273\n",
      "Iteration 1117: loss -3.122197151184082\n",
      "Iteration 1118: loss -3.0765678882598877\n",
      "Iteration 1119: loss -3.0246949195861816\n",
      "Iteration 1120: loss -3.2399098873138428\n",
      "Iteration 1121: loss -3.1690900325775146\n",
      "Iteration 1122: loss -2.928361415863037\n",
      "Iteration 1123: loss -2.9915521144866943\n",
      "Iteration 1124: loss -3.190070390701294\n",
      "Iteration 1125: loss -3.021388530731201\n",
      "Iteration 1126: loss -3.0950350761413574\n",
      "Iteration 1127: loss -3.0510759353637695\n",
      "Iteration 1128: loss -3.0678319931030273\n",
      "Iteration 1129: loss -3.0316414833068848\n",
      "Iteration 1130: loss -2.87333345413208\n",
      "Iteration 1131: loss -3.2342889308929443\n",
      "Iteration 1132: loss -3.029776096343994\n",
      "Iteration 1133: loss -3.020076274871826\n",
      "Iteration 1134: loss -2.8517768383026123\n",
      "Iteration 1135: loss -3.190098285675049\n",
      "Iteration 1136: loss -2.9844067096710205\n",
      "Iteration 1137: loss -2.8662078380584717\n",
      "Iteration 1138: loss -3.0919580459594727\n",
      "Iteration 1139: loss -3.000640630722046\n",
      "Iteration 1140: loss -2.9038612842559814\n",
      "Iteration 1141: loss -3.1374895572662354\n",
      "Iteration 1142: loss -3.0585999488830566\n",
      "Iteration 1143: loss -2.7621734142303467\n",
      "Iteration 1144: loss -3.116602897644043\n",
      "Iteration 1145: loss -2.92830753326416\n",
      "Iteration 1146: loss -3.1136159896850586\n",
      "Iteration 1147: loss -3.018350124359131\n",
      "Iteration 1148: loss -3.0433096885681152\n",
      "Iteration 1149: loss -3.1410751342773438\n",
      "Iteration 1150: loss -3.1218628883361816\n",
      "Iteration 1151: loss -3.1204352378845215\n",
      "Iteration 1152: loss -2.9506757259368896\n",
      "Iteration 1153: loss -2.9842095375061035\n",
      "Iteration 1154: loss -3.084397792816162\n",
      "Iteration 1155: loss -2.7133965492248535\n",
      "Iteration 1156: loss -3.0560214519500732\n",
      "Iteration 1157: loss -3.0080554485321045\n",
      "Iteration 1158: loss -2.997020959854126\n",
      "Iteration 1159: loss -2.9404594898223877\n",
      "Iteration 1160: loss -3.083064079284668\n",
      "Iteration 1161: loss -3.0697457790374756\n",
      "Iteration 1162: loss -3.0678577423095703\n",
      "Iteration 1163: loss -3.0611395835876465\n",
      "Iteration 1164: loss -3.0038208961486816\n",
      "Iteration 1165: loss -3.0029168128967285\n",
      "Iteration 1166: loss -3.1505355834960938\n",
      "Iteration 1167: loss -3.0204966068267822\n",
      "Iteration 1168: loss -3.1645667552948\n",
      "Iteration 1169: loss -2.8993663787841797\n",
      "Iteration 1170: loss -3.1270759105682373\n",
      "Iteration 1171: loss -3.0854451656341553\n",
      "Iteration 1172: loss -3.2777695655822754\n",
      "Iteration 1173: loss -3.068185329437256\n",
      "Iteration 1174: loss -3.048476457595825\n",
      "Iteration 1175: loss -3.0646777153015137\n",
      "Iteration 1176: loss -3.0851213932037354\n",
      "Iteration 1177: loss -3.3535468578338623\n",
      "Iteration 1178: loss -3.0647833347320557\n",
      "Iteration 1179: loss -3.1004440784454346\n",
      "Iteration 1180: loss -3.2625906467437744\n",
      "Iteration 1181: loss -3.1105968952178955\n",
      "Iteration 1182: loss -3.1814346313476562\n",
      "Iteration 1183: loss -3.132690191268921\n",
      "Iteration 1184: loss -3.1236767768859863\n",
      "Iteration 1185: loss -3.3977794647216797\n",
      "Iteration 1186: loss -3.105275869369507\n",
      "Iteration 1187: loss -3.2177650928497314\n",
      "Iteration 1188: loss -3.072747230529785\n",
      "Iteration 1189: loss -3.166810989379883\n",
      "Iteration 1190: loss -3.151947259902954\n",
      "Iteration 1191: loss -3.1349446773529053\n",
      "Iteration 1192: loss -3.262995719909668\n",
      "Iteration 1193: loss -3.1693477630615234\n",
      "Iteration 1194: loss -3.06280517578125\n",
      "Iteration 1195: loss -3.0139319896698\n",
      "Iteration 1196: loss -3.044356107711792\n",
      "Iteration 1197: loss -2.788398027420044\n",
      "Iteration 1198: loss -3.005380868911743\n",
      "Iteration 1199: loss -3.24702787399292\n",
      "Iteration 1200: loss -3.0529510974884033\n",
      "Iteration 1201: loss -3.151949405670166\n",
      "Iteration 1202: loss -3.144232749938965\n",
      "Iteration 1203: loss -3.079317569732666\n",
      "Iteration 1204: loss -3.1427721977233887\n",
      "Iteration 1205: loss -3.2105300426483154\n",
      "Iteration 1206: loss -3.2866060733795166\n",
      "Iteration 1207: loss -3.1868157386779785\n",
      "Iteration 1208: loss -3.2323436737060547\n",
      "Iteration 1209: loss -3.2052948474884033\n",
      "Iteration 1210: loss -3.2509310245513916\n",
      "Iteration 1211: loss -3.178694248199463\n",
      "Iteration 1212: loss -3.3708245754241943\n",
      "Iteration 1213: loss -3.2393267154693604\n",
      "Iteration 1214: loss -3.1230926513671875\n",
      "Iteration 1215: loss -3.2975025177001953\n",
      "Iteration 1216: loss -2.864361047744751\n",
      "Iteration 1217: loss -3.191706657409668\n",
      "Iteration 1218: loss -3.1709084510803223\n",
      "Iteration 1219: loss -2.949324607849121\n",
      "Iteration 1220: loss -3.2436978816986084\n",
      "Iteration 1221: loss -3.1562552452087402\n",
      "Iteration 1222: loss -3.069010019302368\n",
      "Iteration 1223: loss -2.9747304916381836\n",
      "Iteration 1224: loss -3.0755584239959717\n",
      "Iteration 1225: loss -2.923842430114746\n",
      "Iteration 1226: loss -2.937119483947754\n",
      "Iteration 1227: loss -2.974673271179199\n",
      "Iteration 1228: loss -2.872140407562256\n",
      "Iteration 1229: loss -3.1888608932495117\n",
      "Iteration 1230: loss -3.1156766414642334\n",
      "Iteration 1231: loss -2.9413280487060547\n",
      "Iteration 1232: loss -3.054290533065796\n",
      "Iteration 1233: loss -3.0790350437164307\n",
      "Iteration 1234: loss -3.124823808670044\n",
      "Iteration 1235: loss -2.9315061569213867\n",
      "Iteration 1236: loss -3.0806031227111816\n",
      "Iteration 1237: loss -2.971890926361084\n",
      "Iteration 1238: loss -3.143078327178955\n",
      "Iteration 1239: loss -3.051853656768799\n",
      "Iteration 1240: loss -3.0467870235443115\n",
      "Iteration 1241: loss -3.0360500812530518\n",
      "Iteration 1242: loss -3.050381660461426\n",
      "Iteration 1243: loss -2.9469985961914062\n",
      "Iteration 1244: loss -3.0179595947265625\n",
      "Iteration 1245: loss -2.9870200157165527\n",
      "Iteration 1246: loss -2.892951011657715\n",
      "Iteration 1247: loss -2.877178907394409\n",
      "Iteration 1248: loss -2.9963266849517822\n",
      "Iteration 1249: loss -3.006368398666382\n",
      "Iteration 1250: loss -3.125256299972534\n",
      "Iteration 1251: loss -3.005951166152954\n",
      "Iteration 1252: loss -2.904552936553955\n",
      "Iteration 1253: loss -3.142423152923584\n",
      "Iteration 1254: loss -3.0511903762817383\n",
      "Iteration 1255: loss -2.864439010620117\n",
      "Iteration 1256: loss -3.1357674598693848\n",
      "Iteration 1257: loss -2.9075264930725098\n",
      "Iteration 1258: loss -2.9845223426818848\n",
      "Iteration 1259: loss -3.21684193611145\n",
      "Iteration 1260: loss -2.9584131240844727\n",
      "Iteration 1261: loss -2.836698055267334\n",
      "Iteration 1262: loss -2.829254388809204\n",
      "Iteration 1263: loss -3.1875414848327637\n",
      "Iteration 1264: loss -2.9759817123413086\n",
      "Iteration 1265: loss -3.0403192043304443\n",
      "Iteration 1266: loss -3.1503655910491943\n",
      "Iteration 1267: loss -3.169677495956421\n",
      "Iteration 1268: loss -3.1101367473602295\n",
      "Iteration 1269: loss -3.063314437866211\n",
      "Iteration 1270: loss -3.1833372116088867\n",
      "Iteration 1271: loss -3.101628303527832\n",
      "Iteration 1272: loss -2.9892358779907227\n",
      "Iteration 1273: loss -3.0520734786987305\n",
      "Iteration 1274: loss -2.7600913047790527\n",
      "Iteration 1275: loss -3.228238582611084\n",
      "Iteration 1276: loss -3.0649492740631104\n",
      "Iteration 1277: loss -3.17630672454834\n",
      "Iteration 1278: loss -3.0247421264648438\n",
      "Iteration 1279: loss -3.0975799560546875\n",
      "Iteration 1280: loss -3.2566497325897217\n",
      "Iteration 1281: loss -3.207221269607544\n",
      "Iteration 1282: loss -3.161513566970825\n",
      "Iteration 1283: loss -3.0938241481781006\n",
      "Iteration 1284: loss -2.779296875\n",
      "Iteration 1285: loss -3.06306529045105\n",
      "Iteration 1286: loss -2.9664907455444336\n",
      "Iteration 1287: loss -2.8965697288513184\n",
      "Iteration 1288: loss -3.2105748653411865\n",
      "Iteration 1289: loss -3.0514655113220215\n",
      "Iteration 1290: loss -2.7672817707061768\n",
      "Iteration 1291: loss -3.1114108562469482\n",
      "Iteration 1292: loss -3.0444116592407227\n",
      "Iteration 1293: loss -3.0070321559906006\n",
      "Iteration 1294: loss -3.0074150562286377\n",
      "Iteration 1295: loss -3.1412079334259033\n",
      "Iteration 1296: loss -2.973276376724243\n",
      "Iteration 1297: loss -3.0316598415374756\n",
      "Iteration 1298: loss -3.1672661304473877\n",
      "Iteration 1299: loss -3.1818530559539795\n",
      "Iteration 1300: loss -3.114917039871216\n",
      "Iteration 1301: loss -3.1118059158325195\n",
      "Iteration 1302: loss -3.1544606685638428\n",
      "Iteration 1303: loss -3.3329849243164062\n",
      "Iteration 1304: loss -3.186185836791992\n",
      "Iteration 1305: loss -3.0253372192382812\n",
      "Iteration 1306: loss -3.074500799179077\n",
      "Iteration 1307: loss -3.1740522384643555\n",
      "Iteration 1308: loss -2.9266185760498047\n",
      "Iteration 1309: loss -3.151358127593994\n",
      "Iteration 1310: loss -3.092012882232666\n",
      "Iteration 1311: loss -3.151010751724243\n",
      "Iteration 1312: loss -2.90727162361145\n",
      "Iteration 1313: loss -3.2199010848999023\n",
      "Iteration 1314: loss -3.1989355087280273\n",
      "Iteration 1315: loss -3.062746524810791\n",
      "Iteration 1316: loss -2.86856746673584\n",
      "Iteration 1317: loss -3.054128408432007\n",
      "Iteration 1318: loss -3.0564424991607666\n",
      "Iteration 1319: loss -3.065824508666992\n",
      "Iteration 1320: loss -3.2531003952026367\n",
      "Iteration 1321: loss -3.093322515487671\n",
      "Iteration 1322: loss -3.2366585731506348\n",
      "Iteration 1323: loss -2.885327100753784\n",
      "Iteration 1324: loss -2.9023306369781494\n",
      "Iteration 1325: loss -3.185606002807617\n",
      "Iteration 1326: loss -2.9499905109405518\n",
      "Iteration 1327: loss -3.0447771549224854\n",
      "Iteration 1328: loss -2.8996927738189697\n",
      "Iteration 1329: loss -3.0084993839263916\n",
      "Iteration 1330: loss -3.0628271102905273\n",
      "Iteration 1331: loss -3.176950693130493\n",
      "Iteration 1332: loss -3.2433362007141113\n",
      "Iteration 1333: loss -3.1561999320983887\n",
      "Iteration 1334: loss -3.150991439819336\n",
      "Iteration 1335: loss -3.2730300426483154\n",
      "Iteration 1336: loss -3.096708059310913\n",
      "Iteration 1337: loss -3.006707191467285\n",
      "Iteration 1338: loss -3.2013158798217773\n",
      "Iteration 1339: loss -3.004672050476074\n",
      "Iteration 1340: loss -3.0860085487365723\n",
      "Iteration 1341: loss -2.934319496154785\n",
      "Iteration 1342: loss -3.0174684524536133\n",
      "Iteration 1343: loss -2.9809670448303223\n",
      "Iteration 1344: loss -3.1592798233032227\n",
      "Iteration 1345: loss -3.2416892051696777\n",
      "Iteration 1346: loss -2.942600965499878\n",
      "Iteration 1347: loss -3.266925096511841\n",
      "Iteration 1348: loss -3.2163350582122803\n",
      "Iteration 1349: loss -3.1833009719848633\n",
      "Iteration 1350: loss -3.169299364089966\n",
      "Iteration 1351: loss -3.21575927734375\n",
      "Iteration 1352: loss -3.279111623764038\n",
      "Iteration 1353: loss -3.168877363204956\n",
      "Iteration 1354: loss -2.7373976707458496\n",
      "Iteration 1355: loss -3.1021945476531982\n",
      "Iteration 1356: loss -3.1630797386169434\n",
      "Iteration 1357: loss -3.1037344932556152\n",
      "Iteration 1358: loss -3.1791574954986572\n",
      "Iteration 1359: loss -3.1101207733154297\n",
      "Iteration 1360: loss -3.1809492111206055\n",
      "Iteration 1361: loss -3.1277832984924316\n",
      "Iteration 1362: loss -3.174556255340576\n",
      "Iteration 1363: loss -3.0842325687408447\n",
      "Iteration 1364: loss -3.231335163116455\n",
      "Iteration 1365: loss -3.1148386001586914\n",
      "Iteration 1366: loss -3.2285377979278564\n",
      "Iteration 1367: loss -3.183727502822876\n",
      "Iteration 1368: loss -3.1846978664398193\n",
      "Iteration 1369: loss -3.1898415088653564\n",
      "Iteration 1370: loss -3.22330904006958\n",
      "Iteration 1371: loss -3.339609384536743\n",
      "Iteration 1372: loss -3.3418753147125244\n",
      "Iteration 1373: loss -3.200869560241699\n",
      "Iteration 1374: loss -3.2025201320648193\n",
      "Iteration 1375: loss -3.2375640869140625\n",
      "Iteration 1376: loss -3.2274439334869385\n",
      "Iteration 1377: loss -3.250800371170044\n",
      "Iteration 1378: loss -3.0747737884521484\n",
      "Iteration 1379: loss -3.2909209728240967\n",
      "Iteration 1380: loss -3.245138168334961\n",
      "Iteration 1381: loss -3.173063278198242\n",
      "Iteration 1382: loss -3.0318520069122314\n",
      "Iteration 1383: loss -3.2550411224365234\n",
      "Iteration 1384: loss -3.1036245822906494\n",
      "Iteration 1385: loss -3.254323720932007\n",
      "Iteration 1386: loss -3.099428653717041\n",
      "Iteration 1387: loss -2.997555732727051\n",
      "Iteration 1388: loss -3.3012819290161133\n",
      "Iteration 1389: loss -3.193814754486084\n",
      "Iteration 1390: loss -2.6414573192596436\n",
      "Iteration 1391: loss -3.050112009048462\n",
      "Iteration 1392: loss -2.9509594440460205\n",
      "Iteration 1393: loss -2.6951022148132324\n",
      "Iteration 1394: loss -2.9637932777404785\n",
      "Iteration 1395: loss -3.1062207221984863\n",
      "Iteration 1396: loss -3.2536721229553223\n",
      "Iteration 1397: loss -3.0872771739959717\n",
      "Iteration 1398: loss -2.9870400428771973\n",
      "Iteration 1399: loss -3.0955710411071777\n",
      "Iteration 1400: loss -3.007040500640869\n",
      "Iteration 1401: loss -2.9346888065338135\n",
      "Iteration 1402: loss -3.1176750659942627\n",
      "Iteration 1403: loss -3.1204237937927246\n",
      "Iteration 1404: loss -3.006464958190918\n",
      "Iteration 1405: loss -3.1291000843048096\n",
      "Iteration 1406: loss -3.212458610534668\n",
      "Iteration 1407: loss -3.151564359664917\n",
      "Iteration 1408: loss -2.9497852325439453\n",
      "Iteration 1409: loss -3.220003128051758\n",
      "Iteration 1410: loss -3.1208529472351074\n",
      "Iteration 1411: loss -3.0898404121398926\n",
      "Iteration 1412: loss -3.1660125255584717\n",
      "Iteration 1413: loss -3.246523380279541\n",
      "Iteration 1414: loss -3.198455572128296\n",
      "Iteration 1415: loss -3.1973016262054443\n",
      "Iteration 1416: loss -3.2006592750549316\n",
      "Iteration 1417: loss -3.1340670585632324\n",
      "Iteration 1418: loss -3.1256003379821777\n",
      "Iteration 1419: loss -3.246309518814087\n",
      "Iteration 1420: loss -3.113645076751709\n",
      "Iteration 1421: loss -3.05735445022583\n",
      "Iteration 1422: loss -3.1315653324127197\n",
      "Iteration 1423: loss -3.0843095779418945\n",
      "Iteration 1424: loss -3.101900100708008\n",
      "Iteration 1425: loss -3.377598524093628\n",
      "Iteration 1426: loss -3.3277437686920166\n",
      "Iteration 1427: loss -3.0902674198150635\n",
      "Iteration 1428: loss -3.303997755050659\n",
      "Iteration 1429: loss -3.312544584274292\n",
      "Iteration 1430: loss -3.1404082775115967\n",
      "Iteration 1431: loss -3.2140917778015137\n",
      "Iteration 1432: loss -3.343554973602295\n",
      "Iteration 1433: loss -3.3294904232025146\n",
      "Iteration 1434: loss -3.290493965148926\n",
      "Iteration 1435: loss -3.475588083267212\n",
      "Iteration 1436: loss -3.0885586738586426\n",
      "Iteration 1437: loss -3.2188620567321777\n",
      "Iteration 1438: loss -3.1950485706329346\n",
      "Iteration 1439: loss -3.160829782485962\n",
      "Iteration 1440: loss -3.3405089378356934\n",
      "Iteration 1441: loss -3.133756160736084\n",
      "Iteration 1442: loss -3.324944496154785\n",
      "Iteration 1443: loss -2.956366539001465\n",
      "Iteration 1444: loss -3.3685357570648193\n",
      "Iteration 1445: loss -3.181990146636963\n",
      "Iteration 1446: loss -3.1538336277008057\n",
      "Iteration 1447: loss -3.356008529663086\n",
      "Iteration 1448: loss -3.2728006839752197\n",
      "Iteration 1449: loss -3.1043753623962402\n",
      "Iteration 1450: loss -3.130645513534546\n",
      "Iteration 1451: loss -3.2542755603790283\n",
      "Iteration 1452: loss -3.350911855697632\n",
      "Iteration 1453: loss -3.1657421588897705\n",
      "Iteration 1454: loss -3.2427868843078613\n",
      "Iteration 1455: loss -3.0936391353607178\n",
      "Iteration 1456: loss -3.167062282562256\n",
      "Iteration 1457: loss -3.2592709064483643\n",
      "Iteration 1458: loss -3.1805248260498047\n",
      "Iteration 1459: loss -3.2772011756896973\n",
      "Iteration 1460: loss -3.0779857635498047\n",
      "Iteration 1461: loss -3.3175859451293945\n",
      "Iteration 1462: loss -3.0960729122161865\n",
      "Iteration 1463: loss -3.135317802429199\n",
      "Iteration 1464: loss -3.2976326942443848\n",
      "Iteration 1465: loss -3.085158586502075\n",
      "Iteration 1466: loss -3.0819175243377686\n",
      "Iteration 1467: loss -3.252293586730957\n",
      "Iteration 1468: loss -3.164170026779175\n",
      "Iteration 1469: loss -3.166398286819458\n",
      "Iteration 1470: loss -3.266049385070801\n",
      "Iteration 1471: loss -3.376154899597168\n",
      "Iteration 1472: loss -3.3045828342437744\n",
      "Iteration 1473: loss -3.433840274810791\n",
      "Iteration 1474: loss -3.1485259532928467\n",
      "Iteration 1475: loss -3.2127208709716797\n",
      "Iteration 1476: loss -3.4097018241882324\n",
      "Iteration 1477: loss -3.260866165161133\n",
      "Iteration 1478: loss -3.3677966594696045\n",
      "Iteration 1479: loss -3.1164565086364746\n",
      "Iteration 1480: loss -3.2593894004821777\n",
      "Iteration 1481: loss -3.2647202014923096\n",
      "Iteration 1482: loss -3.2767040729522705\n",
      "Iteration 1483: loss -3.252107620239258\n",
      "Iteration 1484: loss -3.365928888320923\n",
      "Iteration 1485: loss -3.009280204772949\n",
      "Iteration 1486: loss -3.1336050033569336\n",
      "Iteration 1487: loss -3.258186101913452\n",
      "Iteration 1488: loss -2.904707431793213\n",
      "Iteration 1489: loss -3.2739531993865967\n",
      "Iteration 1490: loss -3.05550479888916\n",
      "Iteration 1491: loss -2.9286627769470215\n",
      "Iteration 1492: loss -3.193690776824951\n",
      "Iteration 1493: loss -3.0706114768981934\n",
      "Iteration 1494: loss -3.219113826751709\n",
      "Iteration 1495: loss -3.250235080718994\n",
      "Iteration 1496: loss -3.196650981903076\n",
      "Iteration 1497: loss -3.1593315601348877\n",
      "Iteration 1498: loss -3.181905508041382\n",
      "Iteration 1499: loss -3.268934965133667\n",
      "Iteration 1500: loss -3.1796343326568604\n",
      "Iteration 1501: loss -3.396472215652466\n",
      "Iteration 1502: loss -3.285494804382324\n",
      "Iteration 1503: loss -3.238581657409668\n",
      "Iteration 1504: loss -3.2998130321502686\n",
      "Iteration 1505: loss -3.453632116317749\n",
      "Iteration 1506: loss -3.3513152599334717\n",
      "Iteration 1507: loss -3.2128286361694336\n",
      "Iteration 1508: loss -3.2473931312561035\n",
      "Iteration 1509: loss -3.3369815349578857\n",
      "Iteration 1510: loss -3.358161687850952\n",
      "Iteration 1511: loss -3.3071608543395996\n",
      "Iteration 1512: loss -3.1274912357330322\n",
      "Iteration 1513: loss -3.1494808197021484\n",
      "Iteration 1514: loss -3.327617883682251\n",
      "Iteration 1515: loss -3.0993945598602295\n",
      "Iteration 1516: loss -3.380441188812256\n",
      "Iteration 1517: loss -3.1450188159942627\n",
      "Iteration 1518: loss -3.2822012901306152\n",
      "Iteration 1519: loss -3.163686752319336\n",
      "Iteration 1520: loss -3.161656379699707\n",
      "Iteration 1521: loss -3.1951961517333984\n",
      "Iteration 1522: loss -3.20609188079834\n",
      "Iteration 1523: loss -3.0971109867095947\n",
      "Iteration 1524: loss -3.065643072128296\n",
      "Iteration 1525: loss -3.122483730316162\n",
      "Iteration 1526: loss -3.279756784439087\n",
      "Iteration 1527: loss -3.1326639652252197\n",
      "Iteration 1528: loss -3.1878533363342285\n",
      "Iteration 1529: loss -3.1363344192504883\n",
      "Iteration 1530: loss -2.943181037902832\n",
      "Iteration 1531: loss -3.104464054107666\n",
      "Iteration 1532: loss -3.029086112976074\n",
      "Iteration 1533: loss -3.143601179122925\n",
      "Iteration 1534: loss -3.190544366836548\n",
      "Iteration 1535: loss -2.6748223304748535\n",
      "Iteration 1536: loss -3.3085546493530273\n",
      "Iteration 1537: loss -3.0732293128967285\n",
      "Iteration 1538: loss -2.9601175785064697\n",
      "Iteration 1539: loss -3.2924087047576904\n",
      "Iteration 1540: loss -2.8569130897521973\n",
      "Iteration 1541: loss -3.2704944610595703\n",
      "Iteration 1542: loss -3.104928970336914\n",
      "Iteration 1543: loss -3.141754388809204\n",
      "Iteration 1544: loss -3.2511720657348633\n",
      "Iteration 1545: loss -3.119274854660034\n",
      "Iteration 1546: loss -3.292860984802246\n",
      "Iteration 1547: loss -3.1117029190063477\n",
      "Iteration 1548: loss -3.200474262237549\n",
      "Iteration 1549: loss -3.230175733566284\n",
      "Iteration 1550: loss -3.2576351165771484\n",
      "Iteration 1551: loss -3.2567715644836426\n",
      "Iteration 1552: loss -3.212209463119507\n",
      "Iteration 1553: loss -3.2657299041748047\n",
      "Iteration 1554: loss -3.324671983718872\n",
      "Iteration 1555: loss -3.278989315032959\n",
      "Iteration 1556: loss -3.3129796981811523\n",
      "Iteration 1557: loss -3.2331430912017822\n",
      "Iteration 1558: loss -3.2826340198516846\n",
      "Iteration 1559: loss -3.40049409866333\n",
      "Iteration 1560: loss -3.261366605758667\n",
      "Iteration 1561: loss -3.289060354232788\n",
      "Iteration 1562: loss -3.33872127532959\n",
      "Iteration 1563: loss -3.278735637664795\n",
      "Iteration 1564: loss -3.225496768951416\n",
      "Iteration 1565: loss -3.3627514839172363\n",
      "Iteration 1566: loss -3.311575412750244\n",
      "Iteration 1567: loss -3.1287682056427\n",
      "Iteration 1568: loss -3.3239572048187256\n",
      "Iteration 1569: loss -3.215041399002075\n",
      "Iteration 1570: loss -3.140183925628662\n",
      "Iteration 1571: loss -3.292051076889038\n",
      "Iteration 1572: loss -3.2198755741119385\n",
      "Iteration 1573: loss -3.161177158355713\n",
      "Iteration 1574: loss -3.5204246044158936\n",
      "Iteration 1575: loss -3.1936309337615967\n",
      "Iteration 1576: loss -2.9942095279693604\n",
      "Iteration 1577: loss -2.99430775642395\n",
      "Iteration 1578: loss -2.981562614440918\n",
      "Iteration 1579: loss -3.2436211109161377\n",
      "Iteration 1580: loss -3.275527238845825\n",
      "Iteration 1581: loss -2.9040157794952393\n",
      "Iteration 1582: loss -3.191629409790039\n",
      "Iteration 1583: loss -3.2321600914001465\n",
      "Iteration 1584: loss -3.1419785022735596\n",
      "Iteration 1585: loss -3.1490695476531982\n",
      "Iteration 1586: loss -2.9803521633148193\n",
      "Iteration 1587: loss -3.1302874088287354\n",
      "Iteration 1588: loss -3.1202712059020996\n",
      "Iteration 1589: loss -3.217510461807251\n",
      "Iteration 1590: loss -3.2968082427978516\n",
      "Iteration 1591: loss -3.0417699813842773\n",
      "Iteration 1592: loss -3.1442792415618896\n",
      "Iteration 1593: loss -3.3380117416381836\n",
      "Iteration 1594: loss -3.2888784408569336\n",
      "Iteration 1595: loss -3.2053897380828857\n",
      "Iteration 1596: loss -3.2809786796569824\n",
      "Iteration 1597: loss -3.2300548553466797\n",
      "Iteration 1598: loss -3.0595908164978027\n",
      "Iteration 1599: loss -3.29160213470459\n",
      "Iteration 1600: loss -3.324160099029541\n",
      "Iteration 1601: loss -3.2250967025756836\n",
      "Iteration 1602: loss -3.2419915199279785\n",
      "Iteration 1603: loss -3.117922306060791\n",
      "Iteration 1604: loss -3.2967939376831055\n",
      "Iteration 1605: loss -3.108609199523926\n",
      "Iteration 1606: loss -3.1387839317321777\n",
      "Iteration 1607: loss -3.146944046020508\n",
      "Iteration 1608: loss -3.2361860275268555\n",
      "Iteration 1609: loss -3.0805323123931885\n",
      "Iteration 1610: loss -3.2701523303985596\n",
      "Iteration 1611: loss -3.116647958755493\n",
      "Iteration 1612: loss -3.227170944213867\n",
      "Iteration 1613: loss -3.089592218399048\n",
      "Iteration 1614: loss -3.2783398628234863\n",
      "Iteration 1615: loss -2.8216054439544678\n",
      "Iteration 1616: loss -3.0469882488250732\n",
      "Iteration 1617: loss -3.172830581665039\n",
      "Iteration 1618: loss -2.9911131858825684\n",
      "Iteration 1619: loss -3.1175243854522705\n",
      "Iteration 1620: loss -2.908416986465454\n",
      "Iteration 1621: loss -2.8515923023223877\n",
      "Iteration 1622: loss -3.0875771045684814\n",
      "Iteration 1623: loss -3.234445095062256\n",
      "Iteration 1624: loss -3.035278558731079\n",
      "Iteration 1625: loss -3.3230888843536377\n",
      "Iteration 1626: loss -3.117077589035034\n",
      "Iteration 1627: loss -2.8074734210968018\n",
      "Iteration 1628: loss -3.213365077972412\n",
      "Iteration 1629: loss -2.992183208465576\n",
      "Iteration 1630: loss -3.2091870307922363\n",
      "Iteration 1631: loss -2.9510042667388916\n",
      "Iteration 1632: loss -3.083829879760742\n",
      "Iteration 1633: loss -3.06111216545105\n",
      "Iteration 1634: loss -3.1006832122802734\n",
      "Iteration 1635: loss -3.1690480709075928\n",
      "Iteration 1636: loss -3.1616780757904053\n",
      "Iteration 1637: loss -3.151064872741699\n",
      "Iteration 1638: loss -3.0973446369171143\n",
      "Iteration 1639: loss -3.050264596939087\n",
      "Iteration 1640: loss -3.1532013416290283\n",
      "Iteration 1641: loss -3.1556015014648438\n",
      "Iteration 1642: loss -2.9532926082611084\n",
      "Iteration 1643: loss -3.0374152660369873\n",
      "Iteration 1644: loss -3.2620253562927246\n",
      "Iteration 1645: loss -3.1154251098632812\n",
      "Iteration 1646: loss -3.0562708377838135\n",
      "Iteration 1647: loss -3.2373995780944824\n",
      "Iteration 1648: loss -3.1672310829162598\n",
      "Iteration 1649: loss -3.096125602722168\n",
      "Iteration 1650: loss -3.1180810928344727\n",
      "Iteration 1651: loss -3.2218432426452637\n",
      "Iteration 1652: loss -3.0957770347595215\n",
      "Iteration 1653: loss -3.3799097537994385\n",
      "Iteration 1654: loss -3.2249186038970947\n",
      "Iteration 1655: loss -3.1200907230377197\n",
      "Iteration 1656: loss -3.2750906944274902\n",
      "Iteration 1657: loss -3.266172170639038\n",
      "Iteration 1658: loss -3.2889292240142822\n",
      "Iteration 1659: loss -3.3116517066955566\n",
      "Iteration 1660: loss -3.323737144470215\n",
      "Iteration 1661: loss -3.35943603515625\n",
      "Iteration 1662: loss -3.3767199516296387\n",
      "Iteration 1663: loss -3.301119089126587\n",
      "Iteration 1664: loss -3.3738176822662354\n",
      "Iteration 1665: loss -3.3221046924591064\n",
      "Iteration 1666: loss -3.20102596282959\n",
      "Iteration 1667: loss -3.351963996887207\n",
      "Iteration 1668: loss -3.169851779937744\n",
      "Iteration 1669: loss -3.3416974544525146\n",
      "Iteration 1670: loss -3.2271125316619873\n",
      "Iteration 1671: loss -3.1837737560272217\n",
      "Iteration 1672: loss -3.221907138824463\n",
      "Iteration 1673: loss -3.2414331436157227\n",
      "Iteration 1674: loss -3.1186728477478027\n",
      "Iteration 1675: loss -3.142199754714966\n",
      "Iteration 1676: loss -3.331996202468872\n",
      "Iteration 1677: loss -3.1402525901794434\n",
      "Iteration 1678: loss -3.2392795085906982\n",
      "Iteration 1679: loss -3.1579270362854004\n",
      "Iteration 1680: loss -3.089172601699829\n",
      "Iteration 1681: loss -3.304988384246826\n",
      "Iteration 1682: loss -3.367490291595459\n",
      "Iteration 1683: loss -3.2998359203338623\n",
      "Iteration 1684: loss -3.2804670333862305\n",
      "Iteration 1685: loss -3.3144829273223877\n",
      "Iteration 1686: loss -3.2803149223327637\n",
      "Iteration 1687: loss -3.232851505279541\n",
      "Iteration 1688: loss -3.2155961990356445\n",
      "Iteration 1689: loss -3.3046462535858154\n",
      "Iteration 1690: loss -3.0926098823547363\n",
      "Iteration 1691: loss -3.183407783508301\n",
      "Iteration 1692: loss -3.0620028972625732\n",
      "Iteration 1693: loss -3.255098581314087\n",
      "Iteration 1694: loss -3.0448498725891113\n",
      "Iteration 1695: loss -3.243727922439575\n",
      "Iteration 1696: loss -2.9978253841400146\n",
      "Iteration 1697: loss -3.1657135486602783\n",
      "Iteration 1698: loss -3.208425283432007\n",
      "Iteration 1699: loss -3.2907707691192627\n",
      "Iteration 1700: loss -3.1805837154388428\n",
      "Iteration 1701: loss -3.03214168548584\n",
      "Iteration 1702: loss -3.282595157623291\n",
      "Iteration 1703: loss -3.197204351425171\n",
      "Iteration 1704: loss -3.1982085704803467\n",
      "Iteration 1705: loss -3.1465539932250977\n",
      "Iteration 1706: loss -3.349470853805542\n",
      "Iteration 1707: loss -3.2483556270599365\n",
      "Iteration 1708: loss -3.289849281311035\n",
      "Iteration 1709: loss -3.279247522354126\n",
      "Iteration 1710: loss -3.310750722885132\n",
      "Iteration 1711: loss -3.3498194217681885\n",
      "Iteration 1712: loss -3.2278878688812256\n",
      "Iteration 1713: loss -3.282167434692383\n",
      "Iteration 1714: loss -3.348616361618042\n",
      "Iteration 1715: loss -3.291501522064209\n",
      "Iteration 1716: loss -3.227747917175293\n",
      "Iteration 1717: loss -3.2344090938568115\n",
      "Iteration 1718: loss -3.303952932357788\n",
      "Iteration 1719: loss -3.350484609603882\n",
      "Iteration 1720: loss -3.4469974040985107\n",
      "Iteration 1721: loss -3.3249151706695557\n",
      "Iteration 1722: loss -3.3714828491210938\n",
      "Iteration 1723: loss -3.2842354774475098\n",
      "Iteration 1724: loss -3.2551498413085938\n",
      "Iteration 1725: loss -3.3266570568084717\n",
      "Iteration 1726: loss -3.2881734371185303\n",
      "Iteration 1727: loss -3.1282269954681396\n",
      "Iteration 1728: loss -3.198991298675537\n",
      "Iteration 1729: loss -3.3140640258789062\n",
      "Iteration 1730: loss -3.2203071117401123\n",
      "Iteration 1731: loss -2.995083808898926\n",
      "Iteration 1732: loss -3.2087457180023193\n",
      "Iteration 1733: loss -3.178577184677124\n",
      "Iteration 1734: loss -3.3427655696868896\n",
      "Iteration 1735: loss -3.2270190715789795\n",
      "Iteration 1736: loss -3.282662868499756\n",
      "Iteration 1737: loss -3.3726563453674316\n",
      "Iteration 1738: loss -3.3088018894195557\n",
      "Iteration 1739: loss -3.3856699466705322\n",
      "Iteration 1740: loss -3.2864880561828613\n",
      "Iteration 1741: loss -3.2853634357452393\n",
      "Iteration 1742: loss -3.425895929336548\n",
      "Iteration 1743: loss -3.199805974960327\n",
      "Iteration 1744: loss -3.4315078258514404\n",
      "Iteration 1745: loss -3.3594253063201904\n",
      "Iteration 1746: loss -3.1925041675567627\n",
      "Iteration 1747: loss -3.2492835521698\n",
      "Iteration 1748: loss -3.094558000564575\n",
      "Iteration 1749: loss -3.3578081130981445\n",
      "Iteration 1750: loss -3.0919406414031982\n",
      "Iteration 1751: loss -3.358931303024292\n",
      "Iteration 1752: loss -3.1850814819335938\n",
      "Iteration 1753: loss -3.282637119293213\n",
      "Iteration 1754: loss -3.0415701866149902\n",
      "Iteration 1755: loss -3.1453986167907715\n",
      "Iteration 1756: loss -3.2533233165740967\n",
      "Iteration 1757: loss -3.079446315765381\n",
      "Iteration 1758: loss -3.2415904998779297\n",
      "Iteration 1759: loss -3.160597801208496\n",
      "Iteration 1760: loss -3.1390655040740967\n",
      "Iteration 1761: loss -3.422689437866211\n",
      "Iteration 1762: loss -3.124882221221924\n",
      "Iteration 1763: loss -3.296389579772949\n",
      "Iteration 1764: loss -3.196474552154541\n",
      "Iteration 1765: loss -3.047262668609619\n",
      "Iteration 1766: loss -3.2398264408111572\n",
      "Iteration 1767: loss -3.103543758392334\n",
      "Iteration 1768: loss -3.041243553161621\n",
      "Iteration 1769: loss -3.2251064777374268\n",
      "Iteration 1770: loss -3.102025032043457\n",
      "Iteration 1771: loss -3.3622164726257324\n",
      "Iteration 1772: loss -3.1288328170776367\n",
      "Iteration 1773: loss -3.0845117568969727\n",
      "Iteration 1774: loss -3.257828712463379\n",
      "Iteration 1775: loss -3.0416958332061768\n",
      "Iteration 1776: loss -3.0725162029266357\n",
      "Iteration 1777: loss -3.355722427368164\n",
      "Iteration 1778: loss -2.9806978702545166\n",
      "Iteration 1779: loss -3.217869281768799\n",
      "Iteration 1780: loss -3.21846866607666\n",
      "Iteration 1781: loss -3.036661148071289\n",
      "Iteration 1782: loss -3.128666877746582\n",
      "Iteration 1783: loss -3.219407558441162\n",
      "Iteration 1784: loss -3.3188815116882324\n",
      "Iteration 1785: loss -3.1880228519439697\n",
      "Iteration 1786: loss -3.060534954071045\n",
      "Iteration 1787: loss -3.3345611095428467\n",
      "Iteration 1788: loss -3.1468875408172607\n",
      "Iteration 1789: loss -3.2738451957702637\n",
      "Iteration 1790: loss -3.3045589923858643\n",
      "Iteration 1791: loss -3.277744770050049\n",
      "Iteration 1792: loss -3.0047245025634766\n",
      "Iteration 1793: loss -3.211198091506958\n",
      "Iteration 1794: loss -3.2290942668914795\n",
      "Iteration 1795: loss -3.0685994625091553\n",
      "Iteration 1796: loss -3.1825695037841797\n",
      "Iteration 1797: loss -3.318300724029541\n",
      "Iteration 1798: loss -3.1778738498687744\n",
      "Iteration 1799: loss -3.093414068222046\n",
      "Iteration 1800: loss -3.1815483570098877\n",
      "Iteration 1801: loss -3.379202365875244\n",
      "Iteration 1802: loss -3.08565616607666\n",
      "Iteration 1803: loss -3.2073168754577637\n",
      "Iteration 1804: loss -3.219424247741699\n",
      "Iteration 1805: loss -3.222343683242798\n",
      "Iteration 1806: loss -3.15470027923584\n",
      "Iteration 1807: loss -3.261322259902954\n",
      "Iteration 1808: loss -3.1720447540283203\n",
      "Iteration 1809: loss -3.125622510910034\n",
      "Iteration 1810: loss -3.3195016384124756\n",
      "Iteration 1811: loss -3.23062801361084\n",
      "Iteration 1812: loss -3.0285544395446777\n",
      "Iteration 1813: loss -3.3155975341796875\n",
      "Iteration 1814: loss -3.1450512409210205\n",
      "Iteration 1815: loss -3.0092809200286865\n",
      "Iteration 1816: loss -3.2755954265594482\n",
      "Iteration 1817: loss -3.338669538497925\n",
      "Iteration 1818: loss -3.252617835998535\n",
      "Iteration 1819: loss -3.106959342956543\n",
      "Iteration 1820: loss -3.110490322113037\n",
      "Iteration 1821: loss -3.008720636367798\n",
      "Iteration 1822: loss -3.2545053958892822\n",
      "Iteration 1823: loss -3.273054599761963\n",
      "Iteration 1824: loss -3.192049026489258\n",
      "Iteration 1825: loss -3.1284124851226807\n",
      "Iteration 1826: loss -3.2528738975524902\n",
      "Iteration 1827: loss -3.324877977371216\n",
      "Iteration 1828: loss -3.191312551498413\n",
      "Iteration 1829: loss -3.2144980430603027\n",
      "Iteration 1830: loss -3.3046839237213135\n",
      "Iteration 1831: loss -3.306426763534546\n",
      "Iteration 1832: loss -3.2682981491088867\n",
      "Iteration 1833: loss -3.2880163192749023\n",
      "Iteration 1834: loss -3.280531644821167\n",
      "Iteration 1835: loss -3.053262233734131\n",
      "Iteration 1836: loss -3.3193864822387695\n",
      "Iteration 1837: loss -3.319540023803711\n",
      "Iteration 1838: loss -3.238870859146118\n",
      "Iteration 1839: loss -3.3855481147766113\n",
      "Iteration 1840: loss -3.2720141410827637\n",
      "Iteration 1841: loss -3.243396520614624\n",
      "Iteration 1842: loss -3.253469467163086\n",
      "Iteration 1843: loss -3.2262206077575684\n",
      "Iteration 1844: loss -3.3054592609405518\n",
      "Iteration 1845: loss -3.359956741333008\n",
      "Iteration 1846: loss -3.380122661590576\n",
      "Iteration 1847: loss -3.445704936981201\n",
      "Iteration 1848: loss -3.312135696411133\n",
      "Iteration 1849: loss -3.3409836292266846\n",
      "Iteration 1850: loss -3.432161808013916\n",
      "Iteration 1851: loss -3.381420373916626\n",
      "Iteration 1852: loss -3.408463478088379\n",
      "Iteration 1853: loss -3.1312410831451416\n",
      "Iteration 1854: loss -3.384188175201416\n",
      "Iteration 1855: loss -3.292961359024048\n",
      "Iteration 1856: loss -3.38508677482605\n",
      "Iteration 1857: loss -3.404866933822632\n",
      "Iteration 1858: loss -3.201869010925293\n",
      "Iteration 1859: loss -3.1527299880981445\n",
      "Iteration 1860: loss -3.150056838989258\n",
      "Iteration 1861: loss -3.1242315769195557\n",
      "Iteration 1862: loss -3.3235507011413574\n",
      "Iteration 1863: loss -3.221616744995117\n",
      "Iteration 1864: loss -3.2710676193237305\n",
      "Iteration 1865: loss -3.2717154026031494\n",
      "Iteration 1866: loss -3.3782873153686523\n",
      "Iteration 1867: loss -3.312471389770508\n",
      "Iteration 1868: loss -3.271409273147583\n",
      "Iteration 1869: loss -3.236201763153076\n",
      "Iteration 1870: loss -3.1960690021514893\n",
      "Iteration 1871: loss -3.255263090133667\n",
      "Iteration 1872: loss -3.4826760292053223\n",
      "Iteration 1873: loss -3.311997175216675\n",
      "Iteration 1874: loss -3.386922597885132\n",
      "Iteration 1875: loss -2.953068494796753\n",
      "Iteration 1876: loss -3.3458340167999268\n",
      "Iteration 1877: loss -3.242903709411621\n",
      "Iteration 1878: loss -3.3170058727264404\n",
      "Iteration 1879: loss -3.1567792892456055\n",
      "Iteration 1880: loss -3.2109756469726562\n",
      "Iteration 1881: loss -3.1679537296295166\n",
      "Iteration 1882: loss -3.313105583190918\n",
      "Iteration 1883: loss -3.1722867488861084\n",
      "Iteration 1884: loss -3.1875362396240234\n",
      "Iteration 1885: loss -3.3098180294036865\n",
      "Iteration 1886: loss -3.4294869899749756\n",
      "Iteration 1887: loss -3.3295905590057373\n",
      "Iteration 1888: loss -3.0960190296173096\n",
      "Iteration 1889: loss -3.3484489917755127\n",
      "Iteration 1890: loss -2.735610246658325\n",
      "Iteration 1891: loss -3.2185065746307373\n",
      "Iteration 1892: loss -3.1761977672576904\n",
      "Iteration 1893: loss -2.9445083141326904\n",
      "Iteration 1894: loss -3.253617525100708\n",
      "Iteration 1895: loss -2.8466379642486572\n",
      "Iteration 1896: loss -3.0877370834350586\n",
      "Iteration 1897: loss -3.2538347244262695\n",
      "Iteration 1898: loss -3.188101291656494\n",
      "Iteration 1899: loss -3.2334189414978027\n",
      "Iteration 1900: loss -3.3991281986236572\n",
      "Iteration 1901: loss -3.0671732425689697\n",
      "Iteration 1902: loss -3.339766263961792\n",
      "Iteration 1903: loss -3.1953320503234863\n",
      "Iteration 1904: loss -3.191787004470825\n",
      "Iteration 1905: loss -3.3094747066497803\n",
      "Iteration 1906: loss -3.242007255554199\n",
      "Iteration 1907: loss -3.412001848220825\n",
      "Iteration 1908: loss -3.466557502746582\n",
      "Iteration 1909: loss -3.3217110633850098\n",
      "Iteration 1910: loss -3.3039684295654297\n",
      "Iteration 1911: loss -3.0674774646759033\n",
      "Iteration 1912: loss -3.2907538414001465\n",
      "Iteration 1913: loss -3.2287583351135254\n",
      "Iteration 1914: loss -3.360205888748169\n",
      "Iteration 1915: loss -3.301516532897949\n",
      "Iteration 1916: loss -3.16571044921875\n",
      "Iteration 1917: loss -3.2494957447052\n",
      "Iteration 1918: loss -3.1783571243286133\n",
      "Iteration 1919: loss -3.2473630905151367\n",
      "Iteration 1920: loss -3.1564440727233887\n",
      "Iteration 1921: loss -3.205029249191284\n",
      "Iteration 1922: loss -3.300654411315918\n",
      "Iteration 1923: loss -3.3898162841796875\n",
      "Iteration 1924: loss -3.310727596282959\n",
      "Iteration 1925: loss -3.3081789016723633\n",
      "Iteration 1926: loss -3.244032621383667\n",
      "Iteration 1927: loss -3.4559009075164795\n",
      "Iteration 1928: loss -3.493356943130493\n",
      "Iteration 1929: loss -3.345407009124756\n",
      "Iteration 1930: loss -3.3443610668182373\n",
      "Iteration 1931: loss -3.243765354156494\n",
      "Iteration 1932: loss -3.379032611846924\n",
      "Iteration 1933: loss -3.3765597343444824\n",
      "Iteration 1934: loss -3.1076366901397705\n",
      "Iteration 1935: loss -3.370914936065674\n",
      "Iteration 1936: loss -3.2627978324890137\n",
      "Iteration 1937: loss -3.2162113189697266\n",
      "Iteration 1938: loss -3.2437338829040527\n",
      "Iteration 1939: loss -3.223820209503174\n",
      "Iteration 1940: loss -3.4124863147735596\n",
      "Iteration 1941: loss -3.196810245513916\n",
      "Iteration 1942: loss -2.943931818008423\n",
      "Iteration 1943: loss -3.2535130977630615\n",
      "Iteration 1944: loss -2.848379135131836\n",
      "Iteration 1945: loss -3.0457656383514404\n",
      "Iteration 1946: loss -3.2092370986938477\n",
      "Iteration 1947: loss -3.3144073486328125\n",
      "Iteration 1948: loss -2.9569013118743896\n",
      "Iteration 1949: loss -3.52873158454895\n",
      "Iteration 1950: loss -3.1556856632232666\n",
      "Iteration 1951: loss -3.105966567993164\n",
      "Iteration 1952: loss -3.262892723083496\n",
      "Iteration 1953: loss -3.1679489612579346\n",
      "Iteration 1954: loss -3.3564789295196533\n",
      "Iteration 1955: loss -3.258127450942993\n",
      "Iteration 1956: loss -3.2991576194763184\n",
      "Iteration 1957: loss -3.1593017578125\n",
      "Iteration 1958: loss -3.303335666656494\n",
      "Iteration 1959: loss -3.329099178314209\n",
      "Iteration 1960: loss -3.320150136947632\n",
      "Iteration 1961: loss -3.313732862472534\n",
      "Iteration 1962: loss -3.067169189453125\n",
      "Iteration 1963: loss -3.322139263153076\n",
      "Iteration 1964: loss -3.295583486557007\n",
      "Iteration 1965: loss -3.002836227416992\n",
      "Iteration 1966: loss -3.137119770050049\n",
      "Iteration 1967: loss -3.2977077960968018\n",
      "Iteration 1968: loss -3.294834852218628\n",
      "Iteration 1969: loss -3.2275495529174805\n",
      "Iteration 1970: loss -3.374809980392456\n",
      "Iteration 1971: loss -3.2484421730041504\n",
      "Iteration 1972: loss -3.2812328338623047\n",
      "Iteration 1973: loss -3.2465569972991943\n",
      "Iteration 1974: loss -3.313717842102051\n",
      "Iteration 1975: loss -3.190844774246216\n",
      "Iteration 1976: loss -3.2434945106506348\n",
      "Iteration 1977: loss -3.379316806793213\n",
      "Iteration 1978: loss -3.2481472492218018\n",
      "Iteration 1979: loss -3.1651411056518555\n",
      "Iteration 1980: loss -3.3677704334259033\n",
      "Iteration 1981: loss -3.0883960723876953\n",
      "Iteration 1982: loss -3.2053704261779785\n",
      "Iteration 1983: loss -3.3749191761016846\n",
      "Iteration 1984: loss -3.295855760574341\n",
      "Iteration 1985: loss -3.316087007522583\n",
      "Iteration 1986: loss -3.4486894607543945\n",
      "Iteration 1987: loss -3.3478498458862305\n",
      "Iteration 1988: loss -3.2365267276763916\n",
      "Iteration 1989: loss -3.3923752307891846\n",
      "Iteration 1990: loss -3.1948325634002686\n",
      "Iteration 1991: loss -3.303046464920044\n",
      "Iteration 1992: loss -3.246769428253174\n",
      "Iteration 1993: loss -3.452143907546997\n",
      "Iteration 1994: loss -3.2186553478240967\n",
      "Iteration 1995: loss -3.3089325428009033\n",
      "Iteration 1996: loss -3.1613070964813232\n",
      "Iteration 1997: loss -3.2590625286102295\n",
      "Iteration 1998: loss -3.433293104171753\n",
      "Iteration 1999: loss -3.1481587886810303\n",
      "Iteration 2000: loss -3.224958896636963\n",
      "Iteration 2001: loss -3.284935712814331\n",
      "Iteration 2002: loss -3.1467370986938477\n",
      "Iteration 2003: loss -2.943438768386841\n",
      "Iteration 2004: loss -3.230055332183838\n",
      "Iteration 2005: loss -3.2911550998687744\n",
      "Iteration 2006: loss -2.9026217460632324\n",
      "Iteration 2007: loss -3.241361141204834\n",
      "Iteration 2008: loss -3.1510000228881836\n",
      "Iteration 2009: loss -3.1021034717559814\n",
      "Iteration 2010: loss -3.2768568992614746\n",
      "Iteration 2011: loss -3.2307097911834717\n",
      "Iteration 2012: loss -3.2439124584198\n",
      "Iteration 2013: loss -3.2598531246185303\n",
      "Iteration 2014: loss -3.162294864654541\n",
      "Iteration 2015: loss -3.3469724655151367\n",
      "Iteration 2016: loss -3.25382661819458\n",
      "Iteration 2017: loss -3.4324471950531006\n",
      "Iteration 2018: loss -3.266249895095825\n",
      "Iteration 2019: loss -3.1223039627075195\n",
      "Iteration 2020: loss -3.127302646636963\n",
      "Iteration 2021: loss -3.2598049640655518\n",
      "Iteration 2022: loss -3.4208474159240723\n",
      "Iteration 2023: loss -3.4051332473754883\n",
      "Iteration 2024: loss -3.3979651927948\n",
      "Iteration 2025: loss -3.2786054611206055\n",
      "Iteration 2026: loss -3.0741147994995117\n",
      "Iteration 2027: loss -3.2319512367248535\n",
      "Iteration 2028: loss -3.2780370712280273\n",
      "Iteration 2029: loss -3.3726084232330322\n",
      "Iteration 2030: loss -3.1766626834869385\n",
      "Iteration 2031: loss -3.2633090019226074\n",
      "Iteration 2032: loss -3.1929874420166016\n",
      "Iteration 2033: loss -3.4242660999298096\n",
      "Iteration 2034: loss -3.3184211254119873\n",
      "Iteration 2035: loss -3.2911782264709473\n",
      "Iteration 2036: loss -3.462507724761963\n",
      "Iteration 2037: loss -3.426781415939331\n",
      "Iteration 2038: loss -3.2650604248046875\n",
      "Iteration 2039: loss -3.232865571975708\n",
      "Iteration 2040: loss -3.2861733436584473\n",
      "Iteration 2041: loss -3.2495522499084473\n",
      "Iteration 2042: loss -3.1503472328186035\n",
      "Iteration 2043: loss -3.244504928588867\n",
      "Iteration 2044: loss -3.407421350479126\n",
      "Iteration 2045: loss -3.1629157066345215\n",
      "Iteration 2046: loss -3.326101303100586\n",
      "Iteration 2047: loss -3.351372718811035\n",
      "Iteration 2048: loss -3.4595110416412354\n",
      "Iteration 2049: loss -3.3584768772125244\n",
      "Iteration 2050: loss -3.276365041732788\n",
      "Iteration 2051: loss -3.382164239883423\n",
      "Iteration 2052: loss -3.2453882694244385\n",
      "Iteration 2053: loss -3.290830135345459\n",
      "Iteration 2054: loss -3.2503392696380615\n",
      "Iteration 2055: loss -3.327298641204834\n",
      "Iteration 2056: loss -3.3122949600219727\n",
      "Iteration 2057: loss -3.149578332901001\n",
      "Iteration 2058: loss -3.4125771522521973\n",
      "Iteration 2059: loss -3.3408029079437256\n",
      "Iteration 2060: loss -3.322226047515869\n",
      "Iteration 2061: loss -3.346315860748291\n",
      "Iteration 2062: loss -3.304011821746826\n",
      "Iteration 2063: loss -3.1446216106414795\n",
      "Iteration 2064: loss -3.3474435806274414\n",
      "Iteration 2065: loss -3.2321367263793945\n",
      "Iteration 2066: loss -3.0821781158447266\n",
      "Iteration 2067: loss -3.3698580265045166\n",
      "Iteration 2068: loss -3.263906240463257\n",
      "Iteration 2069: loss -3.3065149784088135\n",
      "Iteration 2070: loss -3.1771297454833984\n",
      "Iteration 2071: loss -3.0992980003356934\n",
      "Iteration 2072: loss -3.116525173187256\n",
      "Iteration 2073: loss -3.275874614715576\n",
      "Iteration 2074: loss -3.1343798637390137\n",
      "Iteration 2075: loss -3.1929738521575928\n",
      "Iteration 2076: loss -3.2196741104125977\n",
      "Iteration 2077: loss -3.083150863647461\n",
      "Iteration 2078: loss -3.2703890800476074\n",
      "Iteration 2079: loss -3.19673228263855\n",
      "Iteration 2080: loss -2.837392568588257\n",
      "Iteration 2081: loss -3.4390745162963867\n",
      "Iteration 2082: loss -2.68727970123291\n",
      "Iteration 2083: loss -3.286181926727295\n",
      "Iteration 2084: loss -3.23786997795105\n",
      "Iteration 2085: loss -3.141371488571167\n",
      "Iteration 2086: loss -3.16788911819458\n",
      "Iteration 2087: loss -3.2595865726470947\n",
      "Iteration 2088: loss -3.3040685653686523\n",
      "Iteration 2089: loss -3.2462899684906006\n",
      "Iteration 2090: loss -3.2595930099487305\n",
      "Iteration 2091: loss -3.1719467639923096\n",
      "Iteration 2092: loss -2.9563632011413574\n",
      "Iteration 2093: loss -3.317659854888916\n",
      "Iteration 2094: loss -3.30049204826355\n",
      "Iteration 2095: loss -3.364255905151367\n",
      "Iteration 2096: loss -3.1573967933654785\n",
      "Iteration 2097: loss -2.9682629108428955\n",
      "Iteration 2098: loss -3.245140314102173\n",
      "Iteration 2099: loss -3.295020818710327\n",
      "Iteration 2100: loss -3.3532321453094482\n",
      "Iteration 2101: loss -3.293513298034668\n",
      "Iteration 2102: loss -3.099713087081909\n",
      "Iteration 2103: loss -3.3034279346466064\n",
      "Iteration 2104: loss -3.356234073638916\n",
      "Iteration 2105: loss -3.228705644607544\n",
      "Iteration 2106: loss -3.3292276859283447\n",
      "Iteration 2107: loss -3.4898898601531982\n",
      "Iteration 2108: loss -3.2785775661468506\n",
      "Iteration 2109: loss -3.2316956520080566\n",
      "Iteration 2110: loss -3.11265230178833\n",
      "Iteration 2111: loss -3.333437919616699\n",
      "Iteration 2112: loss -3.25028920173645\n",
      "Iteration 2113: loss -3.3457326889038086\n",
      "Iteration 2114: loss -3.0860302448272705\n",
      "Iteration 2115: loss -3.136672258377075\n",
      "Iteration 2116: loss -3.209488868713379\n",
      "Iteration 2117: loss -3.356865882873535\n",
      "Iteration 2118: loss -3.119677782058716\n",
      "Iteration 2119: loss -3.3780689239501953\n",
      "Iteration 2120: loss -3.190958261489868\n",
      "Iteration 2121: loss -3.284562349319458\n",
      "Iteration 2122: loss -3.272705078125\n",
      "Iteration 2123: loss -3.321460247039795\n",
      "Iteration 2124: loss -3.380598783493042\n",
      "Iteration 2125: loss -3.4859237670898438\n",
      "Iteration 2126: loss -3.2058792114257812\n",
      "Iteration 2127: loss -3.376237392425537\n",
      "Iteration 2128: loss -3.4257047176361084\n",
      "Iteration 2129: loss -3.2426841259002686\n",
      "Iteration 2130: loss -3.3177247047424316\n",
      "Iteration 2131: loss -3.3580286502838135\n",
      "Iteration 2132: loss -3.1571011543273926\n",
      "Iteration 2133: loss -3.355011224746704\n",
      "Iteration 2134: loss -3.480368137359619\n",
      "Iteration 2135: loss -3.1501762866973877\n",
      "Iteration 2136: loss -3.3230342864990234\n",
      "Iteration 2137: loss -3.3752875328063965\n",
      "Iteration 2138: loss -3.199662685394287\n",
      "Iteration 2139: loss -3.372112512588501\n",
      "Iteration 2140: loss -3.2298941612243652\n",
      "Iteration 2141: loss -3.1265764236450195\n",
      "Iteration 2142: loss -3.249821662902832\n",
      "Iteration 2143: loss -3.266662836074829\n",
      "Iteration 2144: loss -3.2360174655914307\n",
      "Iteration 2145: loss -3.3968963623046875\n",
      "Iteration 2146: loss -3.1286065578460693\n",
      "Iteration 2147: loss -3.3513522148132324\n",
      "Iteration 2148: loss -3.306988000869751\n",
      "Iteration 2149: loss -3.3654675483703613\n",
      "Iteration 2150: loss -3.3539562225341797\n",
      "Iteration 2151: loss -3.363776922225952\n",
      "Iteration 2152: loss -3.169633150100708\n",
      "Iteration 2153: loss -3.3682374954223633\n",
      "Iteration 2154: loss -3.44429087638855\n",
      "Iteration 2155: loss -3.316281318664551\n",
      "Iteration 2156: loss -3.3832497596740723\n",
      "Iteration 2157: loss -3.1225838661193848\n",
      "Iteration 2158: loss -3.2625293731689453\n",
      "Iteration 2159: loss -3.223454475402832\n",
      "Iteration 2160: loss -3.1150903701782227\n",
      "Iteration 2161: loss -3.4285311698913574\n",
      "Iteration 2162: loss -3.071800470352173\n",
      "Iteration 2163: loss -2.7351717948913574\n",
      "Iteration 2164: loss -3.1586806774139404\n",
      "Iteration 2165: loss -3.0484931468963623\n",
      "Iteration 2166: loss -3.2595784664154053\n",
      "Iteration 2167: loss -3.178847551345825\n",
      "Iteration 2168: loss -2.918792724609375\n",
      "Iteration 2169: loss -3.0745151042938232\n",
      "Iteration 2170: loss -3.2080626487731934\n",
      "Iteration 2171: loss -3.167818069458008\n",
      "Iteration 2172: loss -3.324252128601074\n",
      "Iteration 2173: loss -3.309201717376709\n",
      "Iteration 2174: loss -3.3775603771209717\n",
      "Iteration 2175: loss -3.441464900970459\n",
      "Iteration 2176: loss -3.394899845123291\n",
      "Iteration 2177: loss -3.2695207595825195\n",
      "Iteration 2178: loss -3.3594274520874023\n",
      "Iteration 2179: loss -3.282095432281494\n",
      "Iteration 2180: loss -3.2334482669830322\n",
      "Iteration 2181: loss -3.3917124271392822\n",
      "Iteration 2182: loss -3.355785608291626\n",
      "Iteration 2183: loss -3.3004631996154785\n",
      "Iteration 2184: loss -3.3139286041259766\n",
      "Iteration 2185: loss -3.337924003601074\n",
      "Iteration 2186: loss -3.2023677825927734\n",
      "Iteration 2187: loss -3.2551584243774414\n",
      "Iteration 2188: loss -3.2461583614349365\n",
      "Iteration 2189: loss -3.125516653060913\n",
      "Iteration 2190: loss -3.3660900592803955\n",
      "Iteration 2191: loss -3.359348773956299\n",
      "Iteration 2192: loss -3.3161635398864746\n",
      "Iteration 2193: loss -3.5019240379333496\n",
      "Iteration 2194: loss -3.3857574462890625\n",
      "Iteration 2195: loss -3.4047060012817383\n",
      "Iteration 2196: loss -3.3388991355895996\n",
      "Iteration 2197: loss -3.1464879512786865\n",
      "Iteration 2198: loss -3.2112948894500732\n",
      "Iteration 2199: loss -3.1635236740112305\n",
      "Iteration 2200: loss -3.367084264755249\n",
      "Iteration 2201: loss -3.1745972633361816\n",
      "Iteration 2202: loss -3.4283740520477295\n",
      "Iteration 2203: loss -3.3498458862304688\n",
      "Iteration 2204: loss -3.314439296722412\n",
      "Iteration 2205: loss -3.2864584922790527\n",
      "Iteration 2206: loss -3.3662798404693604\n",
      "Iteration 2207: loss -3.3917009830474854\n",
      "Iteration 2208: loss -3.3704137802124023\n",
      "Iteration 2209: loss -3.3725497722625732\n",
      "Iteration 2210: loss -3.306140661239624\n",
      "Iteration 2211: loss -3.3720648288726807\n",
      "Iteration 2212: loss -3.201322078704834\n",
      "Iteration 2213: loss -3.312441349029541\n",
      "Iteration 2214: loss -3.0803825855255127\n",
      "Iteration 2215: loss -3.392504930496216\n",
      "Iteration 2216: loss -3.382200002670288\n",
      "Iteration 2217: loss -3.1585381031036377\n",
      "Iteration 2218: loss -3.2190566062927246\n",
      "Iteration 2219: loss -3.205961227416992\n",
      "Iteration 2220: loss -3.3520004749298096\n",
      "Iteration 2221: loss -3.2981629371643066\n",
      "Iteration 2222: loss -3.2548773288726807\n",
      "Iteration 2223: loss -3.441434621810913\n",
      "Iteration 2224: loss -3.3564798831939697\n",
      "Iteration 2225: loss -3.340327739715576\n",
      "Iteration 2226: loss -3.412846088409424\n",
      "Iteration 2227: loss -3.323598623275757\n",
      "Iteration 2228: loss -3.286362648010254\n",
      "Iteration 2229: loss -3.4533815383911133\n",
      "Iteration 2230: loss -3.371425151824951\n",
      "Iteration 2231: loss -3.3358659744262695\n",
      "Iteration 2232: loss -3.333441734313965\n",
      "Iteration 2233: loss -3.340696334838867\n",
      "Iteration 2234: loss -3.465451240539551\n",
      "Iteration 2235: loss -3.4500627517700195\n",
      "Iteration 2236: loss -3.3796873092651367\n",
      "Iteration 2237: loss -3.370054244995117\n",
      "Iteration 2238: loss -3.3177621364593506\n",
      "Iteration 2239: loss -3.1075992584228516\n",
      "Iteration 2240: loss -3.2574856281280518\n",
      "Iteration 2241: loss -3.353123188018799\n",
      "Iteration 2242: loss -3.3359274864196777\n",
      "Iteration 2243: loss -3.3461365699768066\n",
      "Iteration 2244: loss -3.3092942237854004\n",
      "Iteration 2245: loss -3.028491258621216\n",
      "Iteration 2246: loss -3.51741886138916\n",
      "Iteration 2247: loss -3.3475964069366455\n",
      "Iteration 2248: loss -3.3560309410095215\n",
      "Iteration 2249: loss -3.361870765686035\n",
      "Iteration 2250: loss -3.0131676197052\n",
      "Iteration 2251: loss -3.18302059173584\n",
      "Iteration 2252: loss -3.2964892387390137\n",
      "Iteration 2253: loss -2.982759952545166\n",
      "Iteration 2254: loss -3.4868273735046387\n",
      "Iteration 2255: loss -3.287029504776001\n",
      "Iteration 2256: loss -3.1847176551818848\n",
      "Iteration 2257: loss -3.538418769836426\n",
      "Iteration 2258: loss -3.2026450634002686\n",
      "Iteration 2259: loss -3.1358401775360107\n",
      "Iteration 2260: loss -3.2362616062164307\n",
      "Iteration 2261: loss -3.2102017402648926\n",
      "Iteration 2262: loss -3.3361101150512695\n",
      "Iteration 2263: loss -3.0788967609405518\n",
      "Iteration 2264: loss -3.171748399734497\n",
      "Iteration 2265: loss -3.1595406532287598\n",
      "Iteration 2266: loss -3.351423740386963\n",
      "Iteration 2267: loss -3.1968960762023926\n",
      "Iteration 2268: loss -3.2327380180358887\n",
      "Iteration 2269: loss -3.2824270725250244\n",
      "Iteration 2270: loss -3.2135753631591797\n",
      "Iteration 2271: loss -3.047018527984619\n",
      "Iteration 2272: loss -3.2289512157440186\n",
      "Iteration 2273: loss -3.286785840988159\n",
      "Iteration 2274: loss -3.274282455444336\n",
      "Iteration 2275: loss -3.0702338218688965\n",
      "Iteration 2276: loss -3.1697139739990234\n",
      "Iteration 2277: loss -3.2418372631073\n",
      "Iteration 2278: loss -3.24275279045105\n",
      "Iteration 2279: loss -3.1229867935180664\n",
      "Iteration 2280: loss -3.3903555870056152\n",
      "Iteration 2281: loss -3.1357436180114746\n",
      "Iteration 2282: loss -3.3895936012268066\n",
      "Iteration 2283: loss -3.3364920616149902\n",
      "Iteration 2284: loss -3.195521593093872\n",
      "Iteration 2285: loss -3.396019220352173\n",
      "Iteration 2286: loss -3.2556846141815186\n",
      "Iteration 2287: loss -3.2113189697265625\n",
      "Iteration 2288: loss -3.5116677284240723\n",
      "Iteration 2289: loss -3.265887498855591\n",
      "Iteration 2290: loss -2.9551022052764893\n",
      "Iteration 2291: loss -3.469144344329834\n",
      "Iteration 2292: loss -3.2856884002685547\n",
      "Iteration 2293: loss -3.3254852294921875\n",
      "Iteration 2294: loss -3.1610686779022217\n",
      "Iteration 2295: loss -3.2509021759033203\n",
      "Iteration 2296: loss -3.373272657394409\n",
      "Iteration 2297: loss -3.2010653018951416\n",
      "Iteration 2298: loss -3.237236261367798\n",
      "Iteration 2299: loss -3.2016217708587646\n",
      "Iteration 2300: loss -3.084947109222412\n",
      "Iteration 2301: loss -3.0556116104125977\n",
      "Iteration 2302: loss -3.2372283935546875\n",
      "Iteration 2303: loss -3.348345994949341\n",
      "Iteration 2304: loss -3.1535303592681885\n",
      "Iteration 2305: loss -3.377560615539551\n",
      "Iteration 2306: loss -3.282350540161133\n",
      "Iteration 2307: loss -2.979053258895874\n",
      "Iteration 2308: loss -3.1793253421783447\n",
      "Iteration 2309: loss -3.37797212600708\n",
      "Iteration 2310: loss -3.3723666667938232\n",
      "Iteration 2311: loss -3.240426540374756\n",
      "Iteration 2312: loss -3.170778751373291\n",
      "Iteration 2313: loss -3.3443944454193115\n",
      "Iteration 2314: loss -3.237375497817993\n",
      "Iteration 2315: loss -3.3913705348968506\n",
      "Iteration 2316: loss -3.2880218029022217\n",
      "Iteration 2317: loss -3.216304063796997\n",
      "Iteration 2318: loss -3.1975696086883545\n",
      "Iteration 2319: loss -3.2121877670288086\n",
      "Iteration 2320: loss -3.2022368907928467\n",
      "Iteration 2321: loss -3.3184995651245117\n",
      "Iteration 2322: loss -3.4536173343658447\n",
      "Iteration 2323: loss -3.3609085083007812\n",
      "Iteration 2324: loss -3.5368874073028564\n",
      "Iteration 2325: loss -3.2737152576446533\n",
      "Iteration 2326: loss -3.3748233318328857\n",
      "Iteration 2327: loss -3.102905035018921\n",
      "Iteration 2328: loss -3.469982862472534\n",
      "Iteration 2329: loss -3.398927688598633\n",
      "Iteration 2330: loss -3.3414034843444824\n",
      "Iteration 2331: loss -3.3386752605438232\n",
      "Iteration 2332: loss -3.3284835815429688\n",
      "Iteration 2333: loss -3.2486400604248047\n",
      "Iteration 2334: loss -3.203099012374878\n",
      "Iteration 2335: loss -3.3275351524353027\n",
      "Iteration 2336: loss -3.1553869247436523\n",
      "Iteration 2337: loss -3.2213613986968994\n",
      "Iteration 2338: loss -3.3194546699523926\n",
      "Iteration 2339: loss -3.1960043907165527\n",
      "Iteration 2340: loss -3.2621898651123047\n",
      "Iteration 2341: loss -3.3486719131469727\n",
      "Iteration 2342: loss -3.2774877548217773\n",
      "Iteration 2343: loss -3.385066270828247\n",
      "Iteration 2344: loss -3.4660422801971436\n",
      "Iteration 2345: loss -3.2506518363952637\n",
      "Iteration 2346: loss -2.9754533767700195\n",
      "Iteration 2347: loss -3.3254568576812744\n",
      "Iteration 2348: loss -3.4038944244384766\n",
      "Iteration 2349: loss -3.094905376434326\n",
      "Iteration 2350: loss -3.2992825508117676\n",
      "Iteration 2351: loss -3.376204252243042\n",
      "Iteration 2352: loss -3.0368564128875732\n",
      "Iteration 2353: loss -3.3349609375\n",
      "Iteration 2354: loss -3.315708875656128\n",
      "Iteration 2355: loss -3.316141366958618\n",
      "Iteration 2356: loss -3.2517919540405273\n",
      "Iteration 2357: loss -3.327765464782715\n",
      "Iteration 2358: loss -3.3581838607788086\n",
      "Iteration 2359: loss -3.4035263061523438\n",
      "Iteration 2360: loss -3.2084293365478516\n",
      "Iteration 2361: loss -3.4267454147338867\n",
      "Iteration 2362: loss -3.2952990531921387\n",
      "Iteration 2363: loss -3.3798389434814453\n",
      "Iteration 2364: loss -3.368748664855957\n",
      "Iteration 2365: loss -3.3000447750091553\n",
      "Iteration 2366: loss -3.47049617767334\n",
      "Iteration 2367: loss -3.3797385692596436\n",
      "Iteration 2368: loss -3.304722785949707\n",
      "Iteration 2369: loss -3.378474712371826\n",
      "Iteration 2370: loss -3.3328745365142822\n",
      "Iteration 2371: loss -3.332425832748413\n",
      "Iteration 2372: loss -3.381786584854126\n",
      "Iteration 2373: loss -3.1643521785736084\n",
      "Iteration 2374: loss -3.417168617248535\n",
      "Iteration 2375: loss -3.179812431335449\n",
      "Iteration 2376: loss -3.3816847801208496\n",
      "Iteration 2377: loss -3.1769258975982666\n",
      "Iteration 2378: loss -3.090155839920044\n",
      "Iteration 2379: loss -3.199533700942993\n",
      "Iteration 2380: loss -3.2818098068237305\n",
      "Iteration 2381: loss -3.3155455589294434\n",
      "Iteration 2382: loss -3.345182180404663\n",
      "Iteration 2383: loss -3.407658576965332\n",
      "Iteration 2384: loss -3.460442543029785\n",
      "Iteration 2385: loss -3.2352211475372314\n",
      "Iteration 2386: loss -3.427701950073242\n",
      "Iteration 2387: loss -3.2303307056427\n",
      "Iteration 2388: loss -3.3837084770202637\n",
      "Iteration 2389: loss -3.2555902004241943\n",
      "Iteration 2390: loss -3.29451847076416\n",
      "Iteration 2391: loss -3.4687938690185547\n",
      "Iteration 2392: loss -3.235560655593872\n",
      "Iteration 2393: loss -3.3754453659057617\n",
      "Iteration 2394: loss -3.352851629257202\n",
      "Iteration 2395: loss -3.352710008621216\n",
      "Iteration 2396: loss -3.178142786026001\n",
      "Iteration 2397: loss -3.3172194957733154\n",
      "Iteration 2398: loss -3.3026721477508545\n",
      "Iteration 2399: loss -3.3193540573120117\n",
      "Iteration 2400: loss -3.2495028972625732\n",
      "Iteration 2401: loss -3.387315273284912\n",
      "Iteration 2402: loss -3.4451217651367188\n",
      "Iteration 2403: loss -3.112347364425659\n",
      "Iteration 2404: loss -3.2843635082244873\n",
      "Iteration 2405: loss -3.149622917175293\n",
      "Iteration 2406: loss -3.291356086730957\n",
      "Iteration 2407: loss -3.3768820762634277\n",
      "Iteration 2408: loss -3.0039315223693848\n",
      "Iteration 2409: loss -3.182985544204712\n",
      "Iteration 2410: loss -3.2874374389648438\n",
      "Iteration 2411: loss -3.1025781631469727\n",
      "Iteration 2412: loss -3.349672555923462\n",
      "Iteration 2413: loss -3.156517267227173\n",
      "Iteration 2414: loss -3.2013890743255615\n",
      "Iteration 2415: loss -3.2262580394744873\n",
      "Iteration 2416: loss -3.3612706661224365\n",
      "Iteration 2417: loss -3.3686704635620117\n",
      "Iteration 2418: loss -3.3901727199554443\n",
      "Iteration 2419: loss -3.4237399101257324\n",
      "Iteration 2420: loss -3.168050765991211\n",
      "Iteration 2421: loss -3.284555673599243\n",
      "Iteration 2422: loss -3.4323229789733887\n",
      "Iteration 2423: loss -3.396503210067749\n",
      "Iteration 2424: loss -3.335806369781494\n",
      "Iteration 2425: loss -3.414870500564575\n",
      "Iteration 2426: loss -3.2413580417633057\n",
      "Iteration 2427: loss -3.4671242237091064\n",
      "Iteration 2428: loss -3.4029505252838135\n",
      "Iteration 2429: loss -3.2530505657196045\n",
      "Iteration 2430: loss -3.404059410095215\n",
      "Iteration 2431: loss -3.371351957321167\n",
      "Iteration 2432: loss -3.4318857192993164\n",
      "Iteration 2433: loss -3.2555158138275146\n",
      "Iteration 2434: loss -3.329759120941162\n",
      "Iteration 2435: loss -3.321105718612671\n",
      "Iteration 2436: loss -3.2939319610595703\n",
      "Iteration 2437: loss -3.4435667991638184\n",
      "Iteration 2438: loss -3.3134405612945557\n",
      "Iteration 2439: loss -3.536206007003784\n",
      "Iteration 2440: loss -3.39959716796875\n",
      "Iteration 2441: loss -3.229496717453003\n",
      "Iteration 2442: loss -3.3747689723968506\n",
      "Iteration 2443: loss -3.2747817039489746\n",
      "Iteration 2444: loss -3.247365951538086\n",
      "Iteration 2445: loss -3.318647861480713\n",
      "Iteration 2446: loss -3.4666051864624023\n",
      "Iteration 2447: loss -3.2353503704071045\n",
      "Iteration 2448: loss -3.0265233516693115\n",
      "Iteration 2449: loss -3.3696985244750977\n",
      "Iteration 2450: loss -3.254714012145996\n",
      "Iteration 2451: loss -3.235163927078247\n",
      "Iteration 2452: loss -3.386772394180298\n",
      "Iteration 2453: loss -3.2507314682006836\n",
      "Iteration 2454: loss -3.316493272781372\n",
      "Iteration 2455: loss -3.337428092956543\n",
      "Iteration 2456: loss -3.069303512573242\n",
      "Iteration 2457: loss -3.401315927505493\n",
      "Iteration 2458: loss -3.1614067554473877\n",
      "Iteration 2459: loss -3.1143176555633545\n",
      "Iteration 2460: loss -3.3740453720092773\n",
      "Iteration 2461: loss -3.2849740982055664\n",
      "Iteration 2462: loss -3.3203160762786865\n",
      "Iteration 2463: loss -3.433713436126709\n",
      "Iteration 2464: loss -3.255657434463501\n",
      "Iteration 2465: loss -3.239108085632324\n",
      "Iteration 2466: loss -3.3657214641571045\n",
      "Iteration 2467: loss -3.2817742824554443\n",
      "Iteration 2468: loss -3.126739263534546\n",
      "Iteration 2469: loss -3.3732986450195312\n",
      "Iteration 2470: loss -3.1697661876678467\n",
      "Iteration 2471: loss -2.869563579559326\n",
      "Iteration 2472: loss -3.396872043609619\n",
      "Iteration 2473: loss -3.0903573036193848\n",
      "Iteration 2474: loss -3.237679958343506\n",
      "Iteration 2475: loss -3.1807632446289062\n",
      "Iteration 2476: loss -3.220607280731201\n",
      "Iteration 2477: loss -3.266723394393921\n",
      "Iteration 2478: loss -3.269064426422119\n",
      "Iteration 2479: loss -3.1580045223236084\n",
      "Iteration 2480: loss -3.1956329345703125\n",
      "Iteration 2481: loss -2.909928560256958\n",
      "Iteration 2482: loss -3.0263304710388184\n",
      "Iteration 2483: loss -3.110198974609375\n",
      "Iteration 2484: loss -3.026881217956543\n",
      "Iteration 2485: loss -3.232198476791382\n",
      "Iteration 2486: loss -2.6468491554260254\n",
      "Iteration 2487: loss -3.161398410797119\n",
      "Iteration 2488: loss -3.2911696434020996\n",
      "Iteration 2489: loss -2.9529144763946533\n",
      "Iteration 2490: loss -2.9258713722229004\n",
      "Iteration 2491: loss -3.1301467418670654\n",
      "Iteration 2492: loss -3.24429988861084\n",
      "Iteration 2493: loss -3.0583677291870117\n",
      "Iteration 2494: loss -3.0958263874053955\n",
      "Iteration 2495: loss -3.2371513843536377\n",
      "Iteration 2496: loss -3.339190721511841\n",
      "Iteration 2497: loss -3.197134494781494\n",
      "Iteration 2498: loss -3.145224094390869\n",
      "Iteration 2499: loss -3.261611223220825\n",
      "Iteration 2500: loss -3.2302932739257812\n",
      "Iteration 2501: loss -3.2671945095062256\n",
      "Iteration 2502: loss -3.29599666595459\n",
      "Iteration 2503: loss -3.2702443599700928\n",
      "Iteration 2504: loss -3.2997043132781982\n",
      "Iteration 2505: loss -3.229537010192871\n",
      "Iteration 2506: loss -3.379956007003784\n",
      "Iteration 2507: loss -3.2613115310668945\n",
      "Iteration 2508: loss -3.1960511207580566\n",
      "Iteration 2509: loss -3.286781072616577\n",
      "Iteration 2510: loss -3.382023334503174\n",
      "Iteration 2511: loss -3.183483839035034\n",
      "Iteration 2512: loss -3.319321870803833\n",
      "Iteration 2513: loss -3.3390488624572754\n",
      "Iteration 2514: loss -3.2830419540405273\n",
      "Iteration 2515: loss -3.3490023612976074\n",
      "Iteration 2516: loss -3.387282609939575\n",
      "Iteration 2517: loss -3.4579901695251465\n",
      "Iteration 2518: loss -3.3336141109466553\n",
      "Iteration 2519: loss -3.2234957218170166\n",
      "Iteration 2520: loss -3.357131004333496\n",
      "Iteration 2521: loss -3.220210075378418\n",
      "Iteration 2522: loss -3.3305559158325195\n",
      "Iteration 2523: loss -3.1245718002319336\n",
      "Iteration 2524: loss -3.2858190536499023\n",
      "Iteration 2525: loss -3.282789707183838\n",
      "Iteration 2526: loss -3.3577394485473633\n",
      "Iteration 2527: loss -3.3868954181671143\n",
      "Iteration 2528: loss -3.483790397644043\n",
      "Iteration 2529: loss -3.406242609024048\n",
      "Iteration 2530: loss -3.450679302215576\n",
      "Iteration 2531: loss -3.3218436241149902\n",
      "Iteration 2532: loss -3.336860418319702\n",
      "Iteration 2533: loss -3.0400588512420654\n",
      "Iteration 2534: loss -3.540424108505249\n",
      "Iteration 2535: loss -3.319265842437744\n",
      "Iteration 2536: loss -3.3164234161376953\n",
      "Iteration 2537: loss -3.4107728004455566\n",
      "Iteration 2538: loss -3.32177734375\n",
      "Iteration 2539: loss -3.4761791229248047\n",
      "Iteration 2540: loss -3.289027690887451\n",
      "Iteration 2541: loss -3.437784194946289\n",
      "Iteration 2542: loss -3.3469109535217285\n",
      "Iteration 2543: loss -3.358989953994751\n",
      "Iteration 2544: loss -3.3004138469696045\n",
      "Iteration 2545: loss -3.347896099090576\n",
      "Iteration 2546: loss -3.292020320892334\n",
      "Iteration 2547: loss -3.3199262619018555\n",
      "Iteration 2548: loss -3.286452054977417\n",
      "Iteration 2549: loss -3.3057610988616943\n",
      "Iteration 2550: loss -3.2549688816070557\n",
      "Iteration 2551: loss -3.3348593711853027\n",
      "Iteration 2552: loss -3.2721118927001953\n",
      "Iteration 2553: loss -3.3405213356018066\n",
      "Iteration 2554: loss -3.3463213443756104\n",
      "Iteration 2555: loss -3.4241602420806885\n",
      "Iteration 2556: loss -3.381610631942749\n",
      "Iteration 2557: loss -3.4041225910186768\n",
      "Iteration 2558: loss -3.4095511436462402\n",
      "Iteration 2559: loss -3.410918951034546\n",
      "Iteration 2560: loss -3.4005496501922607\n",
      "Iteration 2561: loss -3.4948911666870117\n",
      "Iteration 2562: loss -3.3077003955841064\n",
      "Iteration 2563: loss -3.334852695465088\n",
      "Iteration 2564: loss -3.255892276763916\n",
      "Iteration 2565: loss -3.3378870487213135\n",
      "Iteration 2566: loss -3.227010726928711\n",
      "Iteration 2567: loss -3.629556655883789\n",
      "Iteration 2568: loss -3.4231250286102295\n",
      "Iteration 2569: loss -3.3026840686798096\n",
      "Iteration 2570: loss -3.483898639678955\n",
      "Iteration 2571: loss -3.380831718444824\n",
      "Iteration 2572: loss -3.396505117416382\n",
      "Iteration 2573: loss -3.2265284061431885\n",
      "Iteration 2574: loss -3.4318642616271973\n",
      "Iteration 2575: loss -3.4340648651123047\n",
      "Iteration 2576: loss -3.277512550354004\n",
      "Iteration 2577: loss -3.250771999359131\n",
      "Iteration 2578: loss -3.4434807300567627\n",
      "Iteration 2579: loss -3.303609848022461\n",
      "Iteration 2580: loss -3.3126633167266846\n",
      "Iteration 2581: loss -3.312812566757202\n",
      "Iteration 2582: loss -3.2690269947052\n",
      "Iteration 2583: loss -3.375447988510132\n",
      "Iteration 2584: loss -3.1691389083862305\n",
      "Iteration 2585: loss -3.3733792304992676\n",
      "Iteration 2586: loss -3.2866382598876953\n",
      "Iteration 2587: loss -3.4250376224517822\n",
      "Iteration 2588: loss -3.434967279434204\n",
      "Iteration 2589: loss -3.3222336769104004\n",
      "Iteration 2590: loss -3.3861541748046875\n",
      "Iteration 2591: loss -3.394533157348633\n",
      "Iteration 2592: loss -3.223250150680542\n",
      "Iteration 2593: loss -3.3362362384796143\n",
      "Iteration 2594: loss -3.1948177814483643\n",
      "Iteration 2595: loss -3.3836703300476074\n",
      "Iteration 2596: loss -3.2987866401672363\n",
      "Iteration 2597: loss -3.250170946121216\n",
      "Iteration 2598: loss -3.187973976135254\n",
      "Iteration 2599: loss -3.2477385997772217\n",
      "Iteration 2600: loss -3.3208181858062744\n",
      "Iteration 2601: loss -3.2993884086608887\n",
      "Iteration 2602: loss -3.121492862701416\n",
      "Iteration 2603: loss -3.438887357711792\n",
      "Iteration 2604: loss -3.218799114227295\n",
      "Iteration 2605: loss -3.354984998703003\n",
      "Iteration 2606: loss -3.346755027770996\n",
      "Iteration 2607: loss -3.228801965713501\n",
      "Iteration 2608: loss -3.3327136039733887\n",
      "Iteration 2609: loss -3.2634363174438477\n",
      "Iteration 2610: loss -3.3107831478118896\n",
      "Iteration 2611: loss -3.2749524116516113\n",
      "Iteration 2612: loss -3.418937921524048\n",
      "Iteration 2613: loss -3.3103749752044678\n",
      "Iteration 2614: loss -3.3833653926849365\n",
      "Iteration 2615: loss -3.0995535850524902\n",
      "Iteration 2616: loss -3.181760787963867\n",
      "Iteration 2617: loss -3.2814486026763916\n",
      "Iteration 2618: loss -3.122859477996826\n",
      "Iteration 2619: loss -3.318864345550537\n",
      "Iteration 2620: loss -3.522019863128662\n",
      "Iteration 2621: loss -3.3711986541748047\n",
      "Iteration 2622: loss -3.1261894702911377\n",
      "Iteration 2623: loss -3.309281587600708\n",
      "Iteration 2624: loss -3.5401880741119385\n",
      "Iteration 2625: loss -3.243255853652954\n",
      "Iteration 2626: loss -3.2095961570739746\n",
      "Iteration 2627: loss -3.3200552463531494\n",
      "Iteration 2628: loss -3.4789888858795166\n",
      "Iteration 2629: loss -3.23684024810791\n",
      "Iteration 2630: loss -3.3308157920837402\n",
      "Iteration 2631: loss -3.531378984451294\n",
      "Iteration 2632: loss -3.1473026275634766\n",
      "Iteration 2633: loss -3.332000732421875\n",
      "Iteration 2634: loss -3.3668994903564453\n",
      "Iteration 2635: loss -3.336310386657715\n",
      "Iteration 2636: loss -3.3195407390594482\n",
      "Iteration 2637: loss -3.2279467582702637\n",
      "Iteration 2638: loss -3.3812496662139893\n",
      "Iteration 2639: loss -3.2253305912017822\n",
      "Iteration 2640: loss -3.3920562267303467\n",
      "Iteration 2641: loss -3.3188605308532715\n",
      "Iteration 2642: loss -3.1248395442962646\n",
      "Iteration 2643: loss -3.4892008304595947\n",
      "Iteration 2644: loss -3.233297824859619\n",
      "Iteration 2645: loss -3.4055752754211426\n",
      "Iteration 2646: loss -3.38546085357666\n",
      "Iteration 2647: loss -3.3947267532348633\n",
      "Iteration 2648: loss -3.316638231277466\n",
      "Iteration 2649: loss -3.4064371585845947\n",
      "Iteration 2650: loss -3.411824941635132\n",
      "Iteration 2651: loss -3.434079885482788\n",
      "Iteration 2652: loss -3.4163713455200195\n",
      "Iteration 2653: loss -3.3023993968963623\n",
      "Iteration 2654: loss -3.4075820446014404\n",
      "Iteration 2655: loss -3.3829455375671387\n",
      "Iteration 2656: loss -3.43557071685791\n",
      "Iteration 2657: loss -3.300624370574951\n",
      "Iteration 2658: loss -3.2082815170288086\n",
      "Iteration 2659: loss -3.537426710128784\n",
      "Iteration 2660: loss -3.4123547077178955\n",
      "Iteration 2661: loss -3.4901463985443115\n",
      "Iteration 2662: loss -3.4813897609710693\n",
      "Iteration 2663: loss -3.3799381256103516\n",
      "Iteration 2664: loss -3.4551589488983154\n",
      "Iteration 2665: loss -3.3928680419921875\n",
      "Iteration 2666: loss -3.411520481109619\n",
      "Iteration 2667: loss -3.3192925453186035\n",
      "Iteration 2668: loss -3.3986878395080566\n",
      "Iteration 2669: loss -3.4353318214416504\n",
      "Iteration 2670: loss -3.363365888595581\n",
      "Iteration 2671: loss -3.3793632984161377\n",
      "Iteration 2672: loss -3.3753364086151123\n",
      "Iteration 2673: loss -3.340228796005249\n",
      "Iteration 2674: loss -3.213705539703369\n",
      "Iteration 2675: loss -3.4697518348693848\n",
      "Iteration 2676: loss -3.450751304626465\n",
      "Iteration 2677: loss -3.3595545291900635\n",
      "Iteration 2678: loss -3.3707940578460693\n",
      "Iteration 2679: loss -3.374971866607666\n",
      "Iteration 2680: loss -2.820011854171753\n",
      "Iteration 2681: loss -3.467085361480713\n",
      "Iteration 2682: loss -3.094440221786499\n",
      "Iteration 2683: loss -3.32061767578125\n",
      "Iteration 2684: loss -3.4309754371643066\n",
      "Iteration 2685: loss -3.2184081077575684\n",
      "Iteration 2686: loss -3.2599706649780273\n",
      "Iteration 2687: loss -3.2592554092407227\n",
      "Iteration 2688: loss -3.182188034057617\n",
      "Iteration 2689: loss -3.2548117637634277\n",
      "Iteration 2690: loss -3.333409309387207\n",
      "Iteration 2691: loss -3.091780185699463\n",
      "Iteration 2692: loss -2.9389617443084717\n",
      "Iteration 2693: loss -3.3612112998962402\n",
      "Iteration 2694: loss -3.094749689102173\n",
      "Iteration 2695: loss -3.264220952987671\n",
      "Iteration 2696: loss -2.9755969047546387\n",
      "Iteration 2697: loss -3.2088019847869873\n",
      "Iteration 2698: loss -3.0576016902923584\n",
      "Iteration 2699: loss -3.080223321914673\n",
      "Iteration 2700: loss -3.3069589138031006\n",
      "Iteration 2701: loss -3.1101691722869873\n",
      "Iteration 2702: loss -3.0099916458129883\n",
      "Iteration 2703: loss -3.3728137016296387\n",
      "Iteration 2704: loss -3.4233598709106445\n",
      "Iteration 2705: loss -3.26249361038208\n",
      "Iteration 2706: loss -3.1806881427764893\n",
      "Iteration 2707: loss -3.408987522125244\n",
      "Iteration 2708: loss -3.405827045440674\n",
      "Iteration 2709: loss -3.064187526702881\n",
      "Iteration 2710: loss -3.281630039215088\n",
      "Iteration 2711: loss -3.3305556774139404\n",
      "Iteration 2712: loss -3.288248062133789\n",
      "Iteration 2713: loss -3.377176284790039\n",
      "Iteration 2714: loss -3.4295403957366943\n",
      "Iteration 2715: loss -3.4328653812408447\n",
      "Iteration 2716: loss -3.396148681640625\n",
      "Iteration 2717: loss -3.3504397869110107\n",
      "Iteration 2718: loss -3.2727694511413574\n",
      "Iteration 2719: loss -3.271296977996826\n",
      "Iteration 2720: loss -3.4634227752685547\n",
      "Iteration 2721: loss -3.3574066162109375\n",
      "Iteration 2722: loss -3.491307258605957\n",
      "Iteration 2723: loss -3.42143177986145\n",
      "Iteration 2724: loss -3.2287752628326416\n",
      "Iteration 2725: loss -3.3620781898498535\n",
      "Iteration 2726: loss -3.3920013904571533\n",
      "Iteration 2727: loss -3.305947780609131\n",
      "Iteration 2728: loss -3.227786064147949\n",
      "Iteration 2729: loss -3.3080382347106934\n",
      "Iteration 2730: loss -3.401024103164673\n",
      "Iteration 2731: loss -3.202509880065918\n",
      "Iteration 2732: loss -3.471275568008423\n",
      "Iteration 2733: loss -3.3839592933654785\n",
      "Iteration 2734: loss -3.4448671340942383\n",
      "Iteration 2735: loss -3.3649840354919434\n",
      "Iteration 2736: loss -3.3575267791748047\n",
      "Iteration 2737: loss -3.243398427963257\n",
      "Iteration 2738: loss -3.437567710876465\n",
      "Iteration 2739: loss -3.382827043533325\n",
      "Iteration 2740: loss -3.3206429481506348\n",
      "Iteration 2741: loss -3.3492302894592285\n",
      "Iteration 2742: loss -3.3079993724823\n",
      "Iteration 2743: loss -3.448925256729126\n",
      "Iteration 2744: loss -3.2677440643310547\n",
      "Iteration 2745: loss -3.4980337619781494\n",
      "Iteration 2746: loss -3.2194812297821045\n",
      "Iteration 2747: loss -3.367875337600708\n",
      "Iteration 2748: loss -3.4603395462036133\n",
      "Iteration 2749: loss -3.304685115814209\n",
      "Iteration 2750: loss -3.2808148860931396\n",
      "Iteration 2751: loss -3.392029047012329\n",
      "Iteration 2752: loss -3.083811044692993\n",
      "Iteration 2753: loss -3.0886707305908203\n",
      "Iteration 2754: loss -3.1824593544006348\n",
      "Iteration 2755: loss -3.116546869277954\n",
      "Iteration 2756: loss -3.021214246749878\n",
      "Iteration 2757: loss -3.1754844188690186\n",
      "Iteration 2758: loss -3.407606840133667\n",
      "Iteration 2759: loss -2.682959794998169\n",
      "Iteration 2760: loss -3.3755645751953125\n",
      "Iteration 2761: loss -3.2855451107025146\n",
      "Iteration 2762: loss -3.1402695178985596\n",
      "Iteration 2763: loss -3.1605112552642822\n",
      "Iteration 2764: loss -3.3261945247650146\n",
      "Iteration 2765: loss -3.2814955711364746\n",
      "Iteration 2766: loss -3.4065093994140625\n",
      "Iteration 2767: loss -3.1288492679595947\n",
      "Iteration 2768: loss -3.3316469192504883\n",
      "Iteration 2769: loss -3.267098903656006\n",
      "Iteration 2770: loss -3.164660930633545\n",
      "Iteration 2771: loss -3.3294732570648193\n",
      "Iteration 2772: loss -3.340679883956909\n",
      "Iteration 2773: loss -3.387583017349243\n",
      "Iteration 2774: loss -3.146195650100708\n",
      "Iteration 2775: loss -3.246072769165039\n",
      "Iteration 2776: loss -3.3598077297210693\n",
      "Iteration 2777: loss -3.352231740951538\n",
      "Iteration 2778: loss -3.3958194255828857\n",
      "Iteration 2779: loss -3.2959625720977783\n",
      "Iteration 2780: loss -3.382917881011963\n",
      "Iteration 2781: loss -3.254086971282959\n",
      "Iteration 2782: loss -3.3031954765319824\n",
      "Iteration 2783: loss -3.394484281539917\n",
      "Iteration 2784: loss -3.3647000789642334\n",
      "Iteration 2785: loss -3.2842376232147217\n",
      "Iteration 2786: loss -3.3520965576171875\n",
      "Iteration 2787: loss -3.2831990718841553\n",
      "Iteration 2788: loss -3.1901745796203613\n",
      "Iteration 2789: loss -3.4308722019195557\n",
      "Iteration 2790: loss -3.1236493587493896\n",
      "Iteration 2791: loss -3.490410566329956\n",
      "Iteration 2792: loss -3.3193109035491943\n",
      "Iteration 2793: loss -3.1138479709625244\n",
      "Iteration 2794: loss -3.3081865310668945\n",
      "Iteration 2795: loss -3.292978286743164\n",
      "Iteration 2796: loss -3.1935439109802246\n",
      "Iteration 2797: loss -3.379056930541992\n",
      "Iteration 2798: loss -3.327669382095337\n",
      "Iteration 2799: loss -3.3939456939697266\n",
      "Iteration 2800: loss -3.302605390548706\n",
      "Iteration 2801: loss -3.4122631549835205\n",
      "Iteration 2802: loss -3.4005205631256104\n",
      "Iteration 2803: loss -3.259533405303955\n",
      "Iteration 2804: loss -3.219717502593994\n",
      "Iteration 2805: loss -3.4616708755493164\n",
      "Iteration 2806: loss -3.2129855155944824\n",
      "Iteration 2807: loss -3.450218439102173\n",
      "Iteration 2808: loss -3.284059762954712\n",
      "Iteration 2809: loss -3.403582811355591\n",
      "Iteration 2810: loss -3.437124729156494\n",
      "Iteration 2811: loss -3.2866315841674805\n",
      "Iteration 2812: loss -3.269491672515869\n",
      "Iteration 2813: loss -3.3442885875701904\n",
      "Iteration 2814: loss -3.36653995513916\n",
      "Iteration 2815: loss -3.400865077972412\n",
      "Iteration 2816: loss -3.1885979175567627\n",
      "Iteration 2817: loss -3.341951370239258\n",
      "Iteration 2818: loss -3.4104983806610107\n",
      "Iteration 2819: loss -3.341898202896118\n",
      "Iteration 2820: loss -3.314194917678833\n",
      "Iteration 2821: loss -3.181288719177246\n",
      "Iteration 2822: loss -3.4000332355499268\n",
      "Iteration 2823: loss -3.4757964611053467\n",
      "Iteration 2824: loss -3.4485814571380615\n",
      "Iteration 2825: loss -3.500290870666504\n",
      "Iteration 2826: loss -3.468121290206909\n",
      "Iteration 2827: loss -3.3166263103485107\n",
      "Iteration 2828: loss -3.1900336742401123\n",
      "Iteration 2829: loss -3.2675554752349854\n",
      "Iteration 2830: loss -3.384127616882324\n",
      "Iteration 2831: loss -3.001650333404541\n",
      "Iteration 2832: loss -3.315934658050537\n",
      "Iteration 2833: loss -3.4418210983276367\n",
      "Iteration 2834: loss -3.187498092651367\n",
      "Iteration 2835: loss -3.319700002670288\n",
      "Iteration 2836: loss -3.4196579456329346\n",
      "Iteration 2837: loss -3.184757709503174\n",
      "Iteration 2838: loss -3.4650583267211914\n",
      "Iteration 2839: loss -3.2956106662750244\n",
      "Iteration 2840: loss -3.284494400024414\n",
      "Iteration 2841: loss -3.356506109237671\n",
      "Iteration 2842: loss -3.215498685836792\n",
      "Iteration 2843: loss -3.173409938812256\n",
      "Iteration 2844: loss -3.3809316158294678\n",
      "Iteration 2845: loss -3.020170211791992\n",
      "Iteration 2846: loss -3.277833938598633\n",
      "Iteration 2847: loss -3.3007919788360596\n",
      "Iteration 2848: loss -3.2399120330810547\n",
      "Iteration 2849: loss -3.218219518661499\n",
      "Iteration 2850: loss -3.3312714099884033\n",
      "Iteration 2851: loss -3.0910286903381348\n",
      "Iteration 2852: loss -3.3680307865142822\n",
      "Iteration 2853: loss -3.3012542724609375\n",
      "Iteration 2854: loss -3.398308038711548\n",
      "Iteration 2855: loss -3.255336046218872\n",
      "Iteration 2856: loss -3.262021780014038\n",
      "Iteration 2857: loss -3.1483876705169678\n",
      "Iteration 2858: loss -3.2037127017974854\n",
      "Iteration 2859: loss -3.3519234657287598\n",
      "Iteration 2860: loss -3.2485504150390625\n",
      "Iteration 2861: loss -3.2989118099212646\n",
      "Iteration 2862: loss -3.4265167713165283\n",
      "Iteration 2863: loss -3.307187080383301\n",
      "Iteration 2864: loss -3.3466992378234863\n",
      "Iteration 2865: loss -3.299612045288086\n",
      "Iteration 2866: loss -3.3850035667419434\n",
      "Iteration 2867: loss -3.3466451168060303\n",
      "Iteration 2868: loss -3.224125385284424\n",
      "Iteration 2869: loss -3.240302801132202\n",
      "Iteration 2870: loss -3.3336000442504883\n",
      "Iteration 2871: loss -3.330648183822632\n",
      "Iteration 2872: loss -3.318366050720215\n",
      "Iteration 2873: loss -3.3602335453033447\n",
      "Iteration 2874: loss -3.300340175628662\n",
      "Iteration 2875: loss -3.2353382110595703\n",
      "Iteration 2876: loss -3.4428112506866455\n",
      "Iteration 2877: loss -3.332151174545288\n",
      "Iteration 2878: loss -3.247422695159912\n",
      "Iteration 2879: loss -3.3197836875915527\n",
      "Iteration 2880: loss -3.4462921619415283\n",
      "Iteration 2881: loss -3.4354193210601807\n",
      "Iteration 2882: loss -3.365938663482666\n",
      "Iteration 2883: loss -3.3859341144561768\n",
      "Iteration 2884: loss -3.354733943939209\n",
      "Iteration 2885: loss -3.344104290008545\n",
      "Iteration 2886: loss -3.4157333374023438\n",
      "Iteration 2887: loss -3.3513917922973633\n",
      "Iteration 2888: loss -3.285057306289673\n",
      "Iteration 2889: loss -3.2075448036193848\n",
      "Iteration 2890: loss -3.3943161964416504\n",
      "Iteration 2891: loss -3.3534717559814453\n",
      "Iteration 2892: loss -2.960193395614624\n",
      "Iteration 2893: loss -3.0945520401000977\n",
      "Iteration 2894: loss -3.1635677814483643\n",
      "Iteration 2895: loss -2.9843637943267822\n",
      "Iteration 2896: loss -3.21868896484375\n",
      "Iteration 2897: loss -3.2873785495758057\n",
      "Iteration 2898: loss -3.372321844100952\n",
      "Iteration 2899: loss -3.245762825012207\n",
      "Iteration 2900: loss -3.2197654247283936\n",
      "Iteration 2901: loss -3.2445919513702393\n",
      "Iteration 2902: loss -3.008819818496704\n",
      "Iteration 2903: loss -3.076547622680664\n",
      "Iteration 2904: loss -3.3054885864257812\n",
      "Iteration 2905: loss -3.123790979385376\n",
      "Iteration 2906: loss -3.3021912574768066\n",
      "Iteration 2907: loss -3.3299176692962646\n",
      "Iteration 2908: loss -3.2346935272216797\n",
      "Iteration 2909: loss -2.883373737335205\n",
      "Iteration 2910: loss -3.3078012466430664\n",
      "Iteration 2911: loss -3.4556069374084473\n",
      "Iteration 2912: loss -3.048604965209961\n",
      "Iteration 2913: loss -3.2097458839416504\n",
      "Iteration 2914: loss -3.269754409790039\n",
      "Iteration 2915: loss -3.3610308170318604\n",
      "Iteration 2916: loss -3.3606512546539307\n",
      "Iteration 2917: loss -3.3396899700164795\n",
      "Iteration 2918: loss -3.3599154949188232\n",
      "Iteration 2919: loss -3.1756603717803955\n",
      "Iteration 2920: loss -3.2512621879577637\n",
      "Iteration 2921: loss -3.4546947479248047\n",
      "Iteration 2922: loss -3.388298749923706\n",
      "Iteration 2923: loss -3.318110942840576\n",
      "Iteration 2924: loss -3.3026483058929443\n",
      "Iteration 2925: loss -3.3708019256591797\n",
      "Iteration 2926: loss -3.324537754058838\n",
      "Iteration 2927: loss -3.276352882385254\n",
      "Iteration 2928: loss -3.2910239696502686\n",
      "Iteration 2929: loss -3.2259085178375244\n",
      "Iteration 2930: loss -3.414238929748535\n",
      "Iteration 2931: loss -3.2426092624664307\n",
      "Iteration 2932: loss -3.1787548065185547\n",
      "Iteration 2933: loss -3.5519723892211914\n",
      "Iteration 2934: loss -3.240022659301758\n",
      "Iteration 2935: loss -3.2698330879211426\n",
      "Iteration 2936: loss -3.478325128555298\n",
      "Iteration 2937: loss -3.028532028198242\n",
      "Iteration 2938: loss -3.341517925262451\n",
      "Iteration 2939: loss -3.2282838821411133\n",
      "Iteration 2940: loss -3.253190517425537\n",
      "Iteration 2941: loss -3.5136640071868896\n",
      "Iteration 2942: loss -3.436983346939087\n",
      "Iteration 2943: loss -3.1229352951049805\n",
      "Iteration 2944: loss -3.2275280952453613\n",
      "Iteration 2945: loss -3.2197983264923096\n",
      "Iteration 2946: loss -3.268941640853882\n",
      "Iteration 2947: loss -3.333172082901001\n",
      "Iteration 2948: loss -3.229902982711792\n",
      "Iteration 2949: loss -3.3118278980255127\n",
      "Iteration 2950: loss -3.3269782066345215\n",
      "Iteration 2951: loss -3.3244869709014893\n",
      "Iteration 2952: loss -3.3616650104522705\n",
      "Iteration 2953: loss -3.3931565284729004\n",
      "Iteration 2954: loss -3.1707968711853027\n",
      "Iteration 2955: loss -3.2667980194091797\n",
      "Iteration 2956: loss -3.296884536743164\n",
      "Iteration 2957: loss -3.0410425662994385\n",
      "Iteration 2958: loss -3.2602384090423584\n",
      "Iteration 2959: loss -3.2998952865600586\n",
      "Iteration 2960: loss -3.0397255420684814\n",
      "Iteration 2961: loss -3.1175549030303955\n",
      "Iteration 2962: loss -3.3284432888031006\n",
      "Iteration 2963: loss -3.2248048782348633\n",
      "Iteration 2964: loss -3.31137752532959\n",
      "Iteration 2965: loss -3.3006341457366943\n",
      "Iteration 2966: loss -3.1764755249023438\n",
      "Iteration 2967: loss -3.3387529850006104\n",
      "Iteration 2968: loss -3.289607048034668\n",
      "Iteration 2969: loss -3.1803832054138184\n",
      "Iteration 2970: loss -3.351591110229492\n",
      "Iteration 2971: loss -3.3765664100646973\n",
      "Iteration 2972: loss -3.293867588043213\n",
      "Iteration 2973: loss -3.174036979675293\n",
      "Iteration 2974: loss -3.3392205238342285\n",
      "Iteration 2975: loss -3.3589484691619873\n",
      "Iteration 2976: loss -3.0855486392974854\n",
      "Iteration 2977: loss -3.196833610534668\n",
      "Iteration 2978: loss -3.153200149536133\n",
      "Iteration 2979: loss -3.3669586181640625\n",
      "Iteration 2980: loss -3.292512893676758\n",
      "Iteration 2981: loss -3.469511032104492\n",
      "Iteration 2982: loss -3.3640053272247314\n",
      "Iteration 2983: loss -3.3462634086608887\n",
      "Iteration 2984: loss -3.401954412460327\n",
      "Iteration 2985: loss -3.3847081661224365\n",
      "Iteration 2986: loss -3.407829999923706\n",
      "Iteration 2987: loss -3.2203078269958496\n",
      "Iteration 2988: loss -3.3968169689178467\n",
      "Iteration 2989: loss -3.4494130611419678\n",
      "Iteration 2990: loss -3.4268288612365723\n",
      "Iteration 2991: loss -3.498756170272827\n",
      "Iteration 2992: loss -3.3622241020202637\n",
      "Iteration 2993: loss -3.351863384246826\n",
      "Iteration 2994: loss -3.3378329277038574\n",
      "Iteration 2995: loss -3.3636255264282227\n",
      "Iteration 2996: loss -3.513939142227173\n",
      "Iteration 2997: loss -3.400604248046875\n",
      "Iteration 2998: loss -3.3335022926330566\n",
      "Iteration 2999: loss -3.319600820541382\n",
      "Iteration 3000: loss -3.340327501296997\n",
      "Iteration 3001: loss -3.3715741634368896\n",
      "Iteration 3002: loss -3.3728482723236084\n",
      "Iteration 3003: loss -3.307924747467041\n",
      "Iteration 3004: loss -3.320685386657715\n",
      "Iteration 3005: loss -3.3855416774749756\n",
      "Iteration 3006: loss -3.2918057441711426\n",
      "Iteration 3007: loss -3.5104668140411377\n",
      "Iteration 3008: loss -3.408731460571289\n",
      "Iteration 3009: loss -3.2688827514648438\n",
      "Iteration 3010: loss -3.41778564453125\n",
      "Iteration 3011: loss -3.4011456966400146\n",
      "Iteration 3012: loss -3.492150068283081\n",
      "Iteration 3013: loss -3.2300939559936523\n",
      "Iteration 3014: loss -3.3954687118530273\n",
      "Iteration 3015: loss -3.391064405441284\n",
      "Iteration 3016: loss -3.0375330448150635\n",
      "Iteration 3017: loss -3.389979600906372\n",
      "Iteration 3018: loss -3.4189083576202393\n",
      "Iteration 3019: loss -3.2996671199798584\n",
      "Iteration 3020: loss -3.371659755706787\n",
      "Iteration 3021: loss -3.2220423221588135\n",
      "Iteration 3022: loss -3.439882278442383\n",
      "Iteration 3023: loss -3.301805019378662\n",
      "Iteration 3024: loss -3.333519697189331\n",
      "Iteration 3025: loss -3.2992541790008545\n",
      "Iteration 3026: loss -3.17008113861084\n",
      "Iteration 3027: loss -3.21854305267334\n",
      "Iteration 3028: loss -3.454087257385254\n",
      "Iteration 3029: loss -3.351033926010132\n",
      "Iteration 3030: loss -3.4003350734710693\n",
      "Iteration 3031: loss -3.292815923690796\n",
      "Iteration 3032: loss -3.4402194023132324\n",
      "Iteration 3033: loss -3.4535999298095703\n",
      "Iteration 3034: loss -3.369539499282837\n",
      "Iteration 3035: loss -3.3504700660705566\n",
      "Iteration 3036: loss -3.427910566329956\n",
      "Iteration 3037: loss -3.443101406097412\n",
      "Iteration 3038: loss -3.3287415504455566\n",
      "Iteration 3039: loss -3.3886401653289795\n",
      "Iteration 3040: loss -3.3584580421447754\n",
      "Iteration 3041: loss -3.374647855758667\n",
      "Iteration 3042: loss -3.3500521183013916\n",
      "Iteration 3043: loss -3.3823740482330322\n",
      "Iteration 3044: loss -3.54457950592041\n",
      "Iteration 3045: loss -3.4172253608703613\n",
      "Iteration 3046: loss -3.3668038845062256\n",
      "Iteration 3047: loss -3.38330078125\n",
      "Iteration 3048: loss -3.198911666870117\n",
      "Iteration 3049: loss -3.475818157196045\n",
      "Iteration 3050: loss -3.506282091140747\n",
      "Iteration 3051: loss -3.4448907375335693\n",
      "Iteration 3052: loss -3.629758358001709\n",
      "Iteration 3053: loss -3.3885345458984375\n",
      "Iteration 3054: loss -3.411766290664673\n",
      "Iteration 3055: loss -3.3859190940856934\n",
      "Iteration 3056: loss -3.370783805847168\n",
      "Iteration 3057: loss -3.4689977169036865\n",
      "Iteration 3058: loss -3.506298780441284\n",
      "Iteration 3059: loss -3.5365419387817383\n",
      "Iteration 3060: loss -3.4246606826782227\n",
      "Iteration 3061: loss -3.5452380180358887\n",
      "Iteration 3062: loss -3.511854648590088\n",
      "Iteration 3063: loss -3.598273277282715\n",
      "Iteration 3064: loss -3.3915047645568848\n",
      "Iteration 3065: loss -3.4594533443450928\n",
      "Iteration 3066: loss -3.538470983505249\n",
      "Iteration 3067: loss -3.439502239227295\n",
      "Iteration 3068: loss -3.2149441242218018\n",
      "Iteration 3069: loss -3.4052484035491943\n",
      "Iteration 3070: loss -3.4276115894317627\n",
      "Iteration 3071: loss -3.2943007946014404\n",
      "Iteration 3072: loss -3.4710657596588135\n",
      "Iteration 3073: loss -3.5440735816955566\n",
      "Iteration 3074: loss -3.433840751647949\n",
      "Iteration 3075: loss -3.301668405532837\n",
      "Iteration 3076: loss -3.463258981704712\n",
      "Iteration 3077: loss -3.170412540435791\n",
      "Iteration 3078: loss -3.1434977054595947\n",
      "Iteration 3079: loss -3.471406936645508\n",
      "Iteration 3080: loss -3.2488744258880615\n",
      "Iteration 3081: loss -3.3304409980773926\n",
      "Iteration 3082: loss -3.4402244091033936\n",
      "Iteration 3083: loss -3.277308702468872\n",
      "Iteration 3084: loss -3.542037010192871\n",
      "Iteration 3085: loss -3.2462034225463867\n",
      "Iteration 3086: loss -3.3143503665924072\n",
      "Iteration 3087: loss -3.527371883392334\n",
      "Iteration 3088: loss -3.3082449436187744\n",
      "Iteration 3089: loss -3.428924798965454\n",
      "Iteration 3090: loss -3.3256609439849854\n",
      "Iteration 3091: loss -3.3930630683898926\n",
      "Iteration 3092: loss -3.2923526763916016\n",
      "Iteration 3093: loss -3.3424017429351807\n",
      "Iteration 3094: loss -3.27193546295166\n",
      "Iteration 3095: loss -3.1782431602478027\n",
      "Iteration 3096: loss -3.256046772003174\n",
      "Iteration 3097: loss -3.3593695163726807\n",
      "Iteration 3098: loss -3.0816924571990967\n",
      "Iteration 3099: loss -3.2234275341033936\n",
      "Iteration 3100: loss -3.3175549507141113\n",
      "Iteration 3101: loss -3.4676756858825684\n",
      "Iteration 3102: loss -3.309504747390747\n",
      "Iteration 3103: loss -3.18544864654541\n",
      "Iteration 3104: loss -3.3961915969848633\n",
      "Iteration 3105: loss -3.2429306507110596\n",
      "Iteration 3106: loss -3.367109775543213\n",
      "Iteration 3107: loss -3.453070878982544\n",
      "Iteration 3108: loss -3.143230676651001\n",
      "Iteration 3109: loss -3.008185625076294\n",
      "Iteration 3110: loss -3.3613245487213135\n",
      "Iteration 3111: loss -3.34785795211792\n",
      "Iteration 3112: loss -3.0942089557647705\n",
      "Iteration 3113: loss -3.237487554550171\n",
      "Iteration 3114: loss -3.384671926498413\n",
      "Iteration 3115: loss -3.3310158252716064\n",
      "Iteration 3116: loss -3.1957592964172363\n",
      "Iteration 3117: loss -3.3198235034942627\n",
      "Iteration 3118: loss -3.642653465270996\n",
      "Iteration 3119: loss -3.3301172256469727\n",
      "Iteration 3120: loss -3.171537160873413\n",
      "Iteration 3121: loss -3.2025649547576904\n",
      "Iteration 3122: loss -3.2431070804595947\n",
      "Iteration 3123: loss -3.206472873687744\n",
      "Iteration 3124: loss -3.441776990890503\n",
      "Iteration 3125: loss -3.3517017364501953\n",
      "Iteration 3126: loss -3.350984573364258\n",
      "Iteration 3127: loss -3.1901214122772217\n",
      "Iteration 3128: loss -3.249814510345459\n",
      "Iteration 3129: loss -3.4318132400512695\n",
      "Iteration 3130: loss -3.344130277633667\n",
      "Iteration 3131: loss -3.209603786468506\n",
      "Iteration 3132: loss -3.544698476791382\n",
      "Iteration 3133: loss -3.4617655277252197\n",
      "Iteration 3134: loss -3.2627310752868652\n",
      "Iteration 3135: loss -3.1222918033599854\n",
      "Iteration 3136: loss -3.614725112915039\n",
      "Iteration 3137: loss -3.2686986923217773\n",
      "Iteration 3138: loss -3.1822235584259033\n",
      "Iteration 3139: loss -3.2494919300079346\n",
      "Iteration 3140: loss -3.4608263969421387\n",
      "Iteration 3141: loss -3.2527565956115723\n",
      "Iteration 3142: loss -3.4514074325561523\n",
      "Iteration 3143: loss -3.3946306705474854\n",
      "Iteration 3144: loss -3.579418659210205\n",
      "Iteration 3145: loss -3.485156297683716\n",
      "Iteration 3146: loss -3.216155767440796\n",
      "Iteration 3147: loss -3.445683479309082\n",
      "Iteration 3148: loss -3.3311798572540283\n",
      "Iteration 3149: loss -3.352841854095459\n",
      "Iteration 3150: loss -3.4214141368865967\n",
      "Iteration 3151: loss -3.475327253341675\n",
      "Iteration 3152: loss -3.403759717941284\n",
      "Iteration 3153: loss -3.4690301418304443\n",
      "Iteration 3154: loss -3.326664686203003\n",
      "Iteration 3155: loss -3.372588872909546\n",
      "Iteration 3156: loss -3.2340946197509766\n",
      "Iteration 3157: loss -3.468473434448242\n",
      "Iteration 3158: loss -3.439394950866699\n",
      "Iteration 3159: loss -3.3681955337524414\n",
      "Iteration 3160: loss -3.3780646324157715\n",
      "Iteration 3161: loss -3.1779983043670654\n",
      "Iteration 3162: loss -3.277540922164917\n",
      "Iteration 3163: loss -3.4196507930755615\n",
      "Iteration 3164: loss -3.2056679725646973\n",
      "Iteration 3165: loss -3.213198184967041\n",
      "Iteration 3166: loss -3.2991254329681396\n",
      "Iteration 3167: loss -3.3894708156585693\n",
      "Iteration 3168: loss -3.4297595024108887\n",
      "Iteration 3169: loss -3.4157793521881104\n",
      "Iteration 3170: loss -3.379703998565674\n",
      "Iteration 3171: loss -3.223606824874878\n",
      "Iteration 3172: loss -3.440822124481201\n",
      "Iteration 3173: loss -3.386298894882202\n",
      "Iteration 3174: loss -3.3573150634765625\n",
      "Iteration 3175: loss -3.161879539489746\n",
      "Iteration 3176: loss -3.3267822265625\n",
      "Iteration 3177: loss -3.3806426525115967\n",
      "Iteration 3178: loss -3.3521790504455566\n",
      "Iteration 3179: loss -3.157527446746826\n",
      "Iteration 3180: loss -3.273200273513794\n",
      "Iteration 3181: loss -3.2978806495666504\n",
      "Iteration 3182: loss -3.2461843490600586\n",
      "Iteration 3183: loss -3.2831714153289795\n",
      "Iteration 3184: loss -3.271500825881958\n",
      "Iteration 3185: loss -3.1310410499572754\n",
      "Iteration 3186: loss -3.2675914764404297\n",
      "Iteration 3187: loss -3.3892219066619873\n",
      "Iteration 3188: loss -3.3447484970092773\n",
      "Iteration 3189: loss -3.408583641052246\n",
      "Iteration 3190: loss -3.214240074157715\n",
      "Iteration 3191: loss -3.455740451812744\n",
      "Iteration 3192: loss -3.3260467052459717\n",
      "Iteration 3193: loss -3.357041358947754\n",
      "Iteration 3194: loss -3.3380825519561768\n",
      "Iteration 3195: loss -3.3705008029937744\n",
      "Iteration 3196: loss -3.3044891357421875\n",
      "Iteration 3197: loss -3.2630202770233154\n",
      "Iteration 3198: loss -3.3413729667663574\n",
      "Iteration 3199: loss -3.3524329662323\n",
      "Iteration 3200: loss -3.266040086746216\n",
      "Iteration 3201: loss -3.4123544692993164\n",
      "Iteration 3202: loss -3.3127336502075195\n",
      "Iteration 3203: loss -3.1880152225494385\n",
      "Iteration 3204: loss -3.392301559448242\n",
      "Iteration 3205: loss -3.2348248958587646\n",
      "Iteration 3206: loss -3.136242628097534\n",
      "Iteration 3207: loss -3.2746846675872803\n",
      "Iteration 3208: loss -3.28521728515625\n",
      "Iteration 3209: loss -3.367661952972412\n",
      "Iteration 3210: loss -3.412764310836792\n",
      "Iteration 3211: loss -3.286289930343628\n",
      "Iteration 3212: loss -3.40494441986084\n",
      "Iteration 3213: loss -3.3002636432647705\n",
      "Iteration 3214: loss -3.191258430480957\n",
      "Iteration 3215: loss -3.3628737926483154\n",
      "Iteration 3216: loss -3.4124603271484375\n",
      "Iteration 3217: loss -3.454394817352295\n",
      "Iteration 3218: loss -3.4274039268493652\n",
      "Iteration 3219: loss -3.461439847946167\n",
      "Iteration 3220: loss -3.6072275638580322\n",
      "Iteration 3221: loss -3.408123254776001\n",
      "Iteration 3222: loss -3.29425048828125\n",
      "Iteration 3223: loss -3.486344575881958\n",
      "Iteration 3224: loss -3.516085147857666\n",
      "Iteration 3225: loss -3.264449119567871\n",
      "Iteration 3226: loss -3.397308588027954\n",
      "Iteration 3227: loss -3.426717519760132\n",
      "Iteration 3228: loss -3.4478676319122314\n",
      "Iteration 3229: loss -3.4047162532806396\n",
      "Iteration 3230: loss -3.2883052825927734\n",
      "Iteration 3231: loss -3.3728692531585693\n",
      "Iteration 3232: loss -3.441603422164917\n",
      "Iteration 3233: loss -3.276714563369751\n",
      "Iteration 3234: loss -3.171909809112549\n",
      "Iteration 3235: loss -3.024054527282715\n",
      "Iteration 3236: loss -3.4279372692108154\n",
      "Iteration 3237: loss -3.4082143306732178\n",
      "Iteration 3238: loss -2.9099342823028564\n",
      "Iteration 3239: loss -3.088174819946289\n",
      "Iteration 3240: loss -3.359909772872925\n",
      "Iteration 3241: loss -3.3184897899627686\n",
      "Iteration 3242: loss -3.3076391220092773\n",
      "Iteration 3243: loss -3.3118085861206055\n",
      "Iteration 3244: loss -3.243025541305542\n",
      "Iteration 3245: loss -3.2821733951568604\n",
      "Iteration 3246: loss -3.391818046569824\n",
      "Iteration 3247: loss -3.2797837257385254\n",
      "Iteration 3248: loss -3.40803861618042\n",
      "Iteration 3249: loss -3.3329131603240967\n",
      "Iteration 3250: loss -3.326611042022705\n",
      "Iteration 3251: loss -3.355475664138794\n",
      "Iteration 3252: loss -3.3993117809295654\n",
      "Iteration 3253: loss -3.2489142417907715\n",
      "Iteration 3254: loss -3.1976959705352783\n",
      "Iteration 3255: loss -3.3365654945373535\n",
      "Iteration 3256: loss -3.48323130607605\n",
      "Iteration 3257: loss -3.388423204421997\n",
      "Iteration 3258: loss -3.3539516925811768\n",
      "Iteration 3259: loss -3.390963077545166\n",
      "Iteration 3260: loss -3.4256224632263184\n",
      "Iteration 3261: loss -3.4513208866119385\n",
      "Iteration 3262: loss -3.4778902530670166\n",
      "Iteration 3263: loss -3.408021926879883\n",
      "Iteration 3264: loss -3.5622918605804443\n",
      "Iteration 3265: loss -3.482063055038452\n",
      "Iteration 3266: loss -3.5742557048797607\n",
      "Iteration 3267: loss -3.524813175201416\n",
      "Iteration 3268: loss -3.414530038833618\n",
      "Iteration 3269: loss -3.4984729290008545\n",
      "Iteration 3270: loss -3.3724563121795654\n",
      "Iteration 3271: loss -3.52241849899292\n",
      "Iteration 3272: loss -3.4679229259490967\n",
      "Iteration 3273: loss -3.469540596008301\n",
      "Iteration 3274: loss -3.4485769271850586\n",
      "Iteration 3275: loss -3.3514318466186523\n",
      "Iteration 3276: loss -3.599946975708008\n",
      "Iteration 3277: loss -3.5783727169036865\n",
      "Iteration 3278: loss -3.501880407333374\n",
      "Iteration 3279: loss -3.5944244861602783\n",
      "Iteration 3280: loss -3.362999200820923\n",
      "Iteration 3281: loss -3.6720893383026123\n",
      "Iteration 3282: loss -3.29799222946167\n",
      "Iteration 3283: loss -3.375436305999756\n",
      "Iteration 3284: loss -3.4406142234802246\n",
      "Iteration 3285: loss -3.2618913650512695\n",
      "Iteration 3286: loss -3.49873423576355\n",
      "Iteration 3287: loss -3.2900686264038086\n",
      "Iteration 3288: loss -3.346506357192993\n",
      "Iteration 3289: loss -3.41165828704834\n",
      "Iteration 3290: loss -3.37447452545166\n",
      "Iteration 3291: loss -3.4276371002197266\n",
      "Iteration 3292: loss -3.4477462768554688\n",
      "Iteration 3293: loss -3.252258539199829\n",
      "Iteration 3294: loss -3.3110997676849365\n",
      "Iteration 3295: loss -3.4319007396698\n",
      "Iteration 3296: loss -3.5786876678466797\n",
      "Iteration 3297: loss -3.337347984313965\n",
      "Iteration 3298: loss -3.340404748916626\n",
      "Iteration 3299: loss -3.2000482082366943\n",
      "Iteration 3300: loss -3.331195116043091\n",
      "Iteration 3301: loss -3.4462413787841797\n",
      "Iteration 3302: loss -3.268641471862793\n",
      "Iteration 3303: loss -3.23736572265625\n",
      "Iteration 3304: loss -3.3619446754455566\n",
      "Iteration 3305: loss -3.3933327198028564\n",
      "Iteration 3306: loss -3.4035069942474365\n",
      "Iteration 3307: loss -3.302748441696167\n",
      "Iteration 3308: loss -3.3526697158813477\n",
      "Iteration 3309: loss -3.2267935276031494\n",
      "Iteration 3310: loss -3.4126060009002686\n",
      "Iteration 3311: loss -3.371262550354004\n",
      "Iteration 3312: loss -3.2837741374969482\n",
      "Iteration 3313: loss -3.2890982627868652\n",
      "Iteration 3314: loss -3.3943898677825928\n",
      "Iteration 3315: loss -3.3247344493865967\n",
      "Iteration 3316: loss -3.2141366004943848\n",
      "Iteration 3317: loss -3.311638832092285\n",
      "Iteration 3318: loss -3.357288122177124\n",
      "Iteration 3319: loss -3.2010457515716553\n",
      "Iteration 3320: loss -3.3292746543884277\n",
      "Iteration 3321: loss -3.2615785598754883\n",
      "Iteration 3322: loss -3.468156337738037\n",
      "Iteration 3323: loss -3.3129918575286865\n",
      "Iteration 3324: loss -3.3575799465179443\n",
      "Iteration 3325: loss -3.2611746788024902\n",
      "Iteration 3326: loss -3.379204034805298\n",
      "Iteration 3327: loss -3.298945903778076\n",
      "Iteration 3328: loss -3.2583861351013184\n",
      "Iteration 3329: loss -3.233393907546997\n",
      "Iteration 3330: loss -3.358508586883545\n",
      "Iteration 3331: loss -3.314781427383423\n",
      "Iteration 3332: loss -3.426436185836792\n",
      "Iteration 3333: loss -3.467097759246826\n",
      "Iteration 3334: loss -3.410565137863159\n",
      "Iteration 3335: loss -3.028005599975586\n",
      "Iteration 3336: loss -3.3441338539123535\n",
      "Iteration 3337: loss -3.378629207611084\n",
      "Iteration 3338: loss -3.359783887863159\n",
      "Iteration 3339: loss -3.1052496433258057\n",
      "Iteration 3340: loss -3.2121737003326416\n",
      "Iteration 3341: loss -3.3732404708862305\n",
      "Iteration 3342: loss -3.3277699947357178\n",
      "Iteration 3343: loss -3.2958366870880127\n",
      "Iteration 3344: loss -3.2637147903442383\n",
      "Iteration 3345: loss -3.3813483715057373\n",
      "Iteration 3346: loss -3.373650312423706\n",
      "Iteration 3347: loss -3.4462897777557373\n",
      "Iteration 3348: loss -3.298849582672119\n",
      "Iteration 3349: loss -3.3060386180877686\n",
      "Iteration 3350: loss -3.369537353515625\n",
      "Iteration 3351: loss -3.2661325931549072\n",
      "Iteration 3352: loss -3.490551710128784\n",
      "Iteration 3353: loss -3.306394100189209\n",
      "Iteration 3354: loss -3.3060553073883057\n",
      "Iteration 3355: loss -3.2694320678710938\n",
      "Iteration 3356: loss -3.297499895095825\n",
      "Iteration 3357: loss -3.303941011428833\n",
      "Iteration 3358: loss -3.370706796646118\n",
      "Iteration 3359: loss -3.292421579360962\n",
      "Iteration 3360: loss -3.2971384525299072\n",
      "Iteration 3361: loss -3.427699565887451\n",
      "Iteration 3362: loss -3.2060329914093018\n",
      "Iteration 3363: loss -3.3060555458068848\n",
      "Iteration 3364: loss -3.4350759983062744\n",
      "Iteration 3365: loss -3.2784311771392822\n",
      "Iteration 3366: loss -3.4571046829223633\n",
      "Iteration 3367: loss -3.418038845062256\n",
      "Iteration 3368: loss -3.2381820678710938\n",
      "Iteration 3369: loss -3.4163317680358887\n",
      "Iteration 3370: loss -3.386441707611084\n",
      "Iteration 3371: loss -3.3496358394622803\n",
      "Iteration 3372: loss -3.374697208404541\n",
      "Iteration 3373: loss -3.3523385524749756\n",
      "Iteration 3374: loss -3.153895378112793\n",
      "Iteration 3375: loss -3.4635040760040283\n",
      "Iteration 3376: loss -3.380826950073242\n",
      "Iteration 3377: loss -3.42100191116333\n",
      "Iteration 3378: loss -3.439297676086426\n",
      "Iteration 3379: loss -3.41709566116333\n",
      "Iteration 3380: loss -3.302884817123413\n",
      "Iteration 3381: loss -3.336094617843628\n",
      "Iteration 3382: loss -3.341015577316284\n",
      "Iteration 3383: loss -3.22749662399292\n",
      "Iteration 3384: loss -3.4589524269104004\n",
      "Iteration 3385: loss -3.4407966136932373\n",
      "Iteration 3386: loss -3.1640784740448\n",
      "Iteration 3387: loss -3.4825072288513184\n",
      "Iteration 3388: loss -3.268786668777466\n",
      "Iteration 3389: loss -3.1658730506896973\n",
      "Iteration 3390: loss -3.424168348312378\n",
      "Iteration 3391: loss -3.292454481124878\n",
      "Iteration 3392: loss -3.098787307739258\n",
      "Iteration 3393: loss -3.2907865047454834\n",
      "Iteration 3394: loss -3.218391180038452\n",
      "Iteration 3395: loss -3.3246796131134033\n",
      "Iteration 3396: loss -3.1967086791992188\n",
      "Iteration 3397: loss -3.1649816036224365\n",
      "Iteration 3398: loss -3.1479547023773193\n",
      "Iteration 3399: loss -3.3099968433380127\n",
      "Iteration 3400: loss -3.04731822013855\n",
      "Iteration 3401: loss -3.3184680938720703\n",
      "Iteration 3402: loss -3.4162333011627197\n",
      "Iteration 3403: loss -3.077425479888916\n",
      "Iteration 3404: loss -3.0434796810150146\n",
      "Iteration 3405: loss -3.2086703777313232\n",
      "Iteration 3406: loss -3.0870542526245117\n",
      "Iteration 3407: loss -3.341085910797119\n",
      "Iteration 3408: loss -3.509767532348633\n",
      "Iteration 3409: loss -3.2280948162078857\n",
      "Iteration 3410: loss -3.320497751235962\n",
      "Iteration 3411: loss -3.342869281768799\n",
      "Iteration 3412: loss -3.4464304447174072\n",
      "Iteration 3413: loss -3.342935562133789\n",
      "Iteration 3414: loss -3.392066240310669\n",
      "Iteration 3415: loss -3.390629291534424\n",
      "Iteration 3416: loss -3.307100296020508\n",
      "Iteration 3417: loss -3.2973010540008545\n",
      "Iteration 3418: loss -3.1848502159118652\n",
      "Iteration 3419: loss -3.235790491104126\n",
      "Iteration 3420: loss -3.3472580909729004\n",
      "Iteration 3421: loss -3.2749016284942627\n",
      "Iteration 3422: loss -3.289285898208618\n",
      "Iteration 3423: loss -3.3719239234924316\n",
      "Iteration 3424: loss -3.446434259414673\n",
      "Iteration 3425: loss -3.368962287902832\n",
      "Iteration 3426: loss -3.255997896194458\n",
      "Iteration 3427: loss -3.4669415950775146\n",
      "Iteration 3428: loss -3.224907875061035\n",
      "Iteration 3429: loss -3.329244375228882\n",
      "Iteration 3430: loss -3.324808120727539\n",
      "Iteration 3431: loss -3.1998157501220703\n",
      "Iteration 3432: loss -3.239022731781006\n",
      "Iteration 3433: loss -3.274550676345825\n",
      "Iteration 3434: loss -3.322549343109131\n",
      "Iteration 3435: loss -3.383319616317749\n",
      "Iteration 3436: loss -3.2771735191345215\n",
      "Iteration 3437: loss -3.255075454711914\n",
      "Iteration 3438: loss -3.343731164932251\n",
      "Iteration 3439: loss -3.2332406044006348\n",
      "Iteration 3440: loss -3.2489471435546875\n",
      "Iteration 3441: loss -3.3752214908599854\n",
      "Iteration 3442: loss -3.418095588684082\n",
      "Iteration 3443: loss -3.1963980197906494\n",
      "Iteration 3444: loss -3.340972900390625\n",
      "Iteration 3445: loss -3.450791120529175\n",
      "Iteration 3446: loss -3.183915138244629\n",
      "Iteration 3447: loss -3.2859342098236084\n",
      "Iteration 3448: loss -3.37374210357666\n",
      "Iteration 3449: loss -3.391892194747925\n",
      "Iteration 3450: loss -3.3253262042999268\n",
      "Iteration 3451: loss -3.4240148067474365\n",
      "Iteration 3452: loss -3.534153938293457\n",
      "Iteration 3453: loss -3.273211717605591\n",
      "Iteration 3454: loss -3.349851608276367\n",
      "Iteration 3455: loss -3.3426499366760254\n",
      "Iteration 3456: loss -3.4372358322143555\n",
      "Iteration 3457: loss -3.1479175090789795\n",
      "Iteration 3458: loss -3.3671176433563232\n",
      "Iteration 3459: loss -3.20283579826355\n",
      "Iteration 3460: loss -3.455216884613037\n",
      "Iteration 3461: loss -3.4598991870880127\n",
      "Iteration 3462: loss -3.3402795791625977\n",
      "Iteration 3463: loss -3.59260630607605\n",
      "Iteration 3464: loss -3.3877358436584473\n",
      "Iteration 3465: loss -3.4684057235717773\n",
      "Iteration 3466: loss -3.3784830570220947\n",
      "Iteration 3467: loss -3.4166488647460938\n",
      "Iteration 3468: loss -3.4657087326049805\n",
      "Iteration 3469: loss -3.462113618850708\n",
      "Iteration 3470: loss -3.367622137069702\n",
      "Iteration 3471: loss -3.4294180870056152\n",
      "Iteration 3472: loss -3.338130235671997\n",
      "Iteration 3473: loss -3.523432970046997\n",
      "Iteration 3474: loss -3.384694814682007\n",
      "Iteration 3475: loss -3.566136121749878\n",
      "Iteration 3476: loss -3.395313024520874\n",
      "Iteration 3477: loss -3.560188055038452\n",
      "Iteration 3478: loss -3.490244150161743\n",
      "Iteration 3479: loss -3.3704421520233154\n",
      "Iteration 3480: loss -3.4678046703338623\n",
      "Iteration 3481: loss -3.6028482913970947\n",
      "Iteration 3482: loss -3.497393846511841\n",
      "Iteration 3483: loss -3.531074285507202\n",
      "Iteration 3484: loss -3.5165200233459473\n",
      "Iteration 3485: loss -3.497838020324707\n",
      "Iteration 3486: loss -3.4047634601593018\n",
      "Iteration 3487: loss -3.3851866722106934\n",
      "Iteration 3488: loss -3.4872493743896484\n",
      "Iteration 3489: loss -3.293539047241211\n",
      "Iteration 3490: loss -3.3722541332244873\n",
      "Iteration 3491: loss -3.444103717803955\n",
      "Iteration 3492: loss -3.46235990524292\n",
      "Iteration 3493: loss -3.3470096588134766\n",
      "Iteration 3494: loss -3.5805163383483887\n",
      "Iteration 3495: loss -3.480189800262451\n",
      "Iteration 3496: loss -3.424215078353882\n",
      "Iteration 3497: loss -3.446150779724121\n",
      "Iteration 3498: loss -3.3443098068237305\n",
      "Iteration 3499: loss -3.3799667358398438\n",
      "Iteration 3500: loss -3.204874277114868\n",
      "Iteration 3501: loss -3.157737970352173\n",
      "Iteration 3502: loss -3.3341972827911377\n",
      "Iteration 3503: loss -3.365983486175537\n",
      "Iteration 3504: loss -3.345660448074341\n",
      "Iteration 3505: loss -3.407615900039673\n",
      "Iteration 3506: loss -3.357212543487549\n",
      "Iteration 3507: loss -3.329408884048462\n",
      "Iteration 3508: loss -3.5095815658569336\n",
      "Iteration 3509: loss -3.5139124393463135\n",
      "Iteration 3510: loss -3.4211456775665283\n",
      "Iteration 3511: loss -3.303079843521118\n",
      "Iteration 3512: loss -3.418241500854492\n",
      "Iteration 3513: loss -3.4500622749328613\n",
      "Iteration 3514: loss -3.4746901988983154\n",
      "Iteration 3515: loss -3.476480722427368\n",
      "Iteration 3516: loss -3.315314531326294\n",
      "Iteration 3517: loss -3.4254281520843506\n",
      "Iteration 3518: loss -3.580387592315674\n",
      "Iteration 3519: loss -3.438410758972168\n",
      "Iteration 3520: loss -3.3988072872161865\n",
      "Iteration 3521: loss -3.5732312202453613\n",
      "Iteration 3522: loss -3.363962173461914\n",
      "Iteration 3523: loss -3.5306806564331055\n",
      "Iteration 3524: loss -3.487497329711914\n",
      "Iteration 3525: loss -3.437243938446045\n",
      "Iteration 3526: loss -3.3691582679748535\n",
      "Iteration 3527: loss -3.4736766815185547\n",
      "Iteration 3528: loss -3.3294293880462646\n",
      "Iteration 3529: loss -3.5017261505126953\n",
      "Iteration 3530: loss -3.413604736328125\n",
      "Iteration 3531: loss -3.197507858276367\n",
      "Iteration 3532: loss -3.3149805068969727\n",
      "Iteration 3533: loss -3.3254222869873047\n",
      "Iteration 3534: loss -3.2158315181732178\n",
      "Iteration 3535: loss -3.2995405197143555\n",
      "Iteration 3536: loss -3.397526741027832\n",
      "Iteration 3537: loss -3.4788825511932373\n",
      "Iteration 3538: loss -3.4493987560272217\n",
      "Iteration 3539: loss -3.331117868423462\n",
      "Iteration 3540: loss -3.4384145736694336\n",
      "Iteration 3541: loss -3.2342870235443115\n",
      "Iteration 3542: loss -3.4579784870147705\n",
      "Iteration 3543: loss -3.3056259155273438\n",
      "Iteration 3544: loss -3.340919256210327\n",
      "Iteration 3545: loss -3.1588099002838135\n",
      "Iteration 3546: loss -3.4577910900115967\n",
      "Iteration 3547: loss -3.4030182361602783\n",
      "Iteration 3548: loss -3.239729642868042\n",
      "Iteration 3549: loss -3.356559991836548\n",
      "Iteration 3550: loss -3.4275760650634766\n",
      "Iteration 3551: loss -3.412137985229492\n",
      "Iteration 3552: loss -3.3365790843963623\n",
      "Iteration 3553: loss -3.4057984352111816\n",
      "Iteration 3554: loss -3.354764938354492\n",
      "Iteration 3555: loss -3.367232322692871\n",
      "Iteration 3556: loss -3.4793286323547363\n",
      "Iteration 3557: loss -3.314068078994751\n",
      "Iteration 3558: loss -3.3722429275512695\n",
      "Iteration 3559: loss -3.3210949897766113\n",
      "Iteration 3560: loss -3.3905036449432373\n",
      "Iteration 3561: loss -3.508004665374756\n",
      "Iteration 3562: loss -3.3198742866516113\n",
      "Iteration 3563: loss -3.4451043605804443\n",
      "Iteration 3564: loss -3.4378433227539062\n",
      "Iteration 3565: loss -3.34812068939209\n",
      "Iteration 3566: loss -3.397369146347046\n",
      "Iteration 3567: loss -3.4776506423950195\n",
      "Iteration 3568: loss -3.5170791149139404\n",
      "Iteration 3569: loss -3.2937259674072266\n",
      "Iteration 3570: loss -3.447538375854492\n",
      "Iteration 3571: loss -3.3312344551086426\n",
      "Iteration 3572: loss -3.316547155380249\n",
      "Iteration 3573: loss -3.3563408851623535\n",
      "Iteration 3574: loss -3.295536756515503\n",
      "Iteration 3575: loss -3.252516746520996\n",
      "Iteration 3576: loss -3.378544807434082\n",
      "Iteration 3577: loss -3.4177956581115723\n",
      "Iteration 3578: loss -3.4560136795043945\n",
      "Iteration 3579: loss -3.265551805496216\n",
      "Iteration 3580: loss -3.5375845432281494\n",
      "Iteration 3581: loss -3.3840889930725098\n",
      "Iteration 3582: loss -3.4503443241119385\n",
      "Iteration 3583: loss -3.35977840423584\n",
      "Iteration 3584: loss -3.471421480178833\n",
      "Iteration 3585: loss -3.4270670413970947\n",
      "Iteration 3586: loss -3.3634488582611084\n",
      "Iteration 3587: loss -3.3159563541412354\n",
      "Iteration 3588: loss -3.3799188137054443\n",
      "Iteration 3589: loss -3.4047045707702637\n",
      "Iteration 3590: loss -3.4497454166412354\n",
      "Iteration 3591: loss -3.3877484798431396\n",
      "Iteration 3592: loss -3.469655752182007\n",
      "Iteration 3593: loss -3.4918057918548584\n",
      "Iteration 3594: loss -3.499943733215332\n",
      "Iteration 3595: loss -3.4529852867126465\n",
      "Iteration 3596: loss -3.4759271144866943\n",
      "Iteration 3597: loss -3.3936073780059814\n",
      "Iteration 3598: loss -3.4121053218841553\n",
      "Iteration 3599: loss -3.3286314010620117\n",
      "Iteration 3600: loss -3.30820369720459\n",
      "Iteration 3601: loss -3.5003461837768555\n",
      "Iteration 3602: loss -3.3730835914611816\n",
      "Iteration 3603: loss -3.515855312347412\n",
      "Iteration 3604: loss -3.472198247909546\n",
      "Iteration 3605: loss -3.2656917572021484\n",
      "Iteration 3606: loss -3.254744529724121\n",
      "Iteration 3607: loss -3.444040298461914\n",
      "Iteration 3608: loss -3.522719383239746\n",
      "Iteration 3609: loss -3.401383399963379\n",
      "Iteration 3610: loss -3.4403765201568604\n",
      "Iteration 3611: loss -3.4259798526763916\n",
      "Iteration 3612: loss -3.393547773361206\n",
      "Iteration 3613: loss -3.4136769771575928\n",
      "Iteration 3614: loss -3.3397786617279053\n",
      "Iteration 3615: loss -3.484102725982666\n",
      "Iteration 3616: loss -3.494136095046997\n",
      "Iteration 3617: loss -3.3788154125213623\n",
      "Iteration 3618: loss -3.3314614295959473\n",
      "Iteration 3619: loss -3.4197428226470947\n",
      "Iteration 3620: loss -3.4698171615600586\n",
      "Iteration 3621: loss -3.0914266109466553\n",
      "Iteration 3622: loss -3.2776267528533936\n",
      "Iteration 3623: loss -3.3079564571380615\n",
      "Iteration 3624: loss -3.448981523513794\n",
      "Iteration 3625: loss -3.4128293991088867\n",
      "Iteration 3626: loss -3.3339099884033203\n",
      "Iteration 3627: loss -3.4244778156280518\n",
      "Iteration 3628: loss -3.3080968856811523\n",
      "Iteration 3629: loss -3.404799461364746\n",
      "Iteration 3630: loss -3.317159414291382\n",
      "Iteration 3631: loss -3.331559181213379\n",
      "Iteration 3632: loss -3.441314935684204\n",
      "Iteration 3633: loss -3.3035645484924316\n",
      "Iteration 3634: loss -3.124760150909424\n",
      "Iteration 3635: loss -3.332692861557007\n",
      "Iteration 3636: loss -3.2157132625579834\n",
      "Iteration 3637: loss -3.2351081371307373\n",
      "Iteration 3638: loss -3.2013237476348877\n",
      "Iteration 3639: loss -3.2610979080200195\n",
      "Iteration 3640: loss -3.337766408920288\n",
      "Iteration 3641: loss -3.2597711086273193\n",
      "Iteration 3642: loss -3.4796009063720703\n",
      "Iteration 3643: loss -3.4162964820861816\n",
      "Iteration 3644: loss -3.4832119941711426\n",
      "Iteration 3645: loss -3.1745429039001465\n",
      "Iteration 3646: loss -3.3204774856567383\n",
      "Iteration 3647: loss -3.3448145389556885\n",
      "Iteration 3648: loss -3.251035213470459\n",
      "Iteration 3649: loss -3.499910831451416\n",
      "Iteration 3650: loss -3.3264312744140625\n",
      "Iteration 3651: loss -3.3630592823028564\n",
      "Iteration 3652: loss -3.5378594398498535\n",
      "Iteration 3653: loss -3.194859504699707\n",
      "Iteration 3654: loss -3.1929421424865723\n",
      "Iteration 3655: loss -3.2540481090545654\n",
      "Iteration 3656: loss -3.4782652854919434\n",
      "Iteration 3657: loss -3.2393064498901367\n",
      "Iteration 3658: loss -3.4712297916412354\n",
      "Iteration 3659: loss -3.2453505992889404\n",
      "Iteration 3660: loss -3.2618408203125\n",
      "Iteration 3661: loss -3.2128419876098633\n",
      "Iteration 3662: loss -3.3470749855041504\n",
      "Iteration 3663: loss -3.1965172290802\n",
      "Iteration 3664: loss -3.527785539627075\n",
      "Iteration 3665: loss -3.3660101890563965\n",
      "Iteration 3666: loss -3.052715539932251\n",
      "Iteration 3667: loss -3.291168451309204\n",
      "Iteration 3668: loss -3.270035743713379\n",
      "Iteration 3669: loss -3.423299551010132\n",
      "Iteration 3670: loss -3.4371862411499023\n",
      "Iteration 3671: loss -3.4322519302368164\n",
      "Iteration 3672: loss -3.5166220664978027\n",
      "Iteration 3673: loss -3.314711093902588\n",
      "Iteration 3674: loss -3.4134511947631836\n",
      "Iteration 3675: loss -3.093557119369507\n",
      "Iteration 3676: loss -3.4429352283477783\n",
      "Iteration 3677: loss -3.4084126949310303\n",
      "Iteration 3678: loss -3.370731830596924\n",
      "Iteration 3679: loss -3.341068744659424\n",
      "Iteration 3680: loss -3.403397798538208\n",
      "Iteration 3681: loss -3.2715444564819336\n",
      "Iteration 3682: loss -3.4551095962524414\n",
      "Iteration 3683: loss -3.3681838512420654\n",
      "Iteration 3684: loss -3.5616610050201416\n",
      "Iteration 3685: loss -3.380044460296631\n",
      "Iteration 3686: loss -3.2342922687530518\n",
      "Iteration 3687: loss -3.43342661857605\n",
      "Iteration 3688: loss -3.2374050617218018\n",
      "Iteration 3689: loss -3.3468849658966064\n",
      "Iteration 3690: loss -3.3449878692626953\n",
      "Iteration 3691: loss -3.3468334674835205\n",
      "Iteration 3692: loss -3.4803783893585205\n",
      "Iteration 3693: loss -3.405726671218872\n",
      "Iteration 3694: loss -3.3658347129821777\n",
      "Iteration 3695: loss -3.4790685176849365\n",
      "Iteration 3696: loss -3.3889458179473877\n",
      "Iteration 3697: loss -3.3836891651153564\n",
      "Iteration 3698: loss -3.5287418365478516\n",
      "Iteration 3699: loss -3.495863437652588\n",
      "Iteration 3700: loss -3.3374245166778564\n",
      "Iteration 3701: loss -3.3768393993377686\n",
      "Iteration 3702: loss -3.5032448768615723\n",
      "Iteration 3703: loss -3.47005033493042\n",
      "Iteration 3704: loss -3.3580939769744873\n",
      "Iteration 3705: loss -3.4647815227508545\n",
      "Iteration 3706: loss -3.5380823612213135\n",
      "Iteration 3707: loss -3.556941270828247\n",
      "Iteration 3708: loss -3.360241651535034\n",
      "Iteration 3709: loss -3.2916088104248047\n",
      "Iteration 3710: loss -3.465489387512207\n",
      "Iteration 3711: loss -3.4300384521484375\n",
      "Iteration 3712: loss -3.4828941822052\n",
      "Iteration 3713: loss -3.3273677825927734\n",
      "Iteration 3714: loss -3.3889288902282715\n",
      "Iteration 3715: loss -3.5796444416046143\n",
      "Iteration 3716: loss -3.4471423625946045\n",
      "Iteration 3717: loss -3.5329132080078125\n",
      "Iteration 3718: loss -3.333125114440918\n",
      "Iteration 3719: loss -3.46773624420166\n",
      "Iteration 3720: loss -3.2889091968536377\n",
      "Iteration 3721: loss -3.4231081008911133\n",
      "Iteration 3722: loss -3.4877729415893555\n",
      "Iteration 3723: loss -3.48844051361084\n",
      "Iteration 3724: loss -3.4603826999664307\n",
      "Iteration 3725: loss -3.4024264812469482\n",
      "Iteration 3726: loss -3.4452402591705322\n",
      "Iteration 3727: loss -3.395109176635742\n",
      "Iteration 3728: loss -3.4287524223327637\n",
      "Iteration 3729: loss -3.3526053428649902\n",
      "Iteration 3730: loss -3.4141852855682373\n",
      "Iteration 3731: loss -3.3861703872680664\n",
      "Iteration 3732: loss -3.2776803970336914\n",
      "Iteration 3733: loss -3.535961627960205\n",
      "Iteration 3734: loss -3.4535791873931885\n",
      "Iteration 3735: loss -3.4686899185180664\n",
      "Iteration 3736: loss -3.5394256114959717\n",
      "Iteration 3737: loss -3.3560118675231934\n",
      "Iteration 3738: loss -3.41762638092041\n",
      "Iteration 3739: loss -3.2777833938598633\n",
      "Iteration 3740: loss -3.353656530380249\n",
      "Iteration 3741: loss -3.3173112869262695\n",
      "Iteration 3742: loss -3.326530694961548\n",
      "Iteration 3743: loss -3.1871044635772705\n",
      "Iteration 3744: loss -3.314493179321289\n",
      "Iteration 3745: loss -3.360912561416626\n",
      "Iteration 3746: loss -3.2885775566101074\n",
      "Iteration 3747: loss -3.431114435195923\n",
      "Iteration 3748: loss -3.3754677772521973\n",
      "Iteration 3749: loss -3.312495708465576\n",
      "Iteration 3750: loss -3.3518381118774414\n",
      "Iteration 3751: loss -3.3687784671783447\n",
      "Iteration 3752: loss -3.4605414867401123\n",
      "Iteration 3753: loss -3.436325788497925\n",
      "Iteration 3754: loss -3.5345239639282227\n",
      "Iteration 3755: loss -3.453425407409668\n",
      "Iteration 3756: loss -3.2448983192443848\n",
      "Iteration 3757: loss -3.4555575847625732\n",
      "Iteration 3758: loss -3.308403253555298\n",
      "Iteration 3759: loss -3.374600887298584\n",
      "Iteration 3760: loss -3.303041696548462\n",
      "Iteration 3761: loss -3.337256669998169\n",
      "Iteration 3762: loss -3.534432888031006\n",
      "Iteration 3763: loss -3.439220905303955\n",
      "Iteration 3764: loss -3.552260637283325\n",
      "Iteration 3765: loss -3.5205068588256836\n",
      "Iteration 3766: loss -3.376852035522461\n",
      "Iteration 3767: loss -3.190653085708618\n",
      "Iteration 3768: loss -3.419743061065674\n",
      "Iteration 3769: loss -3.4981086254119873\n",
      "Iteration 3770: loss -3.3288135528564453\n",
      "Iteration 3771: loss -3.506201982498169\n",
      "Iteration 3772: loss -3.365584135055542\n",
      "Iteration 3773: loss -3.2622833251953125\n",
      "Iteration 3774: loss -3.499384880065918\n",
      "Iteration 3775: loss -3.508732318878174\n",
      "Iteration 3776: loss -3.305584192276001\n",
      "Iteration 3777: loss -3.3440630435943604\n",
      "Iteration 3778: loss -3.3284366130828857\n",
      "Iteration 3779: loss -3.181992292404175\n",
      "Iteration 3780: loss -3.521557092666626\n",
      "Iteration 3781: loss -3.3896961212158203\n",
      "Iteration 3782: loss -3.296645402908325\n",
      "Iteration 3783: loss -3.50374698638916\n",
      "Iteration 3784: loss -3.1120543479919434\n",
      "Iteration 3785: loss -3.434375047683716\n",
      "Iteration 3786: loss -3.557913303375244\n",
      "Iteration 3787: loss -3.219367265701294\n",
      "Iteration 3788: loss -3.4990975856781006\n",
      "Iteration 3789: loss -3.3504514694213867\n",
      "Iteration 3790: loss -3.333665132522583\n",
      "Iteration 3791: loss -3.4327504634857178\n",
      "Iteration 3792: loss -3.383054256439209\n",
      "Iteration 3793: loss -3.3116953372955322\n",
      "Iteration 3794: loss -3.3913586139678955\n",
      "Iteration 3795: loss -3.4733669757843018\n",
      "Iteration 3796: loss -3.269014596939087\n",
      "Iteration 3797: loss -3.427459955215454\n",
      "Iteration 3798: loss -3.484619140625\n",
      "Iteration 3799: loss -3.373481512069702\n",
      "Iteration 3800: loss -3.300759792327881\n",
      "Iteration 3801: loss -3.403890371322632\n",
      "Iteration 3802: loss -3.467672109603882\n",
      "Iteration 3803: loss -3.450047016143799\n",
      "Iteration 3804: loss -3.3584792613983154\n",
      "Iteration 3805: loss -3.373185157775879\n",
      "Iteration 3806: loss -3.3299810886383057\n",
      "Iteration 3807: loss -3.5676915645599365\n",
      "Iteration 3808: loss -3.46431040763855\n",
      "Iteration 3809: loss -3.3583264350891113\n",
      "Iteration 3810: loss -3.153944730758667\n",
      "Iteration 3811: loss -3.428462266921997\n",
      "Iteration 3812: loss -3.266728401184082\n",
      "Iteration 3813: loss -3.277082920074463\n",
      "Iteration 3814: loss -3.325352430343628\n",
      "Iteration 3815: loss -3.5018041133880615\n",
      "Iteration 3816: loss -3.3975870609283447\n",
      "Iteration 3817: loss -3.29518461227417\n",
      "Iteration 3818: loss -3.3967947959899902\n",
      "Iteration 3819: loss -3.3223907947540283\n",
      "Iteration 3820: loss -3.50416898727417\n",
      "Iteration 3821: loss -3.320096492767334\n",
      "Iteration 3822: loss -3.3919293880462646\n",
      "Iteration 3823: loss -3.256232500076294\n",
      "Iteration 3824: loss -3.439150333404541\n",
      "Iteration 3825: loss -3.241008996963501\n",
      "Iteration 3826: loss -3.3740503787994385\n",
      "Iteration 3827: loss -3.2535736560821533\n",
      "Iteration 3828: loss -3.2569634914398193\n",
      "Iteration 3829: loss -3.5302956104278564\n",
      "Iteration 3830: loss -3.401848554611206\n",
      "Iteration 3831: loss -3.2811439037323\n",
      "Iteration 3832: loss -3.4125778675079346\n",
      "Iteration 3833: loss -3.340632915496826\n",
      "Iteration 3834: loss -3.347231149673462\n",
      "Iteration 3835: loss -3.5224387645721436\n",
      "Iteration 3836: loss -3.634502649307251\n",
      "Iteration 3837: loss -3.4722490310668945\n",
      "Iteration 3838: loss -3.453791856765747\n",
      "Iteration 3839: loss -3.5064644813537598\n",
      "Iteration 3840: loss -3.3600351810455322\n",
      "Iteration 3841: loss -3.4707863330841064\n",
      "Iteration 3842: loss -3.4505276679992676\n",
      "Iteration 3843: loss -3.524099349975586\n",
      "Iteration 3844: loss -3.4215762615203857\n",
      "Iteration 3845: loss -3.438742160797119\n",
      "Iteration 3846: loss -3.3969151973724365\n",
      "Iteration 3847: loss -3.545875310897827\n",
      "Iteration 3848: loss -3.252016544342041\n",
      "Iteration 3849: loss -3.4161572456359863\n",
      "Iteration 3850: loss -3.3693687915802\n",
      "Iteration 3851: loss -3.221832275390625\n",
      "Iteration 3852: loss -3.4685983657836914\n",
      "Iteration 3853: loss -3.086848735809326\n",
      "Iteration 3854: loss -3.4882757663726807\n",
      "Iteration 3855: loss -3.5798041820526123\n",
      "Iteration 3856: loss -3.3474669456481934\n",
      "Iteration 3857: loss -3.5696306228637695\n",
      "Iteration 3858: loss -3.39017391204834\n",
      "Iteration 3859: loss -3.479668378829956\n",
      "Iteration 3860: loss -3.4925243854522705\n",
      "Iteration 3861: loss -3.5329413414001465\n",
      "Iteration 3862: loss -3.5398082733154297\n",
      "Iteration 3863: loss -3.363426923751831\n",
      "Iteration 3864: loss -3.50662899017334\n",
      "Iteration 3865: loss -3.1942250728607178\n",
      "Iteration 3866: loss -3.4612069129943848\n",
      "Iteration 3867: loss -3.2407212257385254\n",
      "Iteration 3868: loss -3.3010637760162354\n",
      "Iteration 3869: loss -3.4628746509552\n",
      "Iteration 3870: loss -3.4454469680786133\n",
      "Iteration 3871: loss -3.3978612422943115\n",
      "Iteration 3872: loss -3.1387932300567627\n",
      "Iteration 3873: loss -3.3504462242126465\n",
      "Iteration 3874: loss -3.3811490535736084\n",
      "Iteration 3875: loss -3.3836939334869385\n",
      "Iteration 3876: loss -3.563828706741333\n",
      "Iteration 3877: loss -3.5293068885803223\n",
      "Iteration 3878: loss -3.4722018241882324\n",
      "Iteration 3879: loss -3.439542770385742\n",
      "Iteration 3880: loss -3.428598403930664\n",
      "Iteration 3881: loss -3.2453479766845703\n",
      "Iteration 3882: loss -3.4443860054016113\n",
      "Iteration 3883: loss -3.394700527191162\n",
      "Iteration 3884: loss -3.34962797164917\n",
      "Iteration 3885: loss -3.3991403579711914\n",
      "Iteration 3886: loss -3.4415903091430664\n",
      "Iteration 3887: loss -3.4361650943756104\n",
      "Iteration 3888: loss -3.556178092956543\n",
      "Iteration 3889: loss -3.3809328079223633\n",
      "Iteration 3890: loss -3.4827933311462402\n",
      "Iteration 3891: loss -3.3901283740997314\n",
      "Iteration 3892: loss -3.405010461807251\n",
      "Iteration 3893: loss -3.3959691524505615\n",
      "Iteration 3894: loss -3.4116361141204834\n",
      "Iteration 3895: loss -3.413470506668091\n",
      "Iteration 3896: loss -3.375964641571045\n",
      "Iteration 3897: loss -3.444193124771118\n",
      "Iteration 3898: loss -3.452277421951294\n",
      "Iteration 3899: loss -3.2789146900177\n",
      "Iteration 3900: loss -3.369065284729004\n",
      "Iteration 3901: loss -3.3218483924865723\n",
      "Iteration 3902: loss -3.322622299194336\n",
      "Iteration 3903: loss -3.4056899547576904\n",
      "Iteration 3904: loss -3.448765754699707\n",
      "Iteration 3905: loss -3.4509143829345703\n",
      "Iteration 3906: loss -3.3115406036376953\n",
      "Iteration 3907: loss -3.575613498687744\n",
      "Iteration 3908: loss -3.5306921005249023\n",
      "Iteration 3909: loss -3.5282254219055176\n",
      "Iteration 3910: loss -3.446986436843872\n",
      "Iteration 3911: loss -3.35078763961792\n",
      "Iteration 3912: loss -3.3554465770721436\n",
      "Iteration 3913: loss -3.481208086013794\n",
      "Iteration 3914: loss -3.378777265548706\n",
      "Iteration 3915: loss -3.4154045581817627\n",
      "Iteration 3916: loss -3.4497861862182617\n",
      "Iteration 3917: loss -3.3793888092041016\n",
      "Iteration 3918: loss -3.4524335861206055\n",
      "Iteration 3919: loss -3.4293761253356934\n",
      "Iteration 3920: loss -3.275256872177124\n",
      "Iteration 3921: loss -3.53944730758667\n",
      "Iteration 3922: loss -3.525285005569458\n",
      "Iteration 3923: loss -3.286499261856079\n",
      "Iteration 3924: loss -3.2368524074554443\n",
      "Iteration 3925: loss -3.382563829421997\n",
      "Iteration 3926: loss -3.306499719619751\n",
      "Iteration 3927: loss -3.4711503982543945\n",
      "Iteration 3928: loss -3.5015974044799805\n",
      "Iteration 3929: loss -3.4290642738342285\n",
      "Iteration 3930: loss -3.3898556232452393\n",
      "Iteration 3931: loss -3.4587602615356445\n",
      "Iteration 3932: loss -3.472058057785034\n",
      "Iteration 3933: loss -3.4614858627319336\n",
      "Iteration 3934: loss -3.451629400253296\n",
      "Iteration 3935: loss -3.493637800216675\n",
      "Iteration 3936: loss -3.5302486419677734\n",
      "Iteration 3937: loss -3.4967241287231445\n",
      "Iteration 3938: loss -3.4321529865264893\n",
      "Iteration 3939: loss -3.378124952316284\n",
      "Iteration 3940: loss -3.6079909801483154\n",
      "Iteration 3941: loss -3.4795775413513184\n",
      "Iteration 3942: loss -3.335984230041504\n",
      "Iteration 3943: loss -3.475147008895874\n",
      "Iteration 3944: loss -3.3420944213867188\n",
      "Iteration 3945: loss -3.372211217880249\n",
      "Iteration 3946: loss -3.2965075969696045\n",
      "Iteration 3947: loss -3.273690700531006\n",
      "Iteration 3948: loss -3.0813000202178955\n",
      "Iteration 3949: loss -3.5060300827026367\n",
      "Iteration 3950: loss -3.1660633087158203\n",
      "Iteration 3951: loss -3.4556851387023926\n",
      "Iteration 3952: loss -3.2986972332000732\n",
      "Iteration 3953: loss -3.315310478210449\n",
      "Iteration 3954: loss -3.402506113052368\n",
      "Iteration 3955: loss -3.283705949783325\n",
      "Iteration 3956: loss -3.45744252204895\n",
      "Iteration 3957: loss -3.250964879989624\n",
      "Iteration 3958: loss -3.4327292442321777\n",
      "Iteration 3959: loss -3.3683297634124756\n",
      "Iteration 3960: loss -3.2918949127197266\n",
      "Iteration 3961: loss -3.517298698425293\n",
      "Iteration 3962: loss -3.338831901550293\n",
      "Iteration 3963: loss -3.3580048084259033\n",
      "Iteration 3964: loss -3.275635004043579\n",
      "Iteration 3965: loss -3.4051380157470703\n",
      "Iteration 3966: loss -3.310793161392212\n",
      "Iteration 3967: loss -3.323211669921875\n",
      "Iteration 3968: loss -3.353367567062378\n",
      "Iteration 3969: loss -3.3943164348602295\n",
      "Iteration 3970: loss -3.404501438140869\n",
      "Iteration 3971: loss -3.539642095565796\n",
      "Iteration 3972: loss -3.4215142726898193\n",
      "Iteration 3973: loss -3.329306125640869\n",
      "Iteration 3974: loss -3.438433885574341\n",
      "Iteration 3975: loss -3.403744697570801\n",
      "Iteration 3976: loss -3.424912691116333\n",
      "Iteration 3977: loss -3.4774227142333984\n",
      "Iteration 3978: loss -3.434457778930664\n",
      "Iteration 3979: loss -3.4514238834381104\n",
      "Iteration 3980: loss -3.2828214168548584\n",
      "Iteration 3981: loss -3.333209276199341\n",
      "Iteration 3982: loss -3.2976748943328857\n",
      "Iteration 3983: loss -3.4776384830474854\n",
      "Iteration 3984: loss -3.5752973556518555\n",
      "Iteration 3985: loss -3.3759121894836426\n",
      "Iteration 3986: loss -3.589658737182617\n",
      "Iteration 3987: loss -3.527252435684204\n",
      "Iteration 3988: loss -3.5064971446990967\n",
      "Iteration 3989: loss -3.4346704483032227\n",
      "Iteration 3990: loss -3.5646157264709473\n",
      "Iteration 3991: loss -3.405649423599243\n",
      "Iteration 3992: loss -3.3707268238067627\n",
      "Iteration 3993: loss -3.5324361324310303\n",
      "Iteration 3994: loss -3.360682487487793\n",
      "Iteration 3995: loss -3.382319450378418\n",
      "Iteration 3996: loss -3.3335165977478027\n",
      "Iteration 3997: loss -3.337268352508545\n",
      "Iteration 3998: loss -3.5831661224365234\n",
      "Iteration 3999: loss -3.406736373901367\n",
      "Iteration 4000: loss -3.343435049057007\n",
      "Iteration 4001: loss -3.386892795562744\n",
      "Iteration 4002: loss -3.322500705718994\n",
      "Iteration 4003: loss -3.541757106781006\n",
      "Iteration 4004: loss -3.262084722518921\n",
      "Iteration 4005: loss -3.43997859954834\n",
      "Iteration 4006: loss -3.4852945804595947\n",
      "Iteration 4007: loss -3.558375835418701\n",
      "Iteration 4008: loss -3.2830705642700195\n",
      "Iteration 4009: loss -3.3732025623321533\n",
      "Iteration 4010: loss -3.5201830863952637\n",
      "Iteration 4011: loss -3.4808766841888428\n",
      "Iteration 4012: loss -3.4527533054351807\n",
      "Iteration 4013: loss -3.3219432830810547\n",
      "Iteration 4014: loss -3.184887170791626\n",
      "Iteration 4015: loss -3.367950201034546\n",
      "Iteration 4016: loss -3.1491613388061523\n",
      "Iteration 4017: loss -3.2760634422302246\n",
      "Iteration 4018: loss -3.2891852855682373\n",
      "Iteration 4019: loss -2.637357473373413\n",
      "Iteration 4020: loss -3.261758327484131\n",
      "Iteration 4021: loss -3.217484474182129\n",
      "Iteration 4022: loss -3.0540759563446045\n",
      "Iteration 4023: loss -3.254713773727417\n",
      "Iteration 4024: loss -3.120479106903076\n",
      "Iteration 4025: loss -2.781965970993042\n",
      "Iteration 4026: loss -3.1881799697875977\n",
      "Iteration 4027: loss -3.3627612590789795\n",
      "Iteration 4028: loss -2.9536304473876953\n",
      "Iteration 4029: loss -3.1988213062286377\n",
      "Iteration 4030: loss -3.2737221717834473\n",
      "Iteration 4031: loss -3.010371446609497\n",
      "Iteration 4032: loss -2.848452568054199\n",
      "Iteration 4033: loss -1.9244056940078735\n",
      "Iteration 4034: loss -1.801281452178955\n",
      "Iteration 4035: loss -2.498307228088379\n",
      "Iteration 4036: loss -2.4452943801879883\n",
      "Iteration 4037: loss -2.28078556060791\n",
      "Iteration 4038: loss -2.5703554153442383\n",
      "Iteration 4039: loss -2.6338624954223633\n",
      "Iteration 4040: loss -2.6294610500335693\n",
      "Iteration 4041: loss -2.5733096599578857\n",
      "Iteration 4042: loss -2.8647758960723877\n",
      "Iteration 4043: loss -2.704758644104004\n",
      "Iteration 4044: loss -2.9646384716033936\n",
      "Iteration 4045: loss -2.9127533435821533\n",
      "Iteration 4046: loss -2.7075018882751465\n",
      "Iteration 4047: loss -2.7407195568084717\n",
      "Iteration 4048: loss -3.014754295349121\n",
      "Iteration 4049: loss -2.9454281330108643\n",
      "Iteration 4050: loss -2.8955130577087402\n",
      "Iteration 4051: loss -3.008150577545166\n",
      "Iteration 4052: loss -3.163816213607788\n",
      "Iteration 4053: loss -2.8497259616851807\n",
      "Iteration 4054: loss -2.968679189682007\n",
      "Iteration 4055: loss -3.0406928062438965\n",
      "Iteration 4056: loss -3.083791494369507\n",
      "Iteration 4057: loss -3.007230758666992\n",
      "Iteration 4058: loss -2.880892038345337\n",
      "Iteration 4059: loss -3.047090768814087\n",
      "Iteration 4060: loss -3.081885576248169\n",
      "Iteration 4061: loss -3.1461341381073\n",
      "Iteration 4062: loss -3.1952359676361084\n",
      "Iteration 4063: loss -3.0961039066314697\n",
      "Iteration 4064: loss -3.1959338188171387\n",
      "Iteration 4065: loss -3.19509220123291\n",
      "Iteration 4066: loss -3.2363529205322266\n",
      "Iteration 4067: loss -3.0766470432281494\n",
      "Iteration 4068: loss -3.1872315406799316\n",
      "Iteration 4069: loss -3.3101396560668945\n",
      "Iteration 4070: loss -3.3061208724975586\n",
      "Iteration 4071: loss -3.2613914012908936\n",
      "Iteration 4072: loss -3.2449231147766113\n",
      "Iteration 4073: loss -3.325207471847534\n",
      "Iteration 4074: loss -3.340947151184082\n",
      "Iteration 4075: loss -3.4091341495513916\n",
      "Iteration 4076: loss -3.2604148387908936\n",
      "Iteration 4077: loss -3.371181011199951\n",
      "Iteration 4078: loss -3.148491621017456\n",
      "Iteration 4079: loss -3.297830104827881\n",
      "Iteration 4080: loss -3.1997334957122803\n",
      "Iteration 4081: loss -3.3090145587921143\n",
      "Iteration 4082: loss -3.5332884788513184\n",
      "Iteration 4083: loss -3.408792734146118\n",
      "Iteration 4084: loss -3.3561017513275146\n",
      "Iteration 4085: loss -3.416360855102539\n",
      "Iteration 4086: loss -3.3825457096099854\n",
      "Iteration 4087: loss -3.2523772716522217\n",
      "Iteration 4088: loss -3.4037585258483887\n",
      "Iteration 4089: loss -3.3328638076782227\n",
      "Iteration 4090: loss -3.3716700077056885\n",
      "Iteration 4091: loss -3.490546464920044\n",
      "Iteration 4092: loss -3.4359488487243652\n",
      "Iteration 4093: loss -3.3628058433532715\n",
      "Iteration 4094: loss -3.4213733673095703\n",
      "Iteration 4095: loss -3.313244104385376\n",
      "Iteration 4096: loss -3.364372491836548\n",
      "Iteration 4097: loss -3.335564374923706\n",
      "Iteration 4098: loss -3.331533193588257\n",
      "Iteration 4099: loss -3.3381848335266113\n",
      "Iteration 4100: loss -3.4665937423706055\n",
      "Iteration 4101: loss -3.2307116985321045\n",
      "Iteration 4102: loss -3.232423782348633\n",
      "Iteration 4103: loss -3.2289834022521973\n",
      "Iteration 4104: loss -3.224069595336914\n",
      "Iteration 4105: loss -2.787656545639038\n",
      "Iteration 4106: loss -3.1182479858398438\n",
      "Iteration 4107: loss -3.409773588180542\n",
      "Iteration 4108: loss -3.0690419673919678\n",
      "Iteration 4109: loss -2.9602646827697754\n",
      "Iteration 4110: loss -3.1125988960266113\n",
      "Iteration 4111: loss -3.15926456451416\n",
      "Iteration 4112: loss -3.111818552017212\n",
      "Iteration 4113: loss -3.2451672554016113\n",
      "Iteration 4114: loss -3.097032070159912\n",
      "Iteration 4115: loss -3.2935945987701416\n",
      "Iteration 4116: loss -3.1969354152679443\n",
      "Iteration 4117: loss -3.1668012142181396\n",
      "Iteration 4118: loss -3.3152379989624023\n",
      "Iteration 4119: loss -3.294367790222168\n",
      "Iteration 4120: loss -3.1861565113067627\n",
      "Iteration 4121: loss -3.2214958667755127\n",
      "Iteration 4122: loss -3.206502914428711\n",
      "Iteration 4123: loss -3.3700006008148193\n",
      "Iteration 4124: loss -3.348069190979004\n",
      "Iteration 4125: loss -3.2724673748016357\n",
      "Iteration 4126: loss -3.3962185382843018\n",
      "Iteration 4127: loss -3.499044895172119\n",
      "Iteration 4128: loss -3.4534969329833984\n",
      "Iteration 4129: loss -3.2610137462615967\n",
      "Iteration 4130: loss -3.3787550926208496\n",
      "Iteration 4131: loss -3.3276267051696777\n",
      "Iteration 4132: loss -3.480130672454834\n",
      "Iteration 4133: loss -3.422915458679199\n",
      "Iteration 4134: loss -3.311352491378784\n",
      "Iteration 4135: loss -3.2834787368774414\n",
      "Iteration 4136: loss -3.441530704498291\n",
      "Iteration 4137: loss -3.405827283859253\n",
      "Iteration 4138: loss -3.4768011569976807\n",
      "Iteration 4139: loss -3.5825917720794678\n",
      "Iteration 4140: loss -3.4208176136016846\n",
      "Iteration 4141: loss -3.397585868835449\n",
      "Iteration 4142: loss -3.384345054626465\n",
      "Iteration 4143: loss -3.4724252223968506\n",
      "Iteration 4144: loss -3.3299055099487305\n",
      "Iteration 4145: loss -3.259211540222168\n",
      "Iteration 4146: loss -3.4328997135162354\n",
      "Iteration 4147: loss -3.468677282333374\n",
      "Iteration 4148: loss -3.212801456451416\n",
      "Iteration 4149: loss -3.3073174953460693\n",
      "Iteration 4150: loss -3.1785452365875244\n",
      "Iteration 4151: loss -3.511430263519287\n",
      "Iteration 4152: loss -3.4521472454071045\n",
      "Iteration 4153: loss -3.329385995864868\n",
      "Iteration 4154: loss -3.5264511108398438\n",
      "Iteration 4155: loss -3.497084140777588\n",
      "Iteration 4156: loss -3.5987513065338135\n",
      "Iteration 4157: loss -3.527857780456543\n",
      "Iteration 4158: loss -3.468506097793579\n",
      "Iteration 4159: loss -3.483081340789795\n",
      "Iteration 4160: loss -3.515669822692871\n",
      "Iteration 4161: loss -3.5380234718322754\n",
      "Iteration 4162: loss -3.3352019786834717\n",
      "Iteration 4163: loss -3.410813093185425\n",
      "Iteration 4164: loss -3.5373406410217285\n",
      "Iteration 4165: loss -3.5420567989349365\n",
      "Iteration 4166: loss -3.4670777320861816\n",
      "Iteration 4167: loss -3.5976953506469727\n",
      "Iteration 4168: loss -3.5441694259643555\n",
      "Iteration 4169: loss -3.2942049503326416\n",
      "Iteration 4170: loss -3.3923943042755127\n",
      "Iteration 4171: loss -3.3818209171295166\n",
      "Iteration 4172: loss -3.398606538772583\n",
      "Iteration 4173: loss -3.4440224170684814\n",
      "Iteration 4174: loss -3.1668057441711426\n",
      "Iteration 4175: loss -3.4796526432037354\n",
      "Iteration 4176: loss -3.2077934741973877\n",
      "Iteration 4177: loss -3.436614990234375\n",
      "Iteration 4178: loss -3.3101494312286377\n",
      "Iteration 4179: loss -3.3955914974212646\n",
      "Iteration 4180: loss -3.422157049179077\n",
      "Iteration 4181: loss -3.4764108657836914\n",
      "Iteration 4182: loss -3.4950690269470215\n",
      "Iteration 4183: loss -3.356192111968994\n",
      "Iteration 4184: loss -3.317293643951416\n",
      "Iteration 4185: loss -3.4927985668182373\n",
      "Iteration 4186: loss -3.5952179431915283\n",
      "Iteration 4187: loss -2.8946752548217773\n",
      "Iteration 4188: loss -3.393988847732544\n",
      "Iteration 4189: loss -3.3891992568969727\n",
      "Iteration 4190: loss -3.3490993976593018\n",
      "Iteration 4191: loss -3.3208627700805664\n",
      "Iteration 4192: loss -3.2903265953063965\n",
      "Iteration 4193: loss -3.5070862770080566\n",
      "Iteration 4194: loss -3.2412045001983643\n",
      "Iteration 4195: loss -3.2353317737579346\n",
      "Iteration 4196: loss -3.4716556072235107\n",
      "Iteration 4197: loss -3.3537023067474365\n",
      "Iteration 4198: loss -3.1668901443481445\n",
      "Iteration 4199: loss -3.4850330352783203\n",
      "Iteration 4200: loss -3.4970386028289795\n",
      "Iteration 4201: loss -3.329472541809082\n",
      "Iteration 4202: loss -3.355548620223999\n",
      "Iteration 4203: loss -3.4920356273651123\n",
      "Iteration 4204: loss -3.3972368240356445\n",
      "Iteration 4205: loss -3.4786429405212402\n",
      "Iteration 4206: loss -3.266292095184326\n",
      "Iteration 4207: loss -3.50990629196167\n",
      "Iteration 4208: loss -3.55049204826355\n",
      "Iteration 4209: loss -3.414846420288086\n",
      "Iteration 4210: loss -3.2087035179138184\n",
      "Iteration 4211: loss -3.4108574390411377\n",
      "Iteration 4212: loss -3.4616408348083496\n",
      "Iteration 4213: loss -3.3000550270080566\n",
      "Iteration 4214: loss -3.388741970062256\n",
      "Iteration 4215: loss -3.6129150390625\n",
      "Iteration 4216: loss -3.5254058837890625\n",
      "Iteration 4217: loss -3.2655529975891113\n",
      "Iteration 4218: loss -3.3405346870422363\n",
      "Iteration 4219: loss -3.5399913787841797\n",
      "Iteration 4220: loss -3.4697492122650146\n",
      "Iteration 4221: loss -3.4912400245666504\n",
      "Iteration 4222: loss -3.2824697494506836\n",
      "Iteration 4223: loss -3.3325772285461426\n",
      "Iteration 4224: loss -3.3930447101593018\n",
      "Iteration 4225: loss -3.3560187816619873\n",
      "Iteration 4226: loss -3.3200948238372803\n",
      "Iteration 4227: loss -3.5592870712280273\n",
      "Iteration 4228: loss -3.3817925453186035\n",
      "Iteration 4229: loss -3.470444917678833\n",
      "Iteration 4230: loss -3.3316664695739746\n",
      "Iteration 4231: loss -3.3494479656219482\n",
      "Iteration 4232: loss -3.4434094429016113\n",
      "Iteration 4233: loss -3.308711528778076\n",
      "Iteration 4234: loss -3.4636452198028564\n",
      "Iteration 4235: loss -3.378155469894409\n",
      "Iteration 4236: loss -3.372129201889038\n",
      "Iteration 4237: loss -3.5876212120056152\n",
      "Iteration 4238: loss -3.4502692222595215\n",
      "Iteration 4239: loss -3.161712646484375\n",
      "Iteration 4240: loss -3.3931827545166016\n",
      "Iteration 4241: loss -3.396799325942993\n",
      "Iteration 4242: loss -3.48207950592041\n",
      "Iteration 4243: loss -3.4451751708984375\n",
      "Iteration 4244: loss -3.1955573558807373\n",
      "Iteration 4245: loss -3.6069977283477783\n",
      "Iteration 4246: loss -3.4132766723632812\n",
      "Iteration 4247: loss -3.380270004272461\n",
      "Iteration 4248: loss -3.272700786590576\n",
      "Iteration 4249: loss -3.1968142986297607\n",
      "Iteration 4250: loss -3.299131155014038\n",
      "Iteration 4251: loss -3.54349946975708\n",
      "Iteration 4252: loss -3.3381989002227783\n",
      "Iteration 4253: loss -3.5001726150512695\n",
      "Iteration 4254: loss -3.5277352333068848\n",
      "Iteration 4255: loss -3.303617000579834\n",
      "Iteration 4256: loss -3.3852481842041016\n",
      "Iteration 4257: loss -3.478975534439087\n",
      "Iteration 4258: loss -3.3181772232055664\n",
      "Iteration 4259: loss -3.4480631351470947\n",
      "Iteration 4260: loss -3.4780144691467285\n",
      "Iteration 4261: loss -3.357464551925659\n",
      "Iteration 4262: loss -3.405245065689087\n",
      "Iteration 4263: loss -3.4008517265319824\n",
      "Iteration 4264: loss -3.3309073448181152\n",
      "Iteration 4265: loss -3.5061349868774414\n",
      "Iteration 4266: loss -3.361522912979126\n",
      "Iteration 4267: loss -3.405177116394043\n",
      "Iteration 4268: loss -3.355914354324341\n",
      "Iteration 4269: loss -3.524259090423584\n",
      "Iteration 4270: loss -3.452416181564331\n",
      "Iteration 4271: loss -3.4668707847595215\n",
      "Iteration 4272: loss -3.440620183944702\n",
      "Iteration 4273: loss -3.320939540863037\n",
      "Iteration 4274: loss -3.596061944961548\n",
      "Iteration 4275: loss -3.395430326461792\n",
      "Iteration 4276: loss -3.412152051925659\n",
      "Iteration 4277: loss -3.481335401535034\n",
      "Iteration 4278: loss -3.42641544342041\n",
      "Iteration 4279: loss -3.503894567489624\n",
      "Iteration 4280: loss -3.297576904296875\n",
      "Iteration 4281: loss -3.465252637863159\n",
      "Iteration 4282: loss -3.3733584880828857\n",
      "Iteration 4283: loss -3.2658557891845703\n",
      "Iteration 4284: loss -3.520648241043091\n",
      "Iteration 4285: loss -3.4809811115264893\n",
      "Iteration 4286: loss -3.3672966957092285\n",
      "Iteration 4287: loss -3.319594621658325\n",
      "Iteration 4288: loss -3.369046211242676\n",
      "Iteration 4289: loss -3.3890740871429443\n",
      "Iteration 4290: loss -3.4434385299682617\n",
      "Iteration 4291: loss -3.3887436389923096\n",
      "Iteration 4292: loss -3.4641218185424805\n",
      "Iteration 4293: loss -2.9114904403686523\n",
      "Iteration 4294: loss -3.5463016033172607\n",
      "Iteration 4295: loss -3.3541834354400635\n",
      "Iteration 4296: loss -3.280930995941162\n",
      "Iteration 4297: loss -3.426481246948242\n",
      "Iteration 4298: loss -3.43721866607666\n",
      "Iteration 4299: loss -3.2392563819885254\n",
      "Iteration 4300: loss -3.071699857711792\n",
      "Iteration 4301: loss -3.5521209239959717\n",
      "Iteration 4302: loss -3.1715338230133057\n",
      "Iteration 4303: loss -3.1943464279174805\n",
      "Iteration 4304: loss -3.436037063598633\n",
      "Iteration 4305: loss -3.3195793628692627\n",
      "Iteration 4306: loss -3.307875394821167\n",
      "Iteration 4307: loss -3.511786460876465\n",
      "Iteration 4308: loss -3.368701696395874\n",
      "Iteration 4309: loss -3.392069101333618\n",
      "Iteration 4310: loss -3.329167366027832\n",
      "Iteration 4311: loss -3.516660690307617\n",
      "Iteration 4312: loss -3.2850589752197266\n",
      "Iteration 4313: loss -3.386948347091675\n",
      "Iteration 4314: loss -3.4145452976226807\n",
      "Iteration 4315: loss -3.3081936836242676\n",
      "Iteration 4316: loss -3.1618497371673584\n",
      "Iteration 4317: loss -3.2868523597717285\n",
      "Iteration 4318: loss -3.3932301998138428\n",
      "Iteration 4319: loss -3.3458728790283203\n",
      "Iteration 4320: loss -3.492696523666382\n",
      "Iteration 4321: loss -3.5326662063598633\n",
      "Iteration 4322: loss -3.358908176422119\n",
      "Iteration 4323: loss -3.5242233276367188\n",
      "Iteration 4324: loss -3.3784966468811035\n",
      "Iteration 4325: loss -3.447580337524414\n",
      "Iteration 4326: loss -3.3723514080047607\n",
      "Iteration 4327: loss -3.434250831604004\n",
      "Iteration 4328: loss -3.4216058254241943\n",
      "Iteration 4329: loss -3.322110652923584\n",
      "Iteration 4330: loss -3.336184024810791\n",
      "Iteration 4331: loss -3.465090036392212\n",
      "Iteration 4332: loss -3.3513405323028564\n",
      "Iteration 4333: loss -3.2719998359680176\n",
      "Iteration 4334: loss -3.4164719581604004\n",
      "Iteration 4335: loss -3.4660913944244385\n",
      "Iteration 4336: loss -3.5252089500427246\n",
      "Iteration 4337: loss -3.38075590133667\n",
      "Iteration 4338: loss -3.473733425140381\n",
      "Iteration 4339: loss -3.411010980606079\n",
      "Iteration 4340: loss -3.5517473220825195\n",
      "Iteration 4341: loss -3.3052268028259277\n",
      "Iteration 4342: loss -3.4863696098327637\n",
      "Iteration 4343: loss -3.3914999961853027\n",
      "Iteration 4344: loss -3.455997943878174\n",
      "Iteration 4345: loss -3.645139217376709\n",
      "Iteration 4346: loss -3.462312936782837\n",
      "Iteration 4347: loss -3.3349051475524902\n",
      "Iteration 4348: loss -3.5029404163360596\n",
      "Iteration 4349: loss -3.3382749557495117\n",
      "Iteration 4350: loss -3.3428406715393066\n",
      "Iteration 4351: loss -3.570202112197876\n",
      "Iteration 4352: loss -3.4282426834106445\n",
      "Iteration 4353: loss -3.4044759273529053\n",
      "Iteration 4354: loss -3.1101748943328857\n",
      "Iteration 4355: loss -3.4304163455963135\n",
      "Iteration 4356: loss -3.604315757751465\n",
      "Iteration 4357: loss -3.3281586170196533\n",
      "Iteration 4358: loss -3.332188129425049\n",
      "Iteration 4359: loss -3.4073407649993896\n",
      "Iteration 4360: loss -3.381347417831421\n",
      "Iteration 4361: loss -3.3327219486236572\n",
      "Iteration 4362: loss -3.439093589782715\n",
      "Iteration 4363: loss -3.5320117473602295\n",
      "Iteration 4364: loss -3.2579116821289062\n",
      "Iteration 4365: loss -3.262531042098999\n",
      "Iteration 4366: loss -3.4609270095825195\n",
      "Iteration 4367: loss -3.414355516433716\n",
      "Iteration 4368: loss -3.4989371299743652\n",
      "Iteration 4369: loss -3.187596321105957\n",
      "Iteration 4370: loss -3.3730547428131104\n",
      "Iteration 4371: loss -3.5482354164123535\n",
      "Iteration 4372: loss -3.255275011062622\n",
      "Iteration 4373: loss -3.433471918106079\n",
      "Iteration 4374: loss -3.475335121154785\n",
      "Iteration 4375: loss -3.333535671234131\n",
      "Iteration 4376: loss -3.486452102661133\n",
      "Iteration 4377: loss -3.5113534927368164\n",
      "Iteration 4378: loss -3.436908006668091\n",
      "Iteration 4379: loss -3.528811454772949\n",
      "Iteration 4380: loss -3.3947646617889404\n",
      "Iteration 4381: loss -3.4790542125701904\n",
      "Iteration 4382: loss -3.5488781929016113\n",
      "Iteration 4383: loss -3.4161345958709717\n",
      "Iteration 4384: loss -3.4675445556640625\n",
      "Iteration 4385: loss -3.4835851192474365\n",
      "Iteration 4386: loss -3.3766751289367676\n",
      "Iteration 4387: loss -3.416806697845459\n",
      "Iteration 4388: loss -3.475621223449707\n",
      "Iteration 4389: loss -3.4997365474700928\n",
      "Iteration 4390: loss -3.342726230621338\n",
      "Iteration 4391: loss -3.0984108448028564\n",
      "Iteration 4392: loss -3.5342421531677246\n",
      "Iteration 4393: loss -3.4460089206695557\n",
      "Iteration 4394: loss -3.3511927127838135\n",
      "Iteration 4395: loss -3.4236109256744385\n",
      "Iteration 4396: loss -3.359724998474121\n",
      "Iteration 4397: loss -3.375011920928955\n",
      "Iteration 4398: loss -3.4357516765594482\n",
      "Iteration 4399: loss -3.4360764026641846\n",
      "Iteration 4400: loss -3.4843335151672363\n",
      "Iteration 4401: loss -3.2499659061431885\n",
      "Iteration 4402: loss -3.3806264400482178\n",
      "Iteration 4403: loss -3.459409475326538\n",
      "Iteration 4404: loss -3.4022557735443115\n",
      "Iteration 4405: loss -3.5006625652313232\n",
      "Iteration 4406: loss -3.457719326019287\n",
      "Iteration 4407: loss -3.338158369064331\n",
      "Iteration 4408: loss -3.3723647594451904\n",
      "Iteration 4409: loss -3.427344560623169\n",
      "Iteration 4410: loss -3.2763500213623047\n",
      "Iteration 4411: loss -3.399336576461792\n",
      "Iteration 4412: loss -3.4760513305664062\n",
      "Iteration 4413: loss -3.3386480808258057\n",
      "Iteration 4414: loss -3.3669989109039307\n",
      "Iteration 4415: loss -3.4703216552734375\n",
      "Iteration 4416: loss -3.3993334770202637\n",
      "Iteration 4417: loss -3.428253412246704\n",
      "Iteration 4418: loss -3.495189905166626\n",
      "Iteration 4419: loss -3.4806013107299805\n",
      "Iteration 4420: loss -3.590540885925293\n",
      "Iteration 4421: loss -3.3249733448028564\n",
      "Iteration 4422: loss -3.5043396949768066\n",
      "Iteration 4423: loss -3.203713893890381\n",
      "Iteration 4424: loss -3.4889473915100098\n",
      "Iteration 4425: loss -3.3897550106048584\n",
      "Iteration 4426: loss -3.4294586181640625\n",
      "Iteration 4427: loss -3.473262310028076\n",
      "Iteration 4428: loss -3.5348358154296875\n",
      "Iteration 4429: loss -3.4012041091918945\n",
      "Iteration 4430: loss -3.5570991039276123\n",
      "Iteration 4431: loss -3.357841730117798\n",
      "Iteration 4432: loss -3.419423818588257\n",
      "Iteration 4433: loss -3.3445284366607666\n",
      "Iteration 4434: loss -3.604776382446289\n",
      "Iteration 4435: loss -3.3772318363189697\n",
      "Iteration 4436: loss -3.2438790798187256\n",
      "Iteration 4437: loss -3.4381115436553955\n",
      "Iteration 4438: loss -3.4647934436798096\n",
      "Iteration 4439: loss -3.39251708984375\n",
      "Iteration 4440: loss -3.460479736328125\n",
      "Iteration 4441: loss -3.429494857788086\n",
      "Iteration 4442: loss -3.278499126434326\n",
      "Iteration 4443: loss -3.498103618621826\n",
      "Iteration 4444: loss -3.241053819656372\n",
      "Iteration 4445: loss -3.2542426586151123\n",
      "Iteration 4446: loss -3.143017053604126\n",
      "Iteration 4447: loss -3.34306001663208\n",
      "Iteration 4448: loss -3.3563873767852783\n",
      "Iteration 4449: loss -3.114015579223633\n",
      "Iteration 4450: loss -3.257877826690674\n",
      "Iteration 4451: loss -3.206306219100952\n",
      "Iteration 4452: loss -3.318319320678711\n",
      "Iteration 4453: loss -3.312347650527954\n",
      "Iteration 4454: loss -3.3952651023864746\n",
      "Iteration 4455: loss -3.2413055896759033\n",
      "Iteration 4456: loss -3.0732080936431885\n",
      "Iteration 4457: loss -3.424776554107666\n",
      "Iteration 4458: loss -3.412189245223999\n",
      "Iteration 4459: loss -3.242246150970459\n",
      "Iteration 4460: loss -3.143559455871582\n",
      "Iteration 4461: loss -3.313720703125\n",
      "Iteration 4462: loss -3.369148015975952\n",
      "Iteration 4463: loss -3.396646022796631\n",
      "Iteration 4464: loss -3.4666049480438232\n",
      "Iteration 4465: loss -3.3456835746765137\n",
      "Iteration 4466: loss -3.289998769760132\n",
      "Iteration 4467: loss -3.178189754486084\n",
      "Iteration 4468: loss -3.3474745750427246\n",
      "Iteration 4469: loss -3.5014212131500244\n",
      "Iteration 4470: loss -3.237031936645508\n",
      "Iteration 4471: loss -3.3194620609283447\n",
      "Iteration 4472: loss -3.403616428375244\n",
      "Iteration 4473: loss -3.2770352363586426\n",
      "Iteration 4474: loss -3.2601590156555176\n",
      "Iteration 4475: loss -3.271256685256958\n",
      "Iteration 4476: loss -3.3662338256835938\n",
      "Iteration 4477: loss -3.3942487239837646\n",
      "Iteration 4478: loss -3.456782341003418\n",
      "Iteration 4479: loss -3.4542856216430664\n",
      "Iteration 4480: loss -3.4250378608703613\n",
      "Iteration 4481: loss -3.5852739810943604\n",
      "Iteration 4482: loss -3.561356782913208\n",
      "Iteration 4483: loss -3.5178821086883545\n",
      "Iteration 4484: loss -3.349850654602051\n",
      "Iteration 4485: loss -3.4649033546447754\n",
      "Iteration 4486: loss -3.4129881858825684\n",
      "Iteration 4487: loss -3.40549898147583\n",
      "Iteration 4488: loss -3.526719331741333\n",
      "Iteration 4489: loss -3.412968158721924\n",
      "Iteration 4490: loss -3.428818464279175\n",
      "Iteration 4491: loss -3.468701124191284\n",
      "Iteration 4492: loss -3.5315351486206055\n",
      "Iteration 4493: loss -3.636227607727051\n",
      "Iteration 4494: loss -3.4239625930786133\n",
      "Iteration 4495: loss -3.282382011413574\n",
      "Iteration 4496: loss -3.6123692989349365\n",
      "Iteration 4497: loss -3.571667432785034\n",
      "Iteration 4498: loss -3.517622470855713\n",
      "Iteration 4499: loss -3.382960557937622\n",
      "Iteration 4500: loss -3.4629812240600586\n",
      "Iteration 4501: loss -3.505014657974243\n",
      "Iteration 4502: loss -3.5503082275390625\n",
      "Iteration 4503: loss -3.4625322818756104\n",
      "Iteration 4504: loss -3.5058372020721436\n",
      "Iteration 4505: loss -3.5670251846313477\n",
      "Iteration 4506: loss -3.5098867416381836\n",
      "Iteration 4507: loss -3.490579605102539\n",
      "Iteration 4508: loss -3.6415352821350098\n",
      "Iteration 4509: loss -3.3694164752960205\n",
      "Iteration 4510: loss -3.2849483489990234\n",
      "Iteration 4511: loss -3.257734775543213\n",
      "Iteration 4512: loss -3.489809513092041\n",
      "Iteration 4513: loss -3.4080028533935547\n",
      "Iteration 4514: loss -3.1964080333709717\n",
      "Iteration 4515: loss -3.49211049079895\n",
      "Iteration 4516: loss -3.400993585586548\n",
      "Iteration 4517: loss -3.5343008041381836\n",
      "Iteration 4518: loss -3.283294677734375\n",
      "Iteration 4519: loss -3.6030161380767822\n",
      "Iteration 4520: loss -3.285829544067383\n",
      "Iteration 4521: loss -3.4093129634857178\n",
      "Iteration 4522: loss -3.320124626159668\n",
      "Iteration 4523: loss -3.489180088043213\n",
      "Iteration 4524: loss -3.4525632858276367\n",
      "Iteration 4525: loss -3.473503828048706\n",
      "Iteration 4526: loss -3.455747604370117\n",
      "Iteration 4527: loss -3.4095065593719482\n",
      "Iteration 4528: loss -3.368112087249756\n",
      "Iteration 4529: loss -3.6113691329956055\n",
      "Iteration 4530: loss -3.427109479904175\n",
      "Iteration 4531: loss -3.5640573501586914\n",
      "Iteration 4532: loss -3.3948934078216553\n",
      "Iteration 4533: loss -3.377464532852173\n",
      "Iteration 4534: loss -3.4364194869995117\n",
      "Iteration 4535: loss -3.4078238010406494\n",
      "Iteration 4536: loss -3.543719530105591\n",
      "Iteration 4537: loss -3.283550500869751\n",
      "Iteration 4538: loss -3.396040439605713\n",
      "Iteration 4539: loss -3.482572078704834\n",
      "Iteration 4540: loss -3.193251609802246\n",
      "Iteration 4541: loss -3.5555636882781982\n",
      "Iteration 4542: loss -3.354036331176758\n",
      "Iteration 4543: loss -3.444148302078247\n",
      "Iteration 4544: loss -3.539947509765625\n",
      "Iteration 4545: loss -3.483412981033325\n",
      "Iteration 4546: loss -3.306567668914795\n",
      "Iteration 4547: loss -3.5686092376708984\n",
      "Iteration 4548: loss -3.54736328125\n",
      "Iteration 4549: loss -3.3143584728240967\n",
      "Iteration 4550: loss -3.380133628845215\n",
      "Iteration 4551: loss -3.3529388904571533\n",
      "Iteration 4552: loss -3.496506452560425\n",
      "Iteration 4553: loss -3.4907009601593018\n",
      "Iteration 4554: loss -3.6037566661834717\n",
      "Iteration 4555: loss -3.4845285415649414\n",
      "Iteration 4556: loss -3.468172073364258\n",
      "Iteration 4557: loss -3.4601476192474365\n",
      "Iteration 4558: loss -3.4250173568725586\n",
      "Iteration 4559: loss -3.541724443435669\n",
      "Iteration 4560: loss -3.515265464782715\n",
      "Iteration 4561: loss -3.4921209812164307\n",
      "Iteration 4562: loss -3.458498477935791\n",
      "Iteration 4563: loss -3.533628463745117\n",
      "Iteration 4564: loss -3.5911691188812256\n",
      "Iteration 4565: loss -3.3063366413116455\n",
      "Iteration 4566: loss -3.3991425037384033\n",
      "Iteration 4567: loss -3.601968288421631\n",
      "Iteration 4568: loss -3.539138078689575\n",
      "Iteration 4569: loss -3.410506010055542\n",
      "Iteration 4570: loss -3.4184212684631348\n",
      "Iteration 4571: loss -3.4965035915374756\n",
      "Iteration 4572: loss -3.337973117828369\n",
      "Iteration 4573: loss -3.554135799407959\n",
      "Iteration 4574: loss -3.1904494762420654\n",
      "Iteration 4575: loss -3.542450189590454\n",
      "Iteration 4576: loss -3.379927396774292\n",
      "Iteration 4577: loss -3.304316997528076\n",
      "Iteration 4578: loss -3.3682570457458496\n",
      "Iteration 4579: loss -3.477747917175293\n",
      "Iteration 4580: loss -3.1738500595092773\n",
      "Iteration 4581: loss -3.412870407104492\n",
      "Iteration 4582: loss -3.413898229598999\n",
      "Iteration 4583: loss -3.289255380630493\n",
      "Iteration 4584: loss -3.275604009628296\n",
      "Iteration 4585: loss -3.465707302093506\n",
      "Iteration 4586: loss -3.480327844619751\n",
      "Iteration 4587: loss -3.414412498474121\n",
      "Iteration 4588: loss -3.483120918273926\n",
      "Iteration 4589: loss -3.332728385925293\n",
      "Iteration 4590: loss -3.432211399078369\n",
      "Iteration 4591: loss -3.474264621734619\n",
      "Iteration 4592: loss -3.180595636367798\n",
      "Iteration 4593: loss -3.4322290420532227\n",
      "Iteration 4594: loss -3.4364824295043945\n",
      "Iteration 4595: loss -3.4507663249969482\n",
      "Iteration 4596: loss -3.3505828380584717\n",
      "Iteration 4597: loss -3.504481554031372\n",
      "Iteration 4598: loss -3.473414897918701\n",
      "Iteration 4599: loss -3.0808074474334717\n",
      "Iteration 4600: loss -3.4029605388641357\n",
      "Iteration 4601: loss -3.540118932723999\n",
      "Iteration 4602: loss -3.196500539779663\n",
      "Iteration 4603: loss -3.5895943641662598\n",
      "Iteration 4604: loss -3.3059308528900146\n",
      "Iteration 4605: loss -3.304755926132202\n",
      "Iteration 4606: loss -3.562991142272949\n",
      "Iteration 4607: loss -3.2489142417907715\n",
      "Iteration 4608: loss -3.4005837440490723\n",
      "Iteration 4609: loss -3.4362845420837402\n",
      "Iteration 4610: loss -3.4168953895568848\n",
      "Iteration 4611: loss -2.9901223182678223\n",
      "Iteration 4612: loss -3.3788135051727295\n",
      "Iteration 4613: loss -3.3935155868530273\n",
      "Iteration 4614: loss -3.2850568294525146\n",
      "Iteration 4615: loss -3.2595977783203125\n",
      "Iteration 4616: loss -3.258225679397583\n",
      "Iteration 4617: loss -3.3234450817108154\n",
      "Iteration 4618: loss -3.387159824371338\n",
      "Iteration 4619: loss -3.503875494003296\n",
      "Iteration 4620: loss -3.327793598175049\n",
      "Iteration 4621: loss -3.376879930496216\n",
      "Iteration 4622: loss -3.3269875049591064\n",
      "Iteration 4623: loss -3.49849009513855\n",
      "Iteration 4624: loss -3.455462694168091\n",
      "Iteration 4625: loss -3.4625580310821533\n",
      "Iteration 4626: loss -3.528663396835327\n",
      "Iteration 4627: loss -3.530339241027832\n",
      "Iteration 4628: loss -3.495292901992798\n",
      "Iteration 4629: loss -3.432060480117798\n",
      "Iteration 4630: loss -3.4141745567321777\n",
      "Iteration 4631: loss -3.4167320728302\n",
      "Iteration 4632: loss -3.318807363510132\n",
      "Iteration 4633: loss -3.273533821105957\n",
      "Iteration 4634: loss -3.4439806938171387\n",
      "Iteration 4635: loss -3.475116491317749\n",
      "Iteration 4636: loss -3.4840247631073\n",
      "Iteration 4637: loss -3.449716567993164\n",
      "Iteration 4638: loss -3.365696430206299\n",
      "Iteration 4639: loss -3.42103910446167\n",
      "Iteration 4640: loss -3.3380801677703857\n",
      "Iteration 4641: loss -3.4060239791870117\n",
      "Iteration 4642: loss -3.5790393352508545\n",
      "Iteration 4643: loss -3.487959861755371\n",
      "Iteration 4644: loss -3.3631393909454346\n",
      "Iteration 4645: loss -3.4201509952545166\n",
      "Iteration 4646: loss -3.5192108154296875\n",
      "Iteration 4647: loss -3.4456217288970947\n",
      "Iteration 4648: loss -3.460082769393921\n",
      "Iteration 4649: loss -3.5079727172851562\n",
      "Iteration 4650: loss -3.4572393894195557\n",
      "Iteration 4651: loss -3.3762378692626953\n",
      "Iteration 4652: loss -3.5486631393432617\n",
      "Iteration 4653: loss -3.497882604598999\n",
      "Iteration 4654: loss -3.380183219909668\n",
      "Iteration 4655: loss -3.3139073848724365\n",
      "Iteration 4656: loss -3.414116144180298\n",
      "Iteration 4657: loss -3.2724852561950684\n",
      "Iteration 4658: loss -3.4346892833709717\n",
      "Iteration 4659: loss -3.4173717498779297\n",
      "Iteration 4660: loss -3.398048162460327\n",
      "Iteration 4661: loss -3.428140163421631\n",
      "Iteration 4662: loss -3.4551732540130615\n",
      "Iteration 4663: loss -3.316819190979004\n",
      "Iteration 4664: loss -3.4659502506256104\n",
      "Iteration 4665: loss -3.4532887935638428\n",
      "Iteration 4666: loss -3.105055332183838\n",
      "Iteration 4667: loss -3.344266891479492\n",
      "Iteration 4668: loss -3.336918592453003\n",
      "Iteration 4669: loss -3.3874120712280273\n",
      "Iteration 4670: loss -3.3091325759887695\n",
      "Iteration 4671: loss -3.3433849811553955\n",
      "Iteration 4672: loss -3.441305160522461\n",
      "Iteration 4673: loss -3.1774792671203613\n",
      "Iteration 4674: loss -3.2191619873046875\n",
      "Iteration 4675: loss -3.47623872756958\n",
      "Iteration 4676: loss -3.356987237930298\n",
      "Iteration 4677: loss -3.299651861190796\n",
      "Iteration 4678: loss -3.336080312728882\n",
      "Iteration 4679: loss -3.387637138366699\n",
      "Iteration 4680: loss -3.4645795822143555\n",
      "Iteration 4681: loss -3.2779593467712402\n",
      "Iteration 4682: loss -3.2886157035827637\n",
      "Iteration 4683: loss -3.5462920665740967\n",
      "Iteration 4684: loss -3.388612985610962\n",
      "Iteration 4685: loss -3.238865852355957\n",
      "Iteration 4686: loss -3.2860360145568848\n",
      "Iteration 4687: loss -3.420217275619507\n",
      "Iteration 4688: loss -3.2881929874420166\n",
      "Iteration 4689: loss -3.388975143432617\n",
      "Iteration 4690: loss -3.30153751373291\n",
      "Iteration 4691: loss -3.4822998046875\n",
      "Iteration 4692: loss -3.2590835094451904\n",
      "Iteration 4693: loss -3.5148046016693115\n",
      "Iteration 4694: loss -3.414219379425049\n",
      "Iteration 4695: loss -3.236248731613159\n",
      "Iteration 4696: loss -3.50266170501709\n",
      "Iteration 4697: loss -3.3497655391693115\n",
      "Iteration 4698: loss -3.30290150642395\n",
      "Iteration 4699: loss -3.377981185913086\n",
      "Iteration 4700: loss -3.4263532161712646\n",
      "Iteration 4701: loss -3.2204675674438477\n",
      "Iteration 4702: loss -3.3264548778533936\n",
      "Iteration 4703: loss -3.4763882160186768\n",
      "Iteration 4704: loss -3.205834150314331\n",
      "Iteration 4705: loss -2.939002275466919\n",
      "Iteration 4706: loss -3.3859827518463135\n",
      "Iteration 4707: loss -3.1258649826049805\n",
      "Iteration 4708: loss -2.958754539489746\n",
      "Iteration 4709: loss -3.3449108600616455\n",
      "Iteration 4710: loss -3.5104141235351562\n",
      "Iteration 4711: loss -3.290252923965454\n",
      "Iteration 4712: loss -3.170133352279663\n",
      "Iteration 4713: loss -3.4743566513061523\n",
      "Iteration 4714: loss -3.1997711658477783\n",
      "Iteration 4715: loss -3.26949405670166\n",
      "Iteration 4716: loss -3.240144968032837\n",
      "Iteration 4717: loss -3.2229344844818115\n",
      "Iteration 4718: loss -3.396852970123291\n",
      "Iteration 4719: loss -3.2690229415893555\n",
      "Iteration 4720: loss -3.208885908126831\n",
      "Iteration 4721: loss -3.4600331783294678\n",
      "Iteration 4722: loss -3.434744358062744\n",
      "Iteration 4723: loss -3.1500298976898193\n",
      "Iteration 4724: loss -3.289682626724243\n",
      "Iteration 4725: loss -3.5137152671813965\n",
      "Iteration 4726: loss -3.384197473526001\n",
      "Iteration 4727: loss -3.353470802307129\n",
      "Iteration 4728: loss -3.3401076793670654\n",
      "Iteration 4729: loss -3.399930477142334\n",
      "Iteration 4730: loss -3.280240058898926\n",
      "Iteration 4731: loss -3.381847620010376\n",
      "Iteration 4732: loss -3.447798252105713\n",
      "Iteration 4733: loss -3.3477871417999268\n",
      "Iteration 4734: loss -3.421459913253784\n",
      "Iteration 4735: loss -3.4276418685913086\n",
      "Iteration 4736: loss -3.25352144241333\n",
      "Iteration 4737: loss -3.467495918273926\n",
      "Iteration 4738: loss -3.504101276397705\n",
      "Iteration 4739: loss -3.4782378673553467\n",
      "Iteration 4740: loss -3.3025686740875244\n",
      "Iteration 4741: loss -3.518610954284668\n",
      "Iteration 4742: loss -3.382880926132202\n",
      "Iteration 4743: loss -3.3925912380218506\n",
      "Iteration 4744: loss -3.432161331176758\n",
      "Iteration 4745: loss -3.3399016857147217\n",
      "Iteration 4746: loss -3.3618314266204834\n",
      "Iteration 4747: loss -3.4543631076812744\n",
      "Iteration 4748: loss -3.229062557220459\n",
      "Iteration 4749: loss -3.320786952972412\n",
      "Iteration 4750: loss -3.3611702919006348\n",
      "Iteration 4751: loss -3.2538061141967773\n",
      "Iteration 4752: loss -3.4571073055267334\n",
      "Iteration 4753: loss -3.5258984565734863\n",
      "Iteration 4754: loss -3.3505687713623047\n",
      "Iteration 4755: loss -3.4673614501953125\n",
      "Iteration 4756: loss -3.4192686080932617\n",
      "Iteration 4757: loss -3.501758337020874\n",
      "Iteration 4758: loss -3.314617872238159\n",
      "Iteration 4759: loss -3.530226469039917\n",
      "Iteration 4760: loss -3.41154408454895\n",
      "Iteration 4761: loss -3.562833309173584\n",
      "Iteration 4762: loss -3.373507022857666\n",
      "Iteration 4763: loss -3.5509254932403564\n",
      "Iteration 4764: loss -3.434614896774292\n",
      "Iteration 4765: loss -3.3581652641296387\n",
      "Iteration 4766: loss -3.463521718978882\n",
      "Iteration 4767: loss -3.4771955013275146\n",
      "Iteration 4768: loss -3.6148183345794678\n",
      "Iteration 4769: loss -3.505106210708618\n",
      "Iteration 4770: loss -3.451657772064209\n",
      "Iteration 4771: loss -3.5049147605895996\n",
      "Iteration 4772: loss -3.322510004043579\n",
      "Iteration 4773: loss -3.395353317260742\n",
      "Iteration 4774: loss -3.4449596405029297\n",
      "Iteration 4775: loss -3.4454686641693115\n",
      "Iteration 4776: loss -3.4850661754608154\n",
      "Iteration 4777: loss -3.3353567123413086\n",
      "Iteration 4778: loss -3.3601064682006836\n",
      "Iteration 4779: loss -3.549696683883667\n",
      "Iteration 4780: loss -3.3677375316619873\n",
      "Iteration 4781: loss -3.4688494205474854\n",
      "Iteration 4782: loss -3.4179527759552\n",
      "Iteration 4783: loss -3.4219720363616943\n",
      "Iteration 4784: loss -3.481205463409424\n",
      "Iteration 4785: loss -3.3978893756866455\n",
      "Iteration 4786: loss -3.4629101753234863\n",
      "Iteration 4787: loss -3.5136590003967285\n",
      "Iteration 4788: loss -3.357710599899292\n",
      "Iteration 4789: loss -3.448753595352173\n",
      "Iteration 4790: loss -3.416321039199829\n",
      "Iteration 4791: loss -3.462392568588257\n",
      "Iteration 4792: loss -3.5084450244903564\n",
      "Iteration 4793: loss -3.4384243488311768\n",
      "Iteration 4794: loss -3.478501319885254\n",
      "Iteration 4795: loss -3.3763363361358643\n",
      "Iteration 4796: loss -3.3484983444213867\n",
      "Iteration 4797: loss -3.533426523208618\n",
      "Iteration 4798: loss -3.5720407962799072\n",
      "Iteration 4799: loss -3.235377550125122\n",
      "Iteration 4800: loss -3.4296257495880127\n",
      "Iteration 4801: loss -3.55552339553833\n",
      "Iteration 4802: loss -3.46163272857666\n",
      "Iteration 4803: loss -3.198211669921875\n",
      "Iteration 4804: loss -3.444692611694336\n",
      "Iteration 4805: loss -3.5316872596740723\n",
      "Iteration 4806: loss -3.4152984619140625\n",
      "Iteration 4807: loss -3.184157371520996\n",
      "Iteration 4808: loss -3.5117642879486084\n",
      "Iteration 4809: loss -3.362635612487793\n",
      "Iteration 4810: loss -3.4782540798187256\n",
      "Iteration 4811: loss -3.421217679977417\n",
      "Iteration 4812: loss -3.4074594974517822\n",
      "Iteration 4813: loss -3.5397720336914062\n",
      "Iteration 4814: loss -3.424428939819336\n",
      "Iteration 4815: loss -3.3510303497314453\n",
      "Iteration 4816: loss -3.4385814666748047\n",
      "Iteration 4817: loss -3.5954205989837646\n",
      "Iteration 4818: loss -3.5697097778320312\n",
      "Iteration 4819: loss -3.326183557510376\n",
      "Iteration 4820: loss -3.464401960372925\n",
      "Iteration 4821: loss -3.4642577171325684\n",
      "Iteration 4822: loss -3.4783315658569336\n",
      "Iteration 4823: loss -3.523043155670166\n",
      "Iteration 4824: loss -3.4076006412506104\n",
      "Iteration 4825: loss -3.439570665359497\n",
      "Iteration 4826: loss -3.504164934158325\n",
      "Iteration 4827: loss -3.2698209285736084\n",
      "Iteration 4828: loss -3.2261803150177\n",
      "Iteration 4829: loss -3.364572525024414\n",
      "Iteration 4830: loss -3.2424261569976807\n",
      "Iteration 4831: loss -3.535146474838257\n",
      "Iteration 4832: loss -3.4746837615966797\n",
      "Iteration 4833: loss -3.14799165725708\n",
      "Iteration 4834: loss -3.4185051918029785\n",
      "Iteration 4835: loss -3.2733635902404785\n",
      "Iteration 4836: loss -3.4061572551727295\n",
      "Iteration 4837: loss -3.4624528884887695\n",
      "Iteration 4838: loss -3.406867742538452\n",
      "Iteration 4839: loss -3.4828178882598877\n",
      "Iteration 4840: loss -3.248279094696045\n",
      "Iteration 4841: loss -3.2539196014404297\n",
      "Iteration 4842: loss -3.5043816566467285\n",
      "Iteration 4843: loss -3.5344772338867188\n",
      "Iteration 4844: loss -3.4942853450775146\n",
      "Iteration 4845: loss -3.4162235260009766\n",
      "Iteration 4846: loss -3.5929579734802246\n",
      "Iteration 4847: loss -3.411769390106201\n",
      "Iteration 4848: loss -3.446295976638794\n",
      "Iteration 4849: loss -3.492337942123413\n",
      "Iteration 4850: loss -3.436474323272705\n",
      "Iteration 4851: loss -3.3145318031311035\n",
      "Iteration 4852: loss -3.603330135345459\n",
      "Iteration 4853: loss -3.546335220336914\n",
      "Iteration 4854: loss -3.4072906970977783\n",
      "Iteration 4855: loss -3.5528934001922607\n",
      "Iteration 4856: loss -3.481865882873535\n",
      "Iteration 4857: loss -3.558842182159424\n",
      "Iteration 4858: loss -3.3572194576263428\n",
      "Iteration 4859: loss -3.4281771183013916\n",
      "Iteration 4860: loss -3.5763251781463623\n",
      "Iteration 4861: loss -3.407616376876831\n",
      "Iteration 4862: loss -3.317737340927124\n",
      "Iteration 4863: loss -3.5828630924224854\n",
      "Iteration 4864: loss -3.2324576377868652\n",
      "Iteration 4865: loss -3.4078381061553955\n",
      "Iteration 4866: loss -3.4664576053619385\n",
      "Iteration 4867: loss -3.310807228088379\n",
      "Iteration 4868: loss -3.440109968185425\n",
      "Iteration 4869: loss -3.464550256729126\n",
      "Iteration 4870: loss -3.2201154232025146\n",
      "Iteration 4871: loss -3.4613404273986816\n",
      "Iteration 4872: loss -3.3726422786712646\n",
      "Iteration 4873: loss -3.369694471359253\n",
      "Iteration 4874: loss -3.3938610553741455\n",
      "Iteration 4875: loss -3.3838248252868652\n",
      "Iteration 4876: loss -3.148183584213257\n",
      "Iteration 4877: loss -3.5614821910858154\n",
      "Iteration 4878: loss -3.264662504196167\n",
      "Iteration 4879: loss -3.4235939979553223\n",
      "Iteration 4880: loss -3.468636989593506\n",
      "Iteration 4881: loss -3.301523447036743\n",
      "Iteration 4882: loss -3.3348119258880615\n",
      "Iteration 4883: loss -3.5746335983276367\n",
      "Iteration 4884: loss -3.3458690643310547\n",
      "Iteration 4885: loss -3.227149248123169\n",
      "Iteration 4886: loss -3.4093515872955322\n",
      "Iteration 4887: loss -3.3318004608154297\n",
      "Iteration 4888: loss -3.4064502716064453\n",
      "Iteration 4889: loss -3.386593818664551\n",
      "Iteration 4890: loss -3.3112916946411133\n",
      "Iteration 4891: loss -3.2980828285217285\n",
      "Iteration 4892: loss -3.4089910984039307\n",
      "Iteration 4893: loss -3.5348901748657227\n",
      "Iteration 4894: loss -3.3604254722595215\n",
      "Iteration 4895: loss -3.4704184532165527\n",
      "Iteration 4896: loss -3.5646467208862305\n",
      "Iteration 4897: loss -3.453824996948242\n",
      "Iteration 4898: loss -3.42214298248291\n",
      "Iteration 4899: loss -3.521347761154175\n",
      "Iteration 4900: loss -3.399078607559204\n",
      "Iteration 4901: loss -3.3837289810180664\n",
      "Iteration 4902: loss -3.435171604156494\n",
      "Iteration 4903: loss -3.3740503787994385\n",
      "Iteration 4904: loss -3.5573313236236572\n",
      "Iteration 4905: loss -3.554962158203125\n",
      "Iteration 4906: loss -3.388578176498413\n",
      "Iteration 4907: loss -3.4921770095825195\n",
      "Iteration 4908: loss -3.527073860168457\n",
      "Iteration 4909: loss -3.524587631225586\n",
      "Iteration 4910: loss -3.5647788047790527\n",
      "Iteration 4911: loss -3.5367488861083984\n",
      "Iteration 4912: loss -3.524413824081421\n",
      "Iteration 4913: loss -3.587883234024048\n",
      "Iteration 4914: loss -3.4345924854278564\n",
      "Iteration 4915: loss -3.3820672035217285\n",
      "Iteration 4916: loss -3.4898123741149902\n",
      "Iteration 4917: loss -3.541072368621826\n",
      "Iteration 4918: loss -3.2621452808380127\n",
      "Iteration 4919: loss -3.4774837493896484\n",
      "Iteration 4920: loss -3.5006070137023926\n",
      "Iteration 4921: loss -3.43058705329895\n",
      "Iteration 4922: loss -3.491783857345581\n",
      "Iteration 4923: loss -3.3633391857147217\n",
      "Iteration 4924: loss -3.1073555946350098\n",
      "Iteration 4925: loss -3.391721248626709\n",
      "Iteration 4926: loss -3.114753007888794\n",
      "Iteration 4927: loss -3.208955764770508\n",
      "Iteration 4928: loss -3.344419240951538\n",
      "Iteration 4929: loss -3.2890729904174805\n",
      "Iteration 4930: loss -3.199047565460205\n",
      "Iteration 4931: loss -3.342090129852295\n",
      "Iteration 4932: loss -3.3046951293945312\n",
      "Iteration 4933: loss -3.3057219982147217\n",
      "Iteration 4934: loss -3.4396636486053467\n",
      "Iteration 4935: loss -3.507290840148926\n",
      "Iteration 4936: loss -3.0916826725006104\n",
      "Iteration 4937: loss -3.3790335655212402\n",
      "Iteration 4938: loss -3.451853036880493\n",
      "Iteration 4939: loss -3.5450446605682373\n",
      "Iteration 4940: loss -3.420138359069824\n",
      "Iteration 4941: loss -3.4360320568084717\n",
      "Iteration 4942: loss -3.330045461654663\n",
      "Iteration 4943: loss -3.3575451374053955\n",
      "Iteration 4944: loss -3.5458242893218994\n",
      "Iteration 4945: loss -3.4372637271881104\n",
      "Iteration 4946: loss -3.3825552463531494\n",
      "Iteration 4947: loss -3.32315731048584\n",
      "Iteration 4948: loss -3.435490608215332\n",
      "Iteration 4949: loss -3.2696661949157715\n",
      "Iteration 4950: loss -3.4520316123962402\n",
      "Iteration 4951: loss -3.5247604846954346\n",
      "Iteration 4952: loss -3.2917816638946533\n",
      "Iteration 4953: loss -3.5703887939453125\n",
      "Iteration 4954: loss -3.539135456085205\n",
      "Iteration 4955: loss -3.357632637023926\n",
      "Iteration 4956: loss -3.4635848999023438\n",
      "Iteration 4957: loss -3.4847829341888428\n",
      "Iteration 4958: loss -3.5236403942108154\n",
      "Iteration 4959: loss -3.469416618347168\n",
      "Iteration 4960: loss -3.3881540298461914\n",
      "Iteration 4961: loss -3.4701101779937744\n",
      "Iteration 4962: loss -3.2939891815185547\n",
      "Iteration 4963: loss -2.813096523284912\n",
      "Iteration 4964: loss -2.8378021717071533\n",
      "Iteration 4965: loss -2.907729148864746\n",
      "Iteration 4966: loss -3.222094774246216\n",
      "Iteration 4967: loss -3.129992723464966\n",
      "Iteration 4968: loss -3.210864305496216\n",
      "Iteration 4969: loss -3.3764357566833496\n",
      "Iteration 4970: loss -3.2486660480499268\n",
      "Iteration 4971: loss -3.2657833099365234\n",
      "Iteration 4972: loss -3.2856245040893555\n",
      "Iteration 4973: loss -3.152089834213257\n",
      "Iteration 4974: loss -3.304990291595459\n",
      "Iteration 4975: loss -3.300624132156372\n",
      "Iteration 4976: loss -3.2741358280181885\n",
      "Iteration 4977: loss -3.26838755607605\n",
      "Iteration 4978: loss -3.320683240890503\n",
      "Iteration 4979: loss -3.2986748218536377\n",
      "Iteration 4980: loss -3.4702584743499756\n",
      "Iteration 4981: loss -3.256810188293457\n",
      "Iteration 4982: loss -3.270190954208374\n",
      "Iteration 4983: loss -3.307983160018921\n",
      "Iteration 4984: loss -3.4664382934570312\n",
      "Iteration 4985: loss -3.449198007583618\n",
      "Iteration 4986: loss -3.48979115486145\n",
      "Iteration 4987: loss -3.387202739715576\n",
      "Iteration 4988: loss -3.3343915939331055\n",
      "Iteration 4989: loss -3.3591883182525635\n",
      "Iteration 4990: loss -3.408747911453247\n",
      "Iteration 4991: loss -3.410031318664551\n",
      "Iteration 4992: loss -3.341963529586792\n",
      "Iteration 4993: loss -3.3813741207122803\n",
      "Iteration 4994: loss -3.2713310718536377\n",
      "Iteration 4995: loss -3.4968068599700928\n",
      "Iteration 4996: loss -3.4157874584198\n",
      "Iteration 4997: loss -3.436232805252075\n",
      "Iteration 4998: loss -3.491786003112793\n",
      "Iteration 4999: loss -3.3047354221343994\n"
     ]
    }
   ],
   "source": [
    "for j in range(5000):\n",
    "    theta, x = generate_data(mb_size, return_theta=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -1*encoder.log_prob(theta, x).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Iteration {}: loss {}'.format(j, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e767721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post(x, theta, **kwargs):\n",
    "    '''vectorized version of the above in theta for fixed x'''\n",
    "    assert theta.shape[1] == 2, \"not yet implemented for evaluation on multiple points at once\"\n",
    "    r_dist = kwargs['r_dist']\n",
    "    new1 = -1*torch.sum(theta, 1).abs()/math.sqrt(2)\n",
    "    new2 = (-1*theta[:,0]+theta[:,1])/math.sqrt(2)\n",
    "    new = torch.stack([new1, new2]).T\n",
    "    p = x-new\n",
    "    u = p[:,0]-.25\n",
    "    v = p[:,1]\n",
    "    r = torch.sqrt(u ** 2 + v ** 2)  # note the angle distribution is uniform\n",
    "    to_adjust = r_dist.log_prob(r)\n",
    "    adjusted = torch.where(u < 0.0, -torch.inf, to_adjust.double())\n",
    "    return adjusted\n",
    "\n",
    "# Code to plot the true posterior density\n",
    "def plot(j, x, encoder, **kwargs):\n",
    "    device = 'cpu'\n",
    "\n",
    "    # Plot exact density\n",
    "    vals = torch.arange(-1., 1., .01)\n",
    "    eval_pts = torch.cartesian_prod(vals, vals)\n",
    "    lps = log_post(x[j], eval_pts, **kwargs)\n",
    "    X, Y = torch.meshgrid(vals, vals)\n",
    "    Z = lps.view(X.shape)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(24,8))\n",
    "    ax[0].pcolormesh(X.numpy(), Y.numpy(), Z.exp().numpy())\n",
    "    ax[0].set_title('Exact')\n",
    "    \n",
    "\n",
    "    vals = torch.arange(-1., 1., .01)\n",
    "    eval_pts = torch.cartesian_prod(vals, vals)\n",
    "    lps = encoder.log_prob(eval_pts.to(device), x[j].view(1,-1).repeat(eval_pts.shape[0],1).to(device)).detach()\n",
    "    X, Y = torch.meshgrid(vals, vals)\n",
    "    Z = lps.view(X.shape)\n",
    "    ax[1].pcolormesh(X.cpu().numpy(), Y.cpu().numpy(), Z.cpu().exp().numpy())\n",
    "    ax[1].set_title('Approximate Posterior Flow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "955455ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/declan/.local/lib/python3.10/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(j=5, x=x, encoder=encoder, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283a06b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(j=3, x=x, encoder=encoder, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bd619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
