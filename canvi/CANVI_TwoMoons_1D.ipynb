{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2712436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_p(r, a):\n",
    "    one = r*torch.cos(a)+.25\n",
    "    two = r*torch.sin(a)\n",
    "    return torch.stack([one, two])\n",
    "\n",
    "def get_x(r, a, theta):\n",
    "    p_val = get_p(r, a)\n",
    "    new1 = -1*torch.sum(theta, -1).abs()/math.sqrt(2)\n",
    "    new2 = (-1*theta[..., 0])/math.sqrt(2)\n",
    "    new = torch.stack([new1, new2])\n",
    "    return (p_val+new).T\n",
    "\n",
    "def generate_data(n_pts, return_theta=False):\n",
    "    prior = D.Uniform(torch.tensor([-1.]), torch.tensor([1.]))\n",
    "    a_dist = D.Uniform(-math.pi/2, math.pi/2)\n",
    "    r_dist = D.Normal(0.1, .01)\n",
    "\n",
    "    theta, a, r = prior.sample((n_pts,)), a_dist.sample((n_pts,)), r_dist.sample((n_pts,))\n",
    "    x = get_x(r, a, theta)\n",
    "    if return_theta:\n",
    "        return theta, x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca0e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknos.nflows import flows, transforms\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "from warnings import warn\n",
    "\n",
    "from pyknos.nflows import distributions as distributions_\n",
    "from pyknos.nflows import flows, transforms\n",
    "from pyknos.nflows.nn import nets\n",
    "from pyknos.nflows.transforms.splines import rational_quadratic\n",
    "from torch import Tensor, nn, relu, tanh, tensor, uint8\n",
    "\n",
    "from sbi.utils.sbiutils import (\n",
    "    standardizing_net,\n",
    "    standardizing_transform,\n",
    "    z_score_parser,\n",
    ")\n",
    "from sbi.utils.torchutils import create_alternating_binary_mask\n",
    "from sbi.utils.user_input_checks import check_data_device, check_embedding_net_device\n",
    "\n",
    "class ContextSplineMap(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network from `context` to the spline parameters.\n",
    "    We cannot use the resnet as conditioner to learn each dimension conditioned\n",
    "    on the other dimensions (because there is only one). Instead, we learn the\n",
    "    spline parameters directly. In the case of conditinal density estimation,\n",
    "    we make the spline parameters conditional on the context. This is\n",
    "    implemented in this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_features: int,\n",
    "        context_features: int,\n",
    "        hidden_layers: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize neural network that learns to predict spline parameters.\n",
    "        Args:\n",
    "            in_features: Unused since there is no `conditioner` in 1D.\n",
    "            out_features: Number of spline parameters.\n",
    "            hidden_features: Number of hidden units.\n",
    "            context_features: Number of context features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # `self.hidden_features` is only defined such that nflows can infer\n",
    "        # a scaling factor for initializations.\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        # Use a non-linearity because otherwise, there will be a linear\n",
    "        # mapping from context features onto distribution parameters.\n",
    "\n",
    "        # Initialize with input layer.\n",
    "        layer_list = [nn.Linear(context_features, hidden_features), nn.ReLU()]\n",
    "        # Add hidden layers.\n",
    "        layer_list += [\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "        ] * hidden_layers\n",
    "        # Add output layer.\n",
    "        layer_list += [nn.Linear(hidden_features, out_features)]\n",
    "        self.spline_predictor = nn.Sequential(*layer_list)\n",
    "\n",
    "    def __call__(self, inputs: Tensor, context: Tensor, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Return parameters of the spline given the context.\n",
    "        Args:\n",
    "            inputs: Unused. It would usually be the other dimensions, but in\n",
    "                1D, there are no other dimensions.\n",
    "            context: Context features.\n",
    "        Returns:\n",
    "            Spline parameters.\n",
    "        \"\"\"\n",
    "        return self.spline_predictor(context)\n",
    "\n",
    "# Declan: this code from SBI library\n",
    "def build_nsf(\n",
    "    batch_x: Tensor,\n",
    "    batch_y: Tensor,\n",
    "    z_score_x: Optional[str] = \"independent\",\n",
    "    z_score_y: Optional[str] = \"independent\",\n",
    "    hidden_features: int = 50,\n",
    "    num_transforms: int = 5,\n",
    "    num_bins: int = 10,\n",
    "    embedding_net: nn.Module = nn.Identity(),\n",
    "    tail_bound: float = 3.0,\n",
    "    hidden_layers_spline_context: int = 1,\n",
    "    num_blocks: int = 2,\n",
    "    dropout_probability: float = 0.0,\n",
    "    use_batch_norm: bool = False,\n",
    "    **kwargs,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Builds NSF p(x|y).\n",
    "    Args:\n",
    "        batch_x: Batch of xs, used to infer dimensionality and (optional) z-scoring.\n",
    "        batch_y: Batch of ys, used to infer dimensionality and (optional) z-scoring.\n",
    "        z_score_x: Whether to z-score xs passing into the network, can be one of:\n",
    "            - `none`, or None: do not z-score.\n",
    "            - `independent`: z-score each dimension independently.\n",
    "            - `structured`: treat dimensions as related, therefore compute mean and std\n",
    "            over the entire batch, instead of per-dimension. Should be used when each\n",
    "            sample is, for example, a time series or an image.\n",
    "        z_score_y: Whether to z-score ys passing into the network, same options as\n",
    "            z_score_x.\n",
    "        hidden_features: Number of hidden features.\n",
    "        num_transforms: Number of transforms.\n",
    "        num_bins: Number of bins used for the splines.\n",
    "        embedding_net: Optional embedding network for y.\n",
    "        tail_bound: tail bound for each spline.\n",
    "        hidden_layers_spline_context: number of hidden layers of the spline context net\n",
    "            for one-dimensional x.\n",
    "        num_blocks: number of blocks used for residual net for context embedding.\n",
    "        dropout_probability: dropout probability for regularization in residual net.\n",
    "        use_batch_norm: whether to use batch norm in residual net.\n",
    "        kwargs: Additional arguments that are passed by the build function but are not\n",
    "            relevant for maf and are therefore ignored.\n",
    "    Returns:\n",
    "        Neural network.\n",
    "    \"\"\"\n",
    "    x_numel = batch_x[0].numel()\n",
    "    # Infer the output dimensionality of the embedding_net by making a forward pass.\n",
    "    check_data_device(batch_x, batch_y)\n",
    "    check_embedding_net_device(embedding_net=embedding_net, datum=batch_y)\n",
    "    y_numel = embedding_net(batch_y[:1]).numel()\n",
    "\n",
    "    # Define mask function to alternate between predicted x-dimensions.\n",
    "    def mask_in_layer(i):\n",
    "        return create_alternating_binary_mask(features=x_numel, even=(i % 2 == 0))\n",
    "\n",
    "    # If x is just a scalar then use a dummy mask and learn spline parameters using the\n",
    "    # conditioning variables only.\n",
    "    if x_numel == 1:\n",
    "        # Conditioner ignores the data and uses the conditioning variables only.\n",
    "        conditioner = partial(\n",
    "            ContextSplineMap,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=y_numel,\n",
    "            hidden_layers=hidden_layers_spline_context,\n",
    "        )\n",
    "    else:\n",
    "        # Use conditional resnet as spline conditioner.\n",
    "        conditioner = partial(\n",
    "            nets.ResidualNet,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=y_numel,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "        )\n",
    "\n",
    "    # Stack spline transforms.\n",
    "    transform_list = []\n",
    "    for i in range(num_transforms):\n",
    "        block = [\n",
    "            transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "                mask=mask_in_layer(i) if x_numel > 1 else tensor([1], dtype=uint8),\n",
    "                transform_net_create_fn=conditioner,\n",
    "                num_bins=num_bins,\n",
    "                tails=\"linear\",\n",
    "                tail_bound=tail_bound,\n",
    "                apply_unconditional_transform=False,\n",
    "            )\n",
    "        ]\n",
    "        # Add LU transform only for high D x. Permutation makes sense only for more than\n",
    "        # one feature.\n",
    "        if x_numel > 1:\n",
    "            block.append(\n",
    "                transforms.LULinear(x_numel, identity_init=True),\n",
    "            )\n",
    "        transform_list += block\n",
    "\n",
    "    z_score_x_bool, structured_x = z_score_parser(z_score_x)\n",
    "    if z_score_x_bool:\n",
    "        # Prepend standardizing transform to nsf transforms.\n",
    "        transform_list = [\n",
    "            standardizing_transform(batch_x, structured_x)\n",
    "        ] + transform_list\n",
    "\n",
    "    z_score_y_bool, structured_y = z_score_parser(z_score_y)\n",
    "    if z_score_y_bool:\n",
    "        # Prepend standardizing transform to y-embedding.\n",
    "        embedding_net = nn.Sequential(\n",
    "            standardizing_net(batch_y, structured_y), embedding_net\n",
    "        )\n",
    "\n",
    "    distribution = distributions_.StandardNormal((x_numel,))\n",
    "\n",
    "    # Combine transforms.\n",
    "    transform = transforms.CompositeTransform(transform_list)\n",
    "    neural_net = flows.Flow(transform, distribution, embedding_net)\n",
    "\n",
    "    return neural_net\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.context_dim = dim\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Assumes context x is of shape (batch_size, self.context_dim)\n",
    "        '''\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a406aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, x = generate_data(100, return_theta=True) \n",
    "\n",
    "prior = D.Uniform(torch.tensor([-1.]), torch.tensor([1.]))\n",
    "a_dist = D.Uniform(-math.pi/2, math.pi/2)\n",
    "r_dist = D.Normal(0.1, .01)\n",
    "mb_size=100\n",
    "device='cuda:0'\n",
    "kwargs = {'prior': prior,\n",
    "        'a_dist': a_dist,\n",
    "        'r_dist': r_dist}\n",
    "\n",
    "# EXAMPLE BATCH FOR SHAPES\n",
    "z_dim = prior.sample().shape[-1]\n",
    "x_dim = x.shape[-1]\n",
    "num_obs_flow = mb_size\n",
    "fake_zs = torch.randn((mb_size, z_dim))\n",
    "fake_xs = torch.randn((mb_size, x_dim))\n",
    "encoder = build_nsf(fake_zs, fake_xs, z_score_x='none', z_score_y='none')\n",
    "\n",
    "encoder.to(device)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87b099ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 1.376520037651062\n",
      "Iteration 1: loss 1.2266581058502197\n",
      "Iteration 2: loss 1.1976221799850464\n",
      "Iteration 3: loss 1.230082392692566\n",
      "Iteration 4: loss 1.166643500328064\n",
      "Iteration 5: loss 1.0738427639007568\n",
      "Iteration 6: loss 1.2030832767486572\n",
      "Iteration 7: loss 1.0469608306884766\n",
      "Iteration 8: loss 1.0662074089050293\n",
      "Iteration 9: loss 1.1161472797393799\n",
      "Iteration 10: loss 1.0854778289794922\n",
      "Iteration 11: loss 1.126595377922058\n",
      "Iteration 12: loss 1.1174176931381226\n",
      "Iteration 13: loss 1.0462520122528076\n",
      "Iteration 14: loss 0.9655378460884094\n",
      "Iteration 15: loss 1.050117015838623\n",
      "Iteration 16: loss 0.9352047443389893\n",
      "Iteration 17: loss 0.9352251887321472\n",
      "Iteration 18: loss 0.9603620767593384\n",
      "Iteration 19: loss 0.9666975140571594\n",
      "Iteration 20: loss 0.927326500415802\n",
      "Iteration 21: loss 0.9115054607391357\n",
      "Iteration 22: loss 0.968351423740387\n",
      "Iteration 23: loss 0.8201296329498291\n",
      "Iteration 24: loss 0.8687020540237427\n",
      "Iteration 25: loss 0.837331235408783\n",
      "Iteration 26: loss 0.7632165551185608\n",
      "Iteration 27: loss 0.8367632627487183\n",
      "Iteration 28: loss 0.787350594997406\n",
      "Iteration 29: loss 0.788874626159668\n",
      "Iteration 30: loss 0.7640880942344666\n",
      "Iteration 31: loss 0.7132380604743958\n",
      "Iteration 32: loss 0.6842644214630127\n",
      "Iteration 33: loss 0.7219951748847961\n",
      "Iteration 34: loss 0.666839063167572\n",
      "Iteration 35: loss 0.6346465945243835\n",
      "Iteration 36: loss 0.6493273973464966\n",
      "Iteration 37: loss 0.585086464881897\n",
      "Iteration 38: loss 0.5421366095542908\n",
      "Iteration 39: loss 0.5466567873954773\n",
      "Iteration 40: loss 0.5121154189109802\n",
      "Iteration 41: loss 0.4819778800010681\n",
      "Iteration 42: loss 0.4589576721191406\n",
      "Iteration 43: loss 0.41918569803237915\n",
      "Iteration 44: loss 0.4585491120815277\n",
      "Iteration 45: loss 0.38201257586479187\n",
      "Iteration 46: loss 0.33111312985420227\n",
      "Iteration 47: loss 0.3429020941257477\n",
      "Iteration 48: loss 0.29067447781562805\n",
      "Iteration 49: loss 0.245243102312088\n",
      "Iteration 50: loss 0.2837420105934143\n",
      "Iteration 51: loss 0.2257038801908493\n",
      "Iteration 52: loss 0.16010120511054993\n",
      "Iteration 53: loss 0.11421163380146027\n",
      "Iteration 54: loss 0.07135933637619019\n",
      "Iteration 55: loss 0.07565094530582428\n",
      "Iteration 56: loss -0.010844884440302849\n",
      "Iteration 57: loss 0.01638305000960827\n",
      "Iteration 58: loss -0.051646050065755844\n",
      "Iteration 59: loss -0.0923379436135292\n",
      "Iteration 60: loss -0.09098776429891586\n",
      "Iteration 61: loss -0.13515138626098633\n",
      "Iteration 62: loss -0.16562901437282562\n",
      "Iteration 63: loss -0.18176032602787018\n",
      "Iteration 64: loss -0.27598387002944946\n",
      "Iteration 65: loss -0.26305893063545227\n",
      "Iteration 66: loss -0.2923518419265747\n",
      "Iteration 67: loss -0.3399507403373718\n",
      "Iteration 68: loss -0.40119901299476624\n",
      "Iteration 69: loss -0.4383476972579956\n",
      "Iteration 70: loss -0.47175848484039307\n",
      "Iteration 71: loss -0.5095300674438477\n",
      "Iteration 72: loss -0.575895369052887\n",
      "Iteration 73: loss -0.5218097567558289\n",
      "Iteration 74: loss -0.5669279098510742\n",
      "Iteration 75: loss -0.5702736377716064\n",
      "Iteration 76: loss -0.614562451839447\n",
      "Iteration 77: loss -0.6657467484474182\n",
      "Iteration 78: loss -0.6952937245368958\n",
      "Iteration 79: loss -0.6075803637504578\n",
      "Iteration 80: loss -0.6447059512138367\n",
      "Iteration 81: loss -0.7140076756477356\n",
      "Iteration 82: loss -0.6754913330078125\n",
      "Iteration 83: loss -0.7743305563926697\n",
      "Iteration 84: loss -0.7967380285263062\n",
      "Iteration 85: loss -0.7123408317565918\n",
      "Iteration 86: loss -0.7123902440071106\n",
      "Iteration 87: loss -0.7081611752510071\n",
      "Iteration 88: loss -0.7785375714302063\n",
      "Iteration 89: loss -0.7753753662109375\n",
      "Iteration 90: loss -0.70499587059021\n",
      "Iteration 91: loss -0.8718125224113464\n",
      "Iteration 92: loss -0.7757315635681152\n",
      "Iteration 93: loss -0.8552908301353455\n",
      "Iteration 94: loss -0.8931379318237305\n",
      "Iteration 95: loss -0.9391727447509766\n",
      "Iteration 96: loss -0.9601234197616577\n",
      "Iteration 97: loss -0.9156821370124817\n",
      "Iteration 98: loss -0.7937841415405273\n",
      "Iteration 99: loss -0.8689330816268921\n",
      "Iteration 100: loss -1.0105717182159424\n",
      "Iteration 101: loss -0.9226737022399902\n",
      "Iteration 102: loss -0.8536982536315918\n",
      "Iteration 103: loss -0.9794235229492188\n",
      "Iteration 104: loss -0.9253072738647461\n",
      "Iteration 105: loss -0.8770423531532288\n",
      "Iteration 106: loss -1.0187492370605469\n",
      "Iteration 107: loss -1.009157419204712\n",
      "Iteration 108: loss -1.0842170715332031\n",
      "Iteration 109: loss -0.9164672493934631\n",
      "Iteration 110: loss -1.091655969619751\n",
      "Iteration 111: loss -1.0777859687805176\n",
      "Iteration 112: loss -1.0914760828018188\n",
      "Iteration 113: loss -1.1830079555511475\n",
      "Iteration 114: loss -1.182745337486267\n",
      "Iteration 115: loss -1.0140427350997925\n",
      "Iteration 116: loss -1.0634113550186157\n",
      "Iteration 117: loss -1.0548583269119263\n",
      "Iteration 118: loss -1.078223466873169\n",
      "Iteration 119: loss -0.9971421360969543\n",
      "Iteration 120: loss -1.205709457397461\n",
      "Iteration 121: loss -1.2624077796936035\n",
      "Iteration 122: loss -1.013726830482483\n",
      "Iteration 123: loss -1.189611792564392\n",
      "Iteration 124: loss -1.2528616189956665\n",
      "Iteration 125: loss -1.0183029174804688\n",
      "Iteration 126: loss -1.0594964027404785\n",
      "Iteration 127: loss -1.2664873600006104\n",
      "Iteration 128: loss -1.2204058170318604\n",
      "Iteration 129: loss -1.1945387125015259\n",
      "Iteration 130: loss -1.2286112308502197\n",
      "Iteration 131: loss -1.1506526470184326\n",
      "Iteration 132: loss -1.265401005744934\n",
      "Iteration 133: loss -1.1886484622955322\n",
      "Iteration 134: loss -1.2531070709228516\n",
      "Iteration 135: loss -1.3887994289398193\n",
      "Iteration 136: loss -1.2757564783096313\n",
      "Iteration 137: loss -1.3088247776031494\n",
      "Iteration 138: loss -1.2952618598937988\n",
      "Iteration 139: loss -1.3260287046432495\n",
      "Iteration 140: loss -1.253438949584961\n",
      "Iteration 141: loss -1.2521182298660278\n",
      "Iteration 142: loss -1.3220691680908203\n",
      "Iteration 143: loss -1.399434208869934\n",
      "Iteration 144: loss -1.2392247915267944\n",
      "Iteration 145: loss -1.2824310064315796\n",
      "Iteration 146: loss -1.3519515991210938\n",
      "Iteration 147: loss -1.3602463006973267\n",
      "Iteration 148: loss -1.3074193000793457\n",
      "Iteration 149: loss -1.2621859312057495\n",
      "Iteration 150: loss -1.4690759181976318\n",
      "Iteration 151: loss -1.379525065422058\n",
      "Iteration 152: loss -1.3435654640197754\n",
      "Iteration 153: loss -1.4148125648498535\n",
      "Iteration 154: loss -1.3889776468276978\n",
      "Iteration 155: loss -1.3916889429092407\n",
      "Iteration 156: loss -1.4247019290924072\n",
      "Iteration 157: loss -1.546600103378296\n",
      "Iteration 158: loss -1.4439382553100586\n",
      "Iteration 159: loss -1.4945625066757202\n",
      "Iteration 160: loss -1.4229241609573364\n",
      "Iteration 161: loss -1.3482519388198853\n",
      "Iteration 162: loss -1.430124282836914\n",
      "Iteration 163: loss -1.4466265439987183\n",
      "Iteration 164: loss -1.51856529712677\n",
      "Iteration 165: loss -1.4304417371749878\n",
      "Iteration 166: loss -1.4576085805892944\n",
      "Iteration 167: loss -1.422428846359253\n",
      "Iteration 168: loss -1.4509660005569458\n",
      "Iteration 169: loss -1.5127712488174438\n",
      "Iteration 170: loss -1.5288392305374146\n",
      "Iteration 171: loss -1.482604742050171\n",
      "Iteration 172: loss -1.5598784685134888\n",
      "Iteration 173: loss -1.512236475944519\n",
      "Iteration 174: loss -1.5743114948272705\n",
      "Iteration 175: loss -1.4208401441574097\n",
      "Iteration 176: loss -1.5948607921600342\n",
      "Iteration 177: loss -1.3767571449279785\n",
      "Iteration 178: loss -1.6158760786056519\n",
      "Iteration 179: loss -1.5235874652862549\n",
      "Iteration 180: loss -1.5290582180023193\n",
      "Iteration 181: loss -1.5931826829910278\n",
      "Iteration 182: loss -1.557824730873108\n",
      "Iteration 183: loss -1.598350167274475\n",
      "Iteration 184: loss -1.5203170776367188\n",
      "Iteration 185: loss -1.4939460754394531\n",
      "Iteration 186: loss -1.5640841722488403\n",
      "Iteration 187: loss -1.6538472175598145\n",
      "Iteration 188: loss -1.5328271389007568\n",
      "Iteration 189: loss -1.4658459424972534\n",
      "Iteration 190: loss -1.4345537424087524\n",
      "Iteration 191: loss -1.66360604763031\n",
      "Iteration 192: loss -1.4989341497421265\n",
      "Iteration 193: loss -1.5274720191955566\n",
      "Iteration 194: loss -1.5978652238845825\n",
      "Iteration 195: loss -1.3660376071929932\n",
      "Iteration 196: loss -1.159610390663147\n",
      "Iteration 197: loss -1.3204619884490967\n",
      "Iteration 198: loss -1.5015912055969238\n",
      "Iteration 199: loss -1.0851682424545288\n",
      "Iteration 200: loss -1.2738343477249146\n",
      "Iteration 201: loss -1.5329554080963135\n",
      "Iteration 202: loss -1.5457308292388916\n",
      "Iteration 203: loss -1.2158197164535522\n",
      "Iteration 204: loss -1.3134729862213135\n",
      "Iteration 205: loss -1.572448968887329\n",
      "Iteration 206: loss -1.4154963493347168\n",
      "Iteration 207: loss -1.3263484239578247\n",
      "Iteration 208: loss -1.3894699811935425\n",
      "Iteration 209: loss -1.523779273033142\n",
      "Iteration 210: loss -1.500567078590393\n",
      "Iteration 211: loss -1.4264190196990967\n",
      "Iteration 212: loss -1.3150171041488647\n",
      "Iteration 213: loss -1.5879124402999878\n",
      "Iteration 214: loss -1.5990679264068604\n",
      "Iteration 215: loss -1.3747435808181763\n",
      "Iteration 216: loss -1.371231198310852\n",
      "Iteration 217: loss -1.561973214149475\n",
      "Iteration 218: loss -1.5677251815795898\n",
      "Iteration 219: loss -1.3768380880355835\n",
      "Iteration 220: loss -1.412701964378357\n",
      "Iteration 221: loss -1.501574993133545\n",
      "Iteration 222: loss -1.542651891708374\n",
      "Iteration 223: loss -1.5608587265014648\n",
      "Iteration 224: loss -1.4592244625091553\n",
      "Iteration 225: loss -1.5697665214538574\n",
      "Iteration 226: loss -1.5539535284042358\n",
      "Iteration 227: loss -1.6911885738372803\n",
      "Iteration 228: loss -1.5658512115478516\n",
      "Iteration 229: loss -1.5433425903320312\n",
      "Iteration 230: loss -1.6016885042190552\n",
      "Iteration 231: loss -1.5790886878967285\n",
      "Iteration 232: loss -1.5765256881713867\n",
      "Iteration 233: loss -1.5417754650115967\n",
      "Iteration 234: loss -1.5850410461425781\n",
      "Iteration 235: loss -1.5835344791412354\n",
      "Iteration 236: loss -1.6379674673080444\n",
      "Iteration 237: loss -1.6769802570343018\n",
      "Iteration 238: loss -1.613609790802002\n",
      "Iteration 239: loss -1.5968563556671143\n",
      "Iteration 240: loss -1.560616135597229\n",
      "Iteration 241: loss -1.6867228746414185\n",
      "Iteration 242: loss -1.7328747510910034\n",
      "Iteration 243: loss -1.6312164068222046\n",
      "Iteration 244: loss -1.650877833366394\n",
      "Iteration 245: loss -1.6560180187225342\n",
      "Iteration 246: loss -1.6916844844818115\n",
      "Iteration 247: loss -1.6974804401397705\n",
      "Iteration 248: loss -1.5938384532928467\n",
      "Iteration 249: loss -1.7640774250030518\n",
      "Iteration 250: loss -1.5818212032318115\n",
      "Iteration 251: loss -1.6928057670593262\n",
      "Iteration 252: loss -1.6777029037475586\n",
      "Iteration 253: loss -1.7264453172683716\n",
      "Iteration 254: loss -1.626868724822998\n",
      "Iteration 255: loss -1.546492576599121\n",
      "Iteration 256: loss -1.678934931755066\n",
      "Iteration 257: loss -1.6613415479660034\n",
      "Iteration 258: loss -1.6091126203536987\n",
      "Iteration 259: loss -1.61455500125885\n",
      "Iteration 260: loss -1.5973039865493774\n",
      "Iteration 261: loss -1.5249111652374268\n",
      "Iteration 262: loss -1.6747689247131348\n",
      "Iteration 263: loss -1.6990315914154053\n",
      "Iteration 264: loss -1.5828508138656616\n",
      "Iteration 265: loss -1.6482714414596558\n",
      "Iteration 266: loss -1.7543500661849976\n",
      "Iteration 267: loss -1.705843448638916\n",
      "Iteration 268: loss -1.742005705833435\n",
      "Iteration 269: loss -1.6625878810882568\n",
      "Iteration 270: loss -1.5797572135925293\n",
      "Iteration 271: loss -1.6401361227035522\n",
      "Iteration 272: loss -1.624984860420227\n",
      "Iteration 273: loss -1.62764573097229\n",
      "Iteration 274: loss -1.8083590269088745\n",
      "Iteration 275: loss -1.671779751777649\n",
      "Iteration 276: loss -1.6209787130355835\n",
      "Iteration 277: loss -1.6439728736877441\n",
      "Iteration 278: loss -1.5782636404037476\n",
      "Iteration 279: loss -1.703503966331482\n",
      "Iteration 280: loss -1.8043094873428345\n",
      "Iteration 281: loss -1.6279296875\n",
      "Iteration 282: loss -1.5794750452041626\n",
      "Iteration 283: loss -1.857828974723816\n",
      "Iteration 284: loss -1.6620185375213623\n",
      "Iteration 285: loss -1.7195125818252563\n",
      "Iteration 286: loss -1.553792119026184\n",
      "Iteration 287: loss -1.7178983688354492\n",
      "Iteration 288: loss -1.6037423610687256\n",
      "Iteration 289: loss -1.6336945295333862\n",
      "Iteration 290: loss -1.66502046585083\n",
      "Iteration 291: loss -1.641703486442566\n",
      "Iteration 292: loss -1.5997889041900635\n",
      "Iteration 293: loss -1.4654606580734253\n",
      "Iteration 294: loss -1.5933443307876587\n",
      "Iteration 295: loss -1.6057202816009521\n",
      "Iteration 296: loss -1.6570452451705933\n",
      "Iteration 297: loss -1.6968435049057007\n",
      "Iteration 298: loss -1.6930757761001587\n",
      "Iteration 299: loss -1.6923850774765015\n",
      "Iteration 300: loss -1.7557216882705688\n",
      "Iteration 301: loss -1.5407907962799072\n",
      "Iteration 302: loss -1.6953809261322021\n",
      "Iteration 303: loss -1.6258175373077393\n",
      "Iteration 304: loss -1.5716304779052734\n",
      "Iteration 305: loss -1.8087786436080933\n",
      "Iteration 306: loss -1.7683309316635132\n",
      "Iteration 307: loss -1.5922701358795166\n",
      "Iteration 308: loss -1.6448757648468018\n",
      "Iteration 309: loss -1.691672682762146\n",
      "Iteration 310: loss -1.7717469930648804\n",
      "Iteration 311: loss -1.8207308053970337\n",
      "Iteration 312: loss -1.7691906690597534\n",
      "Iteration 313: loss -1.6593843698501587\n",
      "Iteration 314: loss -1.470502257347107\n",
      "Iteration 315: loss -1.695801854133606\n",
      "Iteration 316: loss -1.6513360738754272\n",
      "Iteration 317: loss -1.5934624671936035\n",
      "Iteration 318: loss -1.7541567087173462\n",
      "Iteration 319: loss -1.58735191822052\n",
      "Iteration 320: loss -1.5339263677597046\n",
      "Iteration 321: loss -1.5243901014328003\n",
      "Iteration 322: loss -1.741152286529541\n",
      "Iteration 323: loss -1.6546757221221924\n",
      "Iteration 324: loss -1.5813249349594116\n",
      "Iteration 325: loss -1.5887184143066406\n",
      "Iteration 326: loss -1.7491594552993774\n",
      "Iteration 327: loss -1.5793414115905762\n",
      "Iteration 328: loss -1.5276532173156738\n",
      "Iteration 329: loss -1.7369384765625\n",
      "Iteration 330: loss -1.7866283655166626\n",
      "Iteration 331: loss -1.5630340576171875\n",
      "Iteration 332: loss -1.7818113565444946\n",
      "Iteration 333: loss -1.6436196565628052\n",
      "Iteration 334: loss -1.7542939186096191\n",
      "Iteration 335: loss -1.7086046934127808\n",
      "Iteration 336: loss -1.670668601989746\n",
      "Iteration 337: loss -1.7601362466812134\n",
      "Iteration 338: loss -1.7469736337661743\n",
      "Iteration 339: loss -1.7841111421585083\n",
      "Iteration 340: loss -1.7519747018814087\n",
      "Iteration 341: loss -1.6666386127471924\n",
      "Iteration 342: loss -1.683667540550232\n",
      "Iteration 343: loss -1.6907018423080444\n",
      "Iteration 344: loss -1.6842855215072632\n",
      "Iteration 345: loss -1.7607227563858032\n",
      "Iteration 346: loss -1.712066411972046\n",
      "Iteration 347: loss -1.811200499534607\n",
      "Iteration 348: loss -1.6689298152923584\n",
      "Iteration 349: loss -1.7438362836837769\n",
      "Iteration 350: loss -1.7175288200378418\n",
      "Iteration 351: loss -1.723334789276123\n",
      "Iteration 352: loss -1.760147213935852\n",
      "Iteration 353: loss -1.8452047109603882\n",
      "Iteration 354: loss -1.7388619184494019\n",
      "Iteration 355: loss -1.6979187726974487\n",
      "Iteration 356: loss -1.8156729936599731\n",
      "Iteration 357: loss -1.7632250785827637\n",
      "Iteration 358: loss -1.6682281494140625\n",
      "Iteration 359: loss -1.7303521633148193\n",
      "Iteration 360: loss -1.7509419918060303\n",
      "Iteration 361: loss -1.7076990604400635\n",
      "Iteration 362: loss -1.5723077058792114\n",
      "Iteration 363: loss -1.8049437999725342\n",
      "Iteration 364: loss -1.869559645652771\n",
      "Iteration 365: loss -1.6927032470703125\n",
      "Iteration 366: loss -1.59664785861969\n",
      "Iteration 367: loss -1.8463891744613647\n",
      "Iteration 368: loss -1.7351107597351074\n",
      "Iteration 369: loss -1.8629580736160278\n",
      "Iteration 370: loss -1.7938796281814575\n",
      "Iteration 371: loss -1.844096302986145\n",
      "Iteration 372: loss -1.5530941486358643\n",
      "Iteration 373: loss -1.7426724433898926\n",
      "Iteration 374: loss -1.8192405700683594\n",
      "Iteration 375: loss -1.7165473699569702\n",
      "Iteration 376: loss -1.7757949829101562\n",
      "Iteration 377: loss -1.7705446481704712\n",
      "Iteration 378: loss -1.6782935857772827\n",
      "Iteration 379: loss -1.7088154554367065\n",
      "Iteration 380: loss -1.7038873434066772\n",
      "Iteration 381: loss -1.758608341217041\n",
      "Iteration 382: loss -1.7493127584457397\n",
      "Iteration 383: loss -1.7618157863616943\n",
      "Iteration 384: loss -1.4144706726074219\n",
      "Iteration 385: loss -1.713476538658142\n",
      "Iteration 386: loss -1.8594881296157837\n",
      "Iteration 387: loss -1.70591402053833\n",
      "Iteration 388: loss -1.5485540628433228\n",
      "Iteration 389: loss -1.5890246629714966\n",
      "Iteration 390: loss -1.7346254587173462\n",
      "Iteration 391: loss -1.8106589317321777\n",
      "Iteration 392: loss -1.7670949697494507\n",
      "Iteration 393: loss -1.5338807106018066\n",
      "Iteration 394: loss -1.8853176832199097\n",
      "Iteration 395: loss -1.7388973236083984\n",
      "Iteration 396: loss -1.7026348114013672\n",
      "Iteration 397: loss -1.7330557107925415\n",
      "Iteration 398: loss -1.6468206644058228\n",
      "Iteration 399: loss -1.6690316200256348\n",
      "Iteration 400: loss -1.58363676071167\n",
      "Iteration 401: loss -1.7682843208312988\n",
      "Iteration 402: loss -1.8541423082351685\n",
      "Iteration 403: loss -1.75214684009552\n",
      "Iteration 404: loss -1.7313865423202515\n",
      "Iteration 405: loss -1.7218366861343384\n",
      "Iteration 406: loss -1.7370779514312744\n",
      "Iteration 407: loss -1.693420171737671\n",
      "Iteration 408: loss -1.6451127529144287\n",
      "Iteration 409: loss -1.5728516578674316\n",
      "Iteration 410: loss -1.6424334049224854\n",
      "Iteration 411: loss -1.6845983266830444\n",
      "Iteration 412: loss -1.7128382921218872\n",
      "Iteration 413: loss -1.7538918256759644\n",
      "Iteration 414: loss -1.821522831916809\n",
      "Iteration 415: loss -1.8344433307647705\n",
      "Iteration 416: loss -1.6545324325561523\n",
      "Iteration 417: loss -1.7030786275863647\n",
      "Iteration 418: loss -1.7346625328063965\n",
      "Iteration 419: loss -1.6143403053283691\n",
      "Iteration 420: loss -1.548845887184143\n",
      "Iteration 421: loss -1.4592519998550415\n",
      "Iteration 422: loss -1.7206237316131592\n",
      "Iteration 423: loss -1.688786268234253\n",
      "Iteration 424: loss -1.7674509286880493\n",
      "Iteration 425: loss -1.909848928451538\n",
      "Iteration 426: loss -1.7076586484909058\n",
      "Iteration 427: loss -1.585638403892517\n",
      "Iteration 428: loss -1.822830319404602\n",
      "Iteration 429: loss -1.6595306396484375\n",
      "Iteration 430: loss -1.800191879272461\n",
      "Iteration 431: loss -1.5872149467468262\n",
      "Iteration 432: loss -1.7628719806671143\n",
      "Iteration 433: loss -1.7548253536224365\n",
      "Iteration 434: loss -1.6677935123443604\n",
      "Iteration 435: loss -1.6701836585998535\n",
      "Iteration 436: loss -1.7437281608581543\n",
      "Iteration 437: loss -1.632468819618225\n",
      "Iteration 438: loss -1.7763549089431763\n",
      "Iteration 439: loss -1.6754283905029297\n",
      "Iteration 440: loss -1.784955382347107\n",
      "Iteration 441: loss -1.8991224765777588\n",
      "Iteration 442: loss -1.6007425785064697\n",
      "Iteration 443: loss -1.722434401512146\n",
      "Iteration 444: loss -1.8299694061279297\n",
      "Iteration 445: loss -1.8265949487686157\n",
      "Iteration 446: loss -1.7094143629074097\n",
      "Iteration 447: loss -1.7268837690353394\n",
      "Iteration 448: loss -1.8257869482040405\n",
      "Iteration 449: loss -1.8085787296295166\n",
      "Iteration 450: loss -1.7397643327713013\n",
      "Iteration 451: loss -1.8635997772216797\n",
      "Iteration 452: loss -1.7564431428909302\n",
      "Iteration 453: loss -1.801282286643982\n",
      "Iteration 454: loss -1.7584948539733887\n",
      "Iteration 455: loss -1.8276846408843994\n",
      "Iteration 456: loss -1.7013863325119019\n",
      "Iteration 457: loss -1.6524544954299927\n",
      "Iteration 458: loss -1.8618510961532593\n",
      "Iteration 459: loss -1.7944921255111694\n",
      "Iteration 460: loss -1.8214370012283325\n",
      "Iteration 461: loss -1.773814082145691\n",
      "Iteration 462: loss -1.8531064987182617\n",
      "Iteration 463: loss -1.8754749298095703\n",
      "Iteration 464: loss -1.9525868892669678\n",
      "Iteration 465: loss -1.8586190938949585\n",
      "Iteration 466: loss -1.6769235134124756\n",
      "Iteration 467: loss -1.8734749555587769\n",
      "Iteration 468: loss -1.6639138460159302\n",
      "Iteration 469: loss -1.8469547033309937\n",
      "Iteration 470: loss -1.701202392578125\n",
      "Iteration 471: loss -1.6957943439483643\n",
      "Iteration 472: loss -1.9239851236343384\n",
      "Iteration 473: loss -1.815659999847412\n",
      "Iteration 474: loss -1.9010732173919678\n",
      "Iteration 475: loss -1.9340347051620483\n",
      "Iteration 476: loss -1.700492024421692\n",
      "Iteration 477: loss -1.7746334075927734\n",
      "Iteration 478: loss -1.840135097503662\n",
      "Iteration 479: loss -1.755435585975647\n",
      "Iteration 480: loss -1.8056877851486206\n",
      "Iteration 481: loss -1.7625168561935425\n",
      "Iteration 482: loss -1.8635404109954834\n",
      "Iteration 483: loss -1.7235506772994995\n",
      "Iteration 484: loss -1.897682785987854\n",
      "Iteration 485: loss -1.7230455875396729\n",
      "Iteration 486: loss -1.7739031314849854\n",
      "Iteration 487: loss -1.7943322658538818\n",
      "Iteration 488: loss -1.7938348054885864\n",
      "Iteration 489: loss -1.8943482637405396\n",
      "Iteration 490: loss -1.753884196281433\n",
      "Iteration 491: loss -1.8073595762252808\n",
      "Iteration 492: loss -1.8190860748291016\n",
      "Iteration 493: loss -1.7646819353103638\n",
      "Iteration 494: loss -1.8440825939178467\n",
      "Iteration 495: loss -1.8493186235427856\n",
      "Iteration 496: loss -1.8017646074295044\n",
      "Iteration 497: loss -1.8510982990264893\n",
      "Iteration 498: loss -1.8371042013168335\n",
      "Iteration 499: loss -1.900161862373352\n",
      "Iteration 500: loss -1.7960947751998901\n",
      "Iteration 501: loss -1.7474923133850098\n",
      "Iteration 502: loss -1.9137014150619507\n",
      "Iteration 503: loss -1.537705659866333\n",
      "Iteration 504: loss -1.6695730686187744\n",
      "Iteration 505: loss -1.882027268409729\n",
      "Iteration 506: loss -1.7536725997924805\n",
      "Iteration 507: loss -1.644570231437683\n",
      "Iteration 508: loss -1.7733532190322876\n",
      "Iteration 509: loss -1.7814123630523682\n",
      "Iteration 510: loss -1.7503376007080078\n",
      "Iteration 511: loss -1.755332112312317\n",
      "Iteration 512: loss -1.8609235286712646\n",
      "Iteration 513: loss -1.8813341856002808\n",
      "Iteration 514: loss -1.602569818496704\n",
      "Iteration 515: loss -1.930482268333435\n",
      "Iteration 516: loss -1.7109500169754028\n",
      "Iteration 517: loss -1.7447558641433716\n",
      "Iteration 518: loss -1.7347074747085571\n",
      "Iteration 519: loss -1.8019492626190186\n",
      "Iteration 520: loss -1.7444932460784912\n",
      "Iteration 521: loss -1.7843352556228638\n",
      "Iteration 522: loss -1.8229660987854004\n",
      "Iteration 523: loss -1.7922179698944092\n",
      "Iteration 524: loss -1.8688344955444336\n",
      "Iteration 525: loss -1.7772517204284668\n",
      "Iteration 526: loss -1.7646687030792236\n",
      "Iteration 527: loss -1.8460391759872437\n",
      "Iteration 528: loss -1.7853825092315674\n",
      "Iteration 529: loss -1.6139440536499023\n",
      "Iteration 530: loss -1.779510259628296\n",
      "Iteration 531: loss -1.862820029258728\n",
      "Iteration 532: loss -1.804358959197998\n",
      "Iteration 533: loss -1.7601568698883057\n",
      "Iteration 534: loss -1.8196560144424438\n",
      "Iteration 535: loss -1.7490774393081665\n",
      "Iteration 536: loss -1.8275035619735718\n",
      "Iteration 537: loss -1.7814141511917114\n",
      "Iteration 538: loss -1.7997926473617554\n",
      "Iteration 539: loss -1.955593466758728\n",
      "Iteration 540: loss -1.764452338218689\n",
      "Iteration 541: loss -1.8551058769226074\n",
      "Iteration 542: loss -1.7740881443023682\n",
      "Iteration 543: loss -1.8648972511291504\n",
      "Iteration 544: loss -1.813193917274475\n",
      "Iteration 545: loss -1.7305316925048828\n",
      "Iteration 546: loss -1.822253704071045\n",
      "Iteration 547: loss -1.826819896697998\n",
      "Iteration 548: loss -1.940151572227478\n",
      "Iteration 549: loss -1.707483172416687\n",
      "Iteration 550: loss -1.806736707687378\n",
      "Iteration 551: loss -1.9188767671585083\n",
      "Iteration 552: loss -1.9184050559997559\n",
      "Iteration 553: loss -1.8810104131698608\n",
      "Iteration 554: loss -1.8335543870925903\n",
      "Iteration 555: loss -1.7411104440689087\n",
      "Iteration 556: loss -1.7296111583709717\n",
      "Iteration 557: loss -1.9124999046325684\n",
      "Iteration 558: loss -1.7935945987701416\n",
      "Iteration 559: loss -1.755680799484253\n",
      "Iteration 560: loss -1.8966283798217773\n",
      "Iteration 561: loss -1.9507585763931274\n",
      "Iteration 562: loss -1.917465329170227\n",
      "Iteration 563: loss -1.7547855377197266\n",
      "Iteration 564: loss -1.88396418094635\n",
      "Iteration 565: loss -1.8362926244735718\n",
      "Iteration 566: loss -2.03244948387146\n",
      "Iteration 567: loss -1.9065051078796387\n",
      "Iteration 568: loss -1.6896140575408936\n",
      "Iteration 569: loss -1.899454951286316\n",
      "Iteration 570: loss -1.885659098625183\n",
      "Iteration 571: loss -1.8315378427505493\n",
      "Iteration 572: loss -1.94320547580719\n",
      "Iteration 573: loss -1.8227540254592896\n",
      "Iteration 574: loss -1.8206626176834106\n",
      "Iteration 575: loss -1.9051140546798706\n",
      "Iteration 576: loss -1.9562759399414062\n",
      "Iteration 577: loss -1.9059206247329712\n",
      "Iteration 578: loss -1.8024647235870361\n",
      "Iteration 579: loss -1.812846302986145\n",
      "Iteration 580: loss -1.9036409854888916\n",
      "Iteration 581: loss -1.8031350374221802\n",
      "Iteration 582: loss -1.8303231000900269\n",
      "Iteration 583: loss -1.8547767400741577\n",
      "Iteration 584: loss -1.754304051399231\n",
      "Iteration 585: loss -1.758473515510559\n",
      "Iteration 586: loss -1.7359914779663086\n",
      "Iteration 587: loss -1.8205920457839966\n",
      "Iteration 588: loss -1.8714443445205688\n",
      "Iteration 589: loss -1.8209099769592285\n",
      "Iteration 590: loss -1.8449848890304565\n",
      "Iteration 591: loss -1.9303803443908691\n",
      "Iteration 592: loss -1.9393662214279175\n",
      "Iteration 593: loss -1.8478546142578125\n",
      "Iteration 594: loss -1.8127918243408203\n",
      "Iteration 595: loss -1.909451961517334\n",
      "Iteration 596: loss -1.8237680196762085\n",
      "Iteration 597: loss -1.8637090921401978\n",
      "Iteration 598: loss -1.799973964691162\n",
      "Iteration 599: loss -1.778389811515808\n",
      "Iteration 600: loss -1.8513118028640747\n",
      "Iteration 601: loss -1.9527413845062256\n",
      "Iteration 602: loss -1.9009422063827515\n",
      "Iteration 603: loss -1.8977999687194824\n",
      "Iteration 604: loss -1.9025276899337769\n",
      "Iteration 605: loss -1.7473098039627075\n",
      "Iteration 606: loss -1.856960415840149\n",
      "Iteration 607: loss -1.9202626943588257\n",
      "Iteration 608: loss -1.7993361949920654\n",
      "Iteration 609: loss -1.8462371826171875\n",
      "Iteration 610: loss -1.761892557144165\n",
      "Iteration 611: loss -1.9512406587600708\n",
      "Iteration 612: loss -1.9012941122055054\n",
      "Iteration 613: loss -1.940682291984558\n",
      "Iteration 614: loss -1.8723394870758057\n",
      "Iteration 615: loss -1.8237972259521484\n",
      "Iteration 616: loss -1.8204082250595093\n",
      "Iteration 617: loss -1.7699908018112183\n",
      "Iteration 618: loss -1.957185983657837\n",
      "Iteration 619: loss -1.8421111106872559\n",
      "Iteration 620: loss -1.648614764213562\n",
      "Iteration 621: loss -1.7148957252502441\n",
      "Iteration 622: loss -1.6737608909606934\n",
      "Iteration 623: loss -1.3873836994171143\n",
      "Iteration 624: loss -1.6885403394699097\n",
      "Iteration 625: loss -1.875124216079712\n",
      "Iteration 626: loss -1.773459792137146\n",
      "Iteration 627: loss -1.7447881698608398\n",
      "Iteration 628: loss -1.7267777919769287\n",
      "Iteration 629: loss -1.6851122379302979\n",
      "Iteration 630: loss -1.7050912380218506\n",
      "Iteration 631: loss -1.6974875926971436\n",
      "Iteration 632: loss -1.8488775491714478\n",
      "Iteration 633: loss -1.5567400455474854\n",
      "Iteration 634: loss -1.688567042350769\n",
      "Iteration 635: loss -1.718278169631958\n",
      "Iteration 636: loss -1.7328786849975586\n",
      "Iteration 637: loss -1.645510196685791\n",
      "Iteration 638: loss -1.4277539253234863\n",
      "Iteration 639: loss -1.6340692043304443\n",
      "Iteration 640: loss -1.713865041732788\n",
      "Iteration 641: loss -1.7656272649765015\n",
      "Iteration 642: loss -1.7268227338790894\n",
      "Iteration 643: loss -1.8290294408798218\n",
      "Iteration 644: loss -1.7387679815292358\n",
      "Iteration 645: loss -1.7829248905181885\n",
      "Iteration 646: loss -1.7688344717025757\n",
      "Iteration 647: loss -1.7443151473999023\n",
      "Iteration 648: loss -1.8075213432312012\n",
      "Iteration 649: loss -1.7936736345291138\n",
      "Iteration 650: loss -1.696610927581787\n",
      "Iteration 651: loss -1.7744981050491333\n",
      "Iteration 652: loss -1.6776913404464722\n",
      "Iteration 653: loss -1.707808494567871\n",
      "Iteration 654: loss -1.7614296674728394\n",
      "Iteration 655: loss -1.7843551635742188\n",
      "Iteration 656: loss -1.9018200635910034\n",
      "Iteration 657: loss -1.8855193853378296\n",
      "Iteration 658: loss -1.8376500606536865\n",
      "Iteration 659: loss -1.8759602308273315\n",
      "Iteration 660: loss -1.9783523082733154\n",
      "Iteration 661: loss -1.806692123413086\n",
      "Iteration 662: loss -1.8039904832839966\n",
      "Iteration 663: loss -1.8747411966323853\n",
      "Iteration 664: loss -1.9606965780258179\n",
      "Iteration 665: loss -1.8426470756530762\n",
      "Iteration 666: loss -1.75520658493042\n",
      "Iteration 667: loss -1.9168696403503418\n",
      "Iteration 668: loss -1.9095401763916016\n",
      "Iteration 669: loss -1.9497531652450562\n",
      "Iteration 670: loss -1.8692948818206787\n",
      "Iteration 671: loss -1.9254512786865234\n",
      "Iteration 672: loss -1.8175534009933472\n",
      "Iteration 673: loss -1.9620821475982666\n",
      "Iteration 674: loss -1.8614044189453125\n",
      "Iteration 675: loss -1.8768962621688843\n",
      "Iteration 676: loss -1.861907958984375\n",
      "Iteration 677: loss -1.8463935852050781\n",
      "Iteration 678: loss -1.948062539100647\n",
      "Iteration 679: loss -1.9906578063964844\n",
      "Iteration 680: loss -1.8511303663253784\n",
      "Iteration 681: loss -1.8795452117919922\n",
      "Iteration 682: loss -1.7478458881378174\n",
      "Iteration 683: loss -1.9157406091690063\n",
      "Iteration 684: loss -1.8012313842773438\n",
      "Iteration 685: loss -1.91970694065094\n",
      "Iteration 686: loss -1.8048580884933472\n",
      "Iteration 687: loss -1.9408849477767944\n",
      "Iteration 688: loss -1.9160125255584717\n",
      "Iteration 689: loss -1.9541155099868774\n",
      "Iteration 690: loss -1.9598308801651\n",
      "Iteration 691: loss -1.8182414770126343\n",
      "Iteration 692: loss -1.7961379289627075\n",
      "Iteration 693: loss -1.9285938739776611\n",
      "Iteration 694: loss -1.915515661239624\n",
      "Iteration 695: loss -1.7059423923492432\n",
      "Iteration 696: loss -1.8571819067001343\n",
      "Iteration 697: loss -1.814320683479309\n",
      "Iteration 698: loss -1.7161186933517456\n",
      "Iteration 699: loss -1.9829095602035522\n",
      "Iteration 700: loss -1.7760009765625\n",
      "Iteration 701: loss -1.7890022993087769\n",
      "Iteration 702: loss -1.7920324802398682\n",
      "Iteration 703: loss -1.7712334394454956\n",
      "Iteration 704: loss -1.9089035987854004\n",
      "Iteration 705: loss -1.8692092895507812\n",
      "Iteration 706: loss -1.7535220384597778\n",
      "Iteration 707: loss -1.7540041208267212\n",
      "Iteration 708: loss -1.8860677480697632\n",
      "Iteration 709: loss -1.917436122894287\n",
      "Iteration 710: loss -1.7187334299087524\n",
      "Iteration 711: loss -1.5787588357925415\n",
      "Iteration 712: loss -1.7860822677612305\n",
      "Iteration 713: loss -1.6844937801361084\n",
      "Iteration 714: loss -1.60276460647583\n",
      "Iteration 715: loss -1.8121691942214966\n",
      "Iteration 716: loss -1.8346508741378784\n",
      "Iteration 717: loss -1.787889003753662\n",
      "Iteration 718: loss -1.760542392730713\n",
      "Iteration 719: loss -1.8923307657241821\n",
      "Iteration 720: loss -1.867519497871399\n",
      "Iteration 721: loss -1.7209511995315552\n",
      "Iteration 722: loss -2.000735282897949\n",
      "Iteration 723: loss -1.9382942914962769\n",
      "Iteration 724: loss -1.9188579320907593\n",
      "Iteration 725: loss -1.8104727268218994\n",
      "Iteration 726: loss -1.8643157482147217\n",
      "Iteration 727: loss -1.8775877952575684\n",
      "Iteration 728: loss -1.8521150350570679\n",
      "Iteration 729: loss -1.9831466674804688\n",
      "Iteration 730: loss -1.8805123567581177\n",
      "Iteration 731: loss -1.8228564262390137\n",
      "Iteration 732: loss -1.8632166385650635\n",
      "Iteration 733: loss -1.9077858924865723\n",
      "Iteration 734: loss -1.8545641899108887\n",
      "Iteration 735: loss -1.8630720376968384\n",
      "Iteration 736: loss -1.9693971872329712\n",
      "Iteration 737: loss -1.8477649688720703\n",
      "Iteration 738: loss -2.039156675338745\n",
      "Iteration 739: loss -1.9624260663986206\n",
      "Iteration 740: loss -1.8234612941741943\n",
      "Iteration 741: loss -1.8390825986862183\n",
      "Iteration 742: loss -1.8921319246292114\n",
      "Iteration 743: loss -1.8961986303329468\n",
      "Iteration 744: loss -2.015369415283203\n",
      "Iteration 745: loss -1.894322156906128\n",
      "Iteration 746: loss -1.8825170993804932\n",
      "Iteration 747: loss -2.0161774158477783\n",
      "Iteration 748: loss -1.8561065196990967\n",
      "Iteration 749: loss -1.9750726222991943\n",
      "Iteration 750: loss -2.0513288974761963\n",
      "Iteration 751: loss -1.8735238313674927\n",
      "Iteration 752: loss -1.770053505897522\n",
      "Iteration 753: loss -1.9849916696548462\n",
      "Iteration 754: loss -1.9373029470443726\n",
      "Iteration 755: loss -1.986130952835083\n",
      "Iteration 756: loss -1.9324404001235962\n",
      "Iteration 757: loss -1.8613559007644653\n",
      "Iteration 758: loss -1.8721098899841309\n",
      "Iteration 759: loss -1.9419327974319458\n",
      "Iteration 760: loss -1.9335732460021973\n",
      "Iteration 761: loss -1.9689525365829468\n",
      "Iteration 762: loss -1.8539636135101318\n",
      "Iteration 763: loss -1.8192188739776611\n",
      "Iteration 764: loss -1.941205382347107\n",
      "Iteration 765: loss -1.8854262828826904\n",
      "Iteration 766: loss -2.0438992977142334\n",
      "Iteration 767: loss -1.87932550907135\n",
      "Iteration 768: loss -1.9553258419036865\n",
      "Iteration 769: loss -2.0981943607330322\n",
      "Iteration 770: loss -1.96254301071167\n",
      "Iteration 771: loss -1.8887444734573364\n",
      "Iteration 772: loss -1.8754833936691284\n",
      "Iteration 773: loss -1.8076860904693604\n",
      "Iteration 774: loss -1.8336442708969116\n",
      "Iteration 775: loss -1.915305733680725\n",
      "Iteration 776: loss -1.8586678504943848\n",
      "Iteration 777: loss -1.9258167743682861\n",
      "Iteration 778: loss -1.877257227897644\n",
      "Iteration 779: loss -1.9379682540893555\n",
      "Iteration 780: loss -1.8763359785079956\n",
      "Iteration 781: loss -1.9953902959823608\n",
      "Iteration 782: loss -1.7997097969055176\n",
      "Iteration 783: loss -2.0075721740722656\n",
      "Iteration 784: loss -1.8964053392410278\n",
      "Iteration 785: loss -2.056370258331299\n",
      "Iteration 786: loss -1.9518226385116577\n",
      "Iteration 787: loss -2.001821279525757\n",
      "Iteration 788: loss -1.8696197271347046\n",
      "Iteration 789: loss -1.8720519542694092\n",
      "Iteration 790: loss -2.012913703918457\n",
      "Iteration 791: loss -1.9600902795791626\n",
      "Iteration 792: loss -1.7912896871566772\n",
      "Iteration 793: loss -1.8299157619476318\n",
      "Iteration 794: loss -1.950201392173767\n",
      "Iteration 795: loss -1.9505921602249146\n",
      "Iteration 796: loss -1.7189428806304932\n",
      "Iteration 797: loss -2.04508900642395\n",
      "Iteration 798: loss -1.8725204467773438\n",
      "Iteration 799: loss -1.8394585847854614\n",
      "Iteration 800: loss -1.886028528213501\n",
      "Iteration 801: loss -1.9524468183517456\n",
      "Iteration 802: loss -1.8407434225082397\n",
      "Iteration 803: loss -1.7204264402389526\n",
      "Iteration 804: loss -1.822418212890625\n",
      "Iteration 805: loss -1.7260388135910034\n",
      "Iteration 806: loss -1.939774751663208\n",
      "Iteration 807: loss -1.8221768140792847\n",
      "Iteration 808: loss -1.8928409814834595\n",
      "Iteration 809: loss -1.877482295036316\n",
      "Iteration 810: loss -1.8217735290527344\n",
      "Iteration 811: loss -1.8680697679519653\n",
      "Iteration 812: loss -1.910207748413086\n",
      "Iteration 813: loss -2.0473110675811768\n",
      "Iteration 814: loss -1.7309294939041138\n",
      "Iteration 815: loss -1.8296762704849243\n",
      "Iteration 816: loss -2.0022735595703125\n",
      "Iteration 817: loss -1.7430654764175415\n",
      "Iteration 818: loss -1.8412777185440063\n",
      "Iteration 819: loss -1.9885401725769043\n",
      "Iteration 820: loss -1.8996000289916992\n",
      "Iteration 821: loss -1.8883297443389893\n",
      "Iteration 822: loss -1.9082154035568237\n",
      "Iteration 823: loss -1.7694065570831299\n",
      "Iteration 824: loss -1.8384478092193604\n",
      "Iteration 825: loss -1.9048986434936523\n",
      "Iteration 826: loss -1.917925477027893\n",
      "Iteration 827: loss -1.770813226699829\n",
      "Iteration 828: loss -1.799094796180725\n",
      "Iteration 829: loss -1.85649836063385\n",
      "Iteration 830: loss -1.866452932357788\n",
      "Iteration 831: loss -1.724650502204895\n",
      "Iteration 832: loss -1.9588797092437744\n",
      "Iteration 833: loss -1.8506160974502563\n",
      "Iteration 834: loss -1.7063767910003662\n",
      "Iteration 835: loss -1.8386034965515137\n",
      "Iteration 836: loss -1.9832535982131958\n",
      "Iteration 837: loss -1.8199008703231812\n",
      "Iteration 838: loss -1.7360045909881592\n",
      "Iteration 839: loss -1.9882981777191162\n",
      "Iteration 840: loss -1.8816834688186646\n",
      "Iteration 841: loss -1.6708333492279053\n",
      "Iteration 842: loss -1.8517333269119263\n",
      "Iteration 843: loss -1.8200219869613647\n",
      "Iteration 844: loss -1.8472075462341309\n",
      "Iteration 845: loss -2.0195868015289307\n",
      "Iteration 846: loss -1.9111379384994507\n",
      "Iteration 847: loss -2.020070791244507\n",
      "Iteration 848: loss -1.9529380798339844\n",
      "Iteration 849: loss -1.8359044790267944\n",
      "Iteration 850: loss -1.856553554534912\n",
      "Iteration 851: loss -1.8588476181030273\n",
      "Iteration 852: loss -2.009995222091675\n",
      "Iteration 853: loss -2.0326988697052\n",
      "Iteration 854: loss -1.9284578561782837\n",
      "Iteration 855: loss -1.9826509952545166\n",
      "Iteration 856: loss -1.9492743015289307\n",
      "Iteration 857: loss -1.7889952659606934\n",
      "Iteration 858: loss -1.9385536909103394\n",
      "Iteration 859: loss -1.8796215057373047\n",
      "Iteration 860: loss -2.030672550201416\n",
      "Iteration 861: loss -1.7788753509521484\n",
      "Iteration 862: loss -1.8096286058425903\n",
      "Iteration 863: loss -1.8879308700561523\n",
      "Iteration 864: loss -1.8515533208847046\n",
      "Iteration 865: loss -1.9441372156143188\n",
      "Iteration 866: loss -1.9935288429260254\n",
      "Iteration 867: loss -1.8979564905166626\n",
      "Iteration 868: loss -1.811657428741455\n",
      "Iteration 869: loss -1.7992353439331055\n",
      "Iteration 870: loss -1.899996042251587\n",
      "Iteration 871: loss -1.8897695541381836\n",
      "Iteration 872: loss -1.7900439500808716\n",
      "Iteration 873: loss -1.8485279083251953\n",
      "Iteration 874: loss -1.9469321966171265\n",
      "Iteration 875: loss -1.9994558095932007\n",
      "Iteration 876: loss -1.9434746503829956\n",
      "Iteration 877: loss -1.9752938747406006\n",
      "Iteration 878: loss -1.753798484802246\n",
      "Iteration 879: loss -1.7850890159606934\n",
      "Iteration 880: loss -1.9385738372802734\n",
      "Iteration 881: loss -1.802909016609192\n",
      "Iteration 882: loss -1.8125923871994019\n",
      "Iteration 883: loss -1.9080818891525269\n",
      "Iteration 884: loss -1.7743514776229858\n",
      "Iteration 885: loss -1.911540150642395\n",
      "Iteration 886: loss -1.930283784866333\n",
      "Iteration 887: loss -1.7833731174468994\n",
      "Iteration 888: loss -1.8695321083068848\n",
      "Iteration 889: loss -1.9195911884307861\n",
      "Iteration 890: loss -1.850704312324524\n",
      "Iteration 891: loss -1.846998691558838\n",
      "Iteration 892: loss -1.8656539916992188\n",
      "Iteration 893: loss -1.9680423736572266\n",
      "Iteration 894: loss -1.820872187614441\n",
      "Iteration 895: loss -1.8604000806808472\n",
      "Iteration 896: loss -1.8470498323440552\n",
      "Iteration 897: loss -1.9072076082229614\n",
      "Iteration 898: loss -2.014685869216919\n",
      "Iteration 899: loss -1.8319936990737915\n",
      "Iteration 900: loss -1.958112120628357\n",
      "Iteration 901: loss -1.8675569295883179\n",
      "Iteration 902: loss -1.9701167345046997\n",
      "Iteration 903: loss -1.991845965385437\n",
      "Iteration 904: loss -1.7481695413589478\n",
      "Iteration 905: loss -1.958178162574768\n",
      "Iteration 906: loss -1.9723680019378662\n",
      "Iteration 907: loss -1.9675832986831665\n",
      "Iteration 908: loss -1.9105315208435059\n",
      "Iteration 909: loss -1.8490328788757324\n",
      "Iteration 910: loss -1.9597570896148682\n",
      "Iteration 911: loss -1.9667794704437256\n",
      "Iteration 912: loss -1.9859368801116943\n",
      "Iteration 913: loss -1.8604085445404053\n",
      "Iteration 914: loss -1.9051733016967773\n",
      "Iteration 915: loss -2.015331268310547\n",
      "Iteration 916: loss -2.1167068481445312\n",
      "Iteration 917: loss -1.8608458042144775\n",
      "Iteration 918: loss -1.9401806592941284\n",
      "Iteration 919: loss -1.8899822235107422\n",
      "Iteration 920: loss -2.0111632347106934\n",
      "Iteration 921: loss -1.9868879318237305\n",
      "Iteration 922: loss -1.8486309051513672\n",
      "Iteration 923: loss -1.8208132982254028\n",
      "Iteration 924: loss -1.806105613708496\n",
      "Iteration 925: loss -2.0544662475585938\n",
      "Iteration 926: loss -1.972859501838684\n",
      "Iteration 927: loss -1.912041425704956\n",
      "Iteration 928: loss -2.076059103012085\n",
      "Iteration 929: loss -1.8809312582015991\n",
      "Iteration 930: loss -2.0087599754333496\n",
      "Iteration 931: loss -1.8938716650009155\n",
      "Iteration 932: loss -1.9814547300338745\n",
      "Iteration 933: loss -2.021162986755371\n",
      "Iteration 934: loss -1.9217586517333984\n",
      "Iteration 935: loss -1.9914183616638184\n",
      "Iteration 936: loss -1.9461486339569092\n",
      "Iteration 937: loss -1.9877671003341675\n",
      "Iteration 938: loss -1.8720780611038208\n",
      "Iteration 939: loss -2.1715784072875977\n",
      "Iteration 940: loss -1.9619494676589966\n",
      "Iteration 941: loss -1.865503191947937\n",
      "Iteration 942: loss -1.8794623613357544\n",
      "Iteration 943: loss -1.9973902702331543\n",
      "Iteration 944: loss -1.9845184087753296\n",
      "Iteration 945: loss -2.048079490661621\n",
      "Iteration 946: loss -1.8391501903533936\n",
      "Iteration 947: loss -1.9987210035324097\n",
      "Iteration 948: loss -1.9558743238449097\n",
      "Iteration 949: loss -1.9071044921875\n",
      "Iteration 950: loss -1.8137106895446777\n",
      "Iteration 951: loss -1.9170767068862915\n",
      "Iteration 952: loss -1.879228115081787\n",
      "Iteration 953: loss -1.9619550704956055\n",
      "Iteration 954: loss -1.9619174003601074\n",
      "Iteration 955: loss -2.0517537593841553\n",
      "Iteration 956: loss -1.79603910446167\n",
      "Iteration 957: loss -1.950976848602295\n",
      "Iteration 958: loss -2.0123136043548584\n",
      "Iteration 959: loss -1.9180026054382324\n",
      "Iteration 960: loss -1.9715420007705688\n",
      "Iteration 961: loss -1.898514986038208\n",
      "Iteration 962: loss -1.9840432405471802\n",
      "Iteration 963: loss -1.90468430519104\n",
      "Iteration 964: loss -1.9480749368667603\n",
      "Iteration 965: loss -2.0129055976867676\n",
      "Iteration 966: loss -2.0012869834899902\n",
      "Iteration 967: loss -1.933753490447998\n",
      "Iteration 968: loss -1.9631590843200684\n",
      "Iteration 969: loss -1.9600002765655518\n",
      "Iteration 970: loss -1.9512267112731934\n",
      "Iteration 971: loss -1.993249773979187\n",
      "Iteration 972: loss -1.9647011756896973\n",
      "Iteration 973: loss -1.9609113931655884\n",
      "Iteration 974: loss -2.010624408721924\n",
      "Iteration 975: loss -1.8650974035263062\n",
      "Iteration 976: loss -1.9305917024612427\n",
      "Iteration 977: loss -1.8640143871307373\n",
      "Iteration 978: loss -1.844660758972168\n",
      "Iteration 979: loss -1.9040427207946777\n",
      "Iteration 980: loss -2.00494647026062\n",
      "Iteration 981: loss -1.7171882390975952\n",
      "Iteration 982: loss -1.835866093635559\n",
      "Iteration 983: loss -1.999237060546875\n",
      "Iteration 984: loss -1.9765311479568481\n",
      "Iteration 985: loss -1.9635272026062012\n",
      "Iteration 986: loss -1.9742099046707153\n",
      "Iteration 987: loss -2.0028440952301025\n",
      "Iteration 988: loss -1.8831292390823364\n",
      "Iteration 989: loss -1.9461904764175415\n",
      "Iteration 990: loss -2.110816240310669\n",
      "Iteration 991: loss -1.8936991691589355\n",
      "Iteration 992: loss -1.862947702407837\n",
      "Iteration 993: loss -1.7877572774887085\n",
      "Iteration 994: loss -1.9818061590194702\n",
      "Iteration 995: loss -2.116166114807129\n",
      "Iteration 996: loss -1.985555648803711\n",
      "Iteration 997: loss -2.072507619857788\n",
      "Iteration 998: loss -2.0231740474700928\n",
      "Iteration 999: loss -1.9607137441635132\n",
      "Iteration 1000: loss -1.8519657850265503\n",
      "Iteration 1001: loss -1.9163329601287842\n",
      "Iteration 1002: loss -1.9317344427108765\n",
      "Iteration 1003: loss -1.9469149112701416\n",
      "Iteration 1004: loss -2.0001842975616455\n",
      "Iteration 1005: loss -1.7834910154342651\n",
      "Iteration 1006: loss -1.8579550981521606\n",
      "Iteration 1007: loss -1.8089122772216797\n",
      "Iteration 1008: loss -1.9352390766143799\n",
      "Iteration 1009: loss -1.810640573501587\n",
      "Iteration 1010: loss -1.8619707822799683\n",
      "Iteration 1011: loss -1.7833527326583862\n",
      "Iteration 1012: loss -1.9615752696990967\n",
      "Iteration 1013: loss -1.9778022766113281\n",
      "Iteration 1014: loss -2.108546495437622\n",
      "Iteration 1015: loss -1.9159069061279297\n",
      "Iteration 1016: loss -1.6154409646987915\n",
      "Iteration 1017: loss -1.8867619037628174\n",
      "Iteration 1018: loss -1.8506126403808594\n",
      "Iteration 1019: loss -1.9816553592681885\n",
      "Iteration 1020: loss -2.0766115188598633\n",
      "Iteration 1021: loss -1.9358566999435425\n",
      "Iteration 1022: loss -1.8774785995483398\n",
      "Iteration 1023: loss -1.8594520092010498\n",
      "Iteration 1024: loss -1.9428112506866455\n",
      "Iteration 1025: loss -1.9666351079940796\n",
      "Iteration 1026: loss -1.8890349864959717\n",
      "Iteration 1027: loss -1.7852632999420166\n",
      "Iteration 1028: loss -2.018510580062866\n",
      "Iteration 1029: loss -1.917403221130371\n",
      "Iteration 1030: loss -2.0383598804473877\n",
      "Iteration 1031: loss -1.9522647857666016\n",
      "Iteration 1032: loss -1.9171627759933472\n",
      "Iteration 1033: loss -1.9522414207458496\n",
      "Iteration 1034: loss -1.9739269018173218\n",
      "Iteration 1035: loss -1.8258174657821655\n",
      "Iteration 1036: loss -1.9237197637557983\n",
      "Iteration 1037: loss -2.138500213623047\n",
      "Iteration 1038: loss -1.9080990552902222\n",
      "Iteration 1039: loss -1.935768961906433\n",
      "Iteration 1040: loss -2.036919355392456\n",
      "Iteration 1041: loss -2.119096279144287\n",
      "Iteration 1042: loss -2.1113667488098145\n",
      "Iteration 1043: loss -1.8598418235778809\n",
      "Iteration 1044: loss -1.880692720413208\n",
      "Iteration 1045: loss -1.8460055589675903\n",
      "Iteration 1046: loss -1.8599947690963745\n",
      "Iteration 1047: loss -1.8743847608566284\n",
      "Iteration 1048: loss -1.9604053497314453\n",
      "Iteration 1049: loss -1.981630563735962\n",
      "Iteration 1050: loss -1.9130239486694336\n",
      "Iteration 1051: loss -2.040269374847412\n",
      "Iteration 1052: loss -2.1153159141540527\n",
      "Iteration 1053: loss -1.9788249731063843\n",
      "Iteration 1054: loss -2.003112316131592\n",
      "Iteration 1055: loss -1.869854211807251\n",
      "Iteration 1056: loss -1.8745543956756592\n",
      "Iteration 1057: loss -1.991459608078003\n",
      "Iteration 1058: loss -2.0486814975738525\n",
      "Iteration 1059: loss -1.9963204860687256\n",
      "Iteration 1060: loss -2.0540552139282227\n",
      "Iteration 1061: loss -1.8975788354873657\n",
      "Iteration 1062: loss -2.1500134468078613\n",
      "Iteration 1063: loss -1.9487991333007812\n",
      "Iteration 1064: loss -1.8987324237823486\n",
      "Iteration 1065: loss -2.033343553543091\n",
      "Iteration 1066: loss -1.9023069143295288\n",
      "Iteration 1067: loss -1.9733752012252808\n",
      "Iteration 1068: loss -1.9996241331100464\n",
      "Iteration 1069: loss -1.9994659423828125\n",
      "Iteration 1070: loss -2.0483055114746094\n",
      "Iteration 1071: loss -2.055600881576538\n",
      "Iteration 1072: loss -1.7263861894607544\n",
      "Iteration 1073: loss -1.889174461364746\n",
      "Iteration 1074: loss -1.9212636947631836\n",
      "Iteration 1075: loss -1.9032557010650635\n",
      "Iteration 1076: loss -1.8994203805923462\n",
      "Iteration 1077: loss -1.9534748792648315\n",
      "Iteration 1078: loss -1.8462917804718018\n",
      "Iteration 1079: loss -1.9448812007904053\n",
      "Iteration 1080: loss -2.161012887954712\n",
      "Iteration 1081: loss -1.9080785512924194\n",
      "Iteration 1082: loss -1.9468953609466553\n",
      "Iteration 1083: loss -1.9289207458496094\n",
      "Iteration 1084: loss -1.9411770105361938\n",
      "Iteration 1085: loss -1.8367143869400024\n",
      "Iteration 1086: loss -1.8509372472763062\n",
      "Iteration 1087: loss -1.75859534740448\n",
      "Iteration 1088: loss -1.9814687967300415\n",
      "Iteration 1089: loss -1.9740930795669556\n",
      "Iteration 1090: loss -1.92091703414917\n",
      "Iteration 1091: loss -2.0443227291107178\n",
      "Iteration 1092: loss -2.069528341293335\n",
      "Iteration 1093: loss -2.017916440963745\n",
      "Iteration 1094: loss -1.851122498512268\n",
      "Iteration 1095: loss -1.9539763927459717\n",
      "Iteration 1096: loss -1.9965044260025024\n",
      "Iteration 1097: loss -1.9029500484466553\n",
      "Iteration 1098: loss -1.977433443069458\n",
      "Iteration 1099: loss -1.8510515689849854\n",
      "Iteration 1100: loss -2.0454604625701904\n",
      "Iteration 1101: loss -1.9717119932174683\n",
      "Iteration 1102: loss -2.0496649742126465\n",
      "Iteration 1103: loss -2.000659465789795\n",
      "Iteration 1104: loss -2.027907609939575\n",
      "Iteration 1105: loss -1.9633435010910034\n",
      "Iteration 1106: loss -1.9817663431167603\n",
      "Iteration 1107: loss -1.977167010307312\n",
      "Iteration 1108: loss -1.9040511846542358\n",
      "Iteration 1109: loss -1.9115393161773682\n",
      "Iteration 1110: loss -1.8780161142349243\n",
      "Iteration 1111: loss -1.8866026401519775\n",
      "Iteration 1112: loss -1.8991488218307495\n",
      "Iteration 1113: loss -1.804118275642395\n",
      "Iteration 1114: loss -2.051621913909912\n",
      "Iteration 1115: loss -1.9545427560806274\n",
      "Iteration 1116: loss -1.9135167598724365\n",
      "Iteration 1117: loss -1.9481637477874756\n",
      "Iteration 1118: loss -1.939263105392456\n",
      "Iteration 1119: loss -1.9869322776794434\n",
      "Iteration 1120: loss -1.8318618535995483\n",
      "Iteration 1121: loss -1.8662859201431274\n",
      "Iteration 1122: loss -1.7528527975082397\n",
      "Iteration 1123: loss -1.9951807260513306\n",
      "Iteration 1124: loss -2.041797637939453\n",
      "Iteration 1125: loss -1.9827309846878052\n",
      "Iteration 1126: loss -1.9870939254760742\n",
      "Iteration 1127: loss -1.8980886936187744\n",
      "Iteration 1128: loss -1.8098678588867188\n",
      "Iteration 1129: loss -1.9883997440338135\n",
      "Iteration 1130: loss -1.9449360370635986\n",
      "Iteration 1131: loss -1.8764413595199585\n",
      "Iteration 1132: loss -1.884925365447998\n",
      "Iteration 1133: loss -2.018270492553711\n",
      "Iteration 1134: loss -2.035165309906006\n",
      "Iteration 1135: loss -2.1066598892211914\n",
      "Iteration 1136: loss -1.9670990705490112\n",
      "Iteration 1137: loss -1.9618818759918213\n",
      "Iteration 1138: loss -2.029924154281616\n",
      "Iteration 1139: loss -2.0535409450531006\n",
      "Iteration 1140: loss -2.0373306274414062\n",
      "Iteration 1141: loss -2.075648069381714\n",
      "Iteration 1142: loss -1.876721739768982\n",
      "Iteration 1143: loss -1.928207516670227\n",
      "Iteration 1144: loss -1.79960298538208\n",
      "Iteration 1145: loss -1.8530919551849365\n",
      "Iteration 1146: loss -1.770574927330017\n",
      "Iteration 1147: loss -1.8023090362548828\n",
      "Iteration 1148: loss -2.0048179626464844\n",
      "Iteration 1149: loss -1.7722688913345337\n",
      "Iteration 1150: loss -1.7250897884368896\n",
      "Iteration 1151: loss -1.752739667892456\n",
      "Iteration 1152: loss -1.8132716417312622\n",
      "Iteration 1153: loss -1.924119472503662\n",
      "Iteration 1154: loss -1.9365872144699097\n",
      "Iteration 1155: loss -1.85878324508667\n",
      "Iteration 1156: loss -2.015442132949829\n",
      "Iteration 1157: loss -1.9007296562194824\n",
      "Iteration 1158: loss -1.701479196548462\n",
      "Iteration 1159: loss -1.9574658870697021\n",
      "Iteration 1160: loss -1.9274141788482666\n",
      "Iteration 1161: loss -1.8414647579193115\n",
      "Iteration 1162: loss -1.9457894563674927\n",
      "Iteration 1163: loss -1.8189163208007812\n",
      "Iteration 1164: loss -1.9222530126571655\n",
      "Iteration 1165: loss -1.8377081155776978\n",
      "Iteration 1166: loss -2.029139280319214\n",
      "Iteration 1167: loss -1.8959174156188965\n",
      "Iteration 1168: loss -2.042421579360962\n",
      "Iteration 1169: loss -2.0324790477752686\n",
      "Iteration 1170: loss -1.9250357151031494\n",
      "Iteration 1171: loss -2.087526321411133\n",
      "Iteration 1172: loss -1.9278157949447632\n",
      "Iteration 1173: loss -1.9025193452835083\n",
      "Iteration 1174: loss -2.102651357650757\n",
      "Iteration 1175: loss -1.912621259689331\n",
      "Iteration 1176: loss -1.9822542667388916\n",
      "Iteration 1177: loss -1.9680827856063843\n",
      "Iteration 1178: loss -2.0730679035186768\n",
      "Iteration 1179: loss -1.843747615814209\n",
      "Iteration 1180: loss -1.9570547342300415\n",
      "Iteration 1181: loss -1.9051543474197388\n",
      "Iteration 1182: loss -1.8984792232513428\n",
      "Iteration 1183: loss -2.007967710494995\n",
      "Iteration 1184: loss -2.0748817920684814\n",
      "Iteration 1185: loss -1.9913891553878784\n",
      "Iteration 1186: loss -1.9219191074371338\n",
      "Iteration 1187: loss -2.0310912132263184\n",
      "Iteration 1188: loss -2.128412961959839\n",
      "Iteration 1189: loss -1.9303405284881592\n",
      "Iteration 1190: loss -2.0108892917633057\n",
      "Iteration 1191: loss -1.9923157691955566\n",
      "Iteration 1192: loss -2.1318986415863037\n",
      "Iteration 1193: loss -1.9356839656829834\n",
      "Iteration 1194: loss -1.8515633344650269\n",
      "Iteration 1195: loss -1.924973726272583\n",
      "Iteration 1196: loss -1.8751624822616577\n",
      "Iteration 1197: loss -1.7220220565795898\n",
      "Iteration 1198: loss -2.009983539581299\n",
      "Iteration 1199: loss -1.984556794166565\n",
      "Iteration 1200: loss -1.8823144435882568\n",
      "Iteration 1201: loss -1.8884694576263428\n",
      "Iteration 1202: loss -2.047163963317871\n",
      "Iteration 1203: loss -1.9351967573165894\n",
      "Iteration 1204: loss -2.0603721141815186\n",
      "Iteration 1205: loss -2.071000337600708\n",
      "Iteration 1206: loss -2.007916212081909\n",
      "Iteration 1207: loss -2.0744268894195557\n",
      "Iteration 1208: loss -2.006693124771118\n",
      "Iteration 1209: loss -2.024660348892212\n",
      "Iteration 1210: loss -2.025651216506958\n",
      "Iteration 1211: loss -1.8664932250976562\n",
      "Iteration 1212: loss -1.984305739402771\n",
      "Iteration 1213: loss -1.8437930345535278\n",
      "Iteration 1214: loss -1.9940959215164185\n",
      "Iteration 1215: loss -2.113238573074341\n",
      "Iteration 1216: loss -2.013155698776245\n",
      "Iteration 1217: loss -1.8313087224960327\n",
      "Iteration 1218: loss -2.104457139968872\n",
      "Iteration 1219: loss -2.0004265308380127\n",
      "Iteration 1220: loss -2.085681676864624\n",
      "Iteration 1221: loss -1.9899593591690063\n",
      "Iteration 1222: loss -1.9895342588424683\n",
      "Iteration 1223: loss -2.133939504623413\n",
      "Iteration 1224: loss -1.80787193775177\n",
      "Iteration 1225: loss -1.8381061553955078\n",
      "Iteration 1226: loss -1.9655488729476929\n",
      "Iteration 1227: loss -2.014920949935913\n",
      "Iteration 1228: loss -2.1024909019470215\n",
      "Iteration 1229: loss -1.85581374168396\n",
      "Iteration 1230: loss -1.992013692855835\n",
      "Iteration 1231: loss -2.017768144607544\n",
      "Iteration 1232: loss -2.004922866821289\n",
      "Iteration 1233: loss -2.0106000900268555\n",
      "Iteration 1234: loss -2.0274434089660645\n",
      "Iteration 1235: loss -1.8643670082092285\n",
      "Iteration 1236: loss -1.9307379722595215\n",
      "Iteration 1237: loss -1.9410958290100098\n",
      "Iteration 1238: loss -1.9302600622177124\n",
      "Iteration 1239: loss -1.925282597541809\n",
      "Iteration 1240: loss -2.0146610736846924\n",
      "Iteration 1241: loss -2.06001615524292\n",
      "Iteration 1242: loss -1.9399337768554688\n",
      "Iteration 1243: loss -2.0467441082000732\n",
      "Iteration 1244: loss -2.011650562286377\n",
      "Iteration 1245: loss -1.951194405555725\n",
      "Iteration 1246: loss -2.0771331787109375\n",
      "Iteration 1247: loss -1.9728367328643799\n",
      "Iteration 1248: loss -2.0650558471679688\n",
      "Iteration 1249: loss -1.9998351335525513\n",
      "Iteration 1250: loss -1.997554898262024\n",
      "Iteration 1251: loss -1.9106404781341553\n",
      "Iteration 1252: loss -1.8006582260131836\n",
      "Iteration 1253: loss -1.8596105575561523\n",
      "Iteration 1254: loss -2.0333712100982666\n",
      "Iteration 1255: loss -1.9541252851486206\n",
      "Iteration 1256: loss -1.904329776763916\n",
      "Iteration 1257: loss -1.7817977666854858\n",
      "Iteration 1258: loss -1.9904245138168335\n",
      "Iteration 1259: loss -1.870882511138916\n",
      "Iteration 1260: loss -1.8489168882369995\n",
      "Iteration 1261: loss -1.7683709859848022\n",
      "Iteration 1262: loss -1.994819164276123\n",
      "Iteration 1263: loss -2.0531466007232666\n",
      "Iteration 1264: loss -1.9794210195541382\n",
      "Iteration 1265: loss -2.0792622566223145\n",
      "Iteration 1266: loss -1.7019751071929932\n",
      "Iteration 1267: loss -1.7489324808120728\n",
      "Iteration 1268: loss -1.86598539352417\n",
      "Iteration 1269: loss -1.832498550415039\n",
      "Iteration 1270: loss -1.9590336084365845\n",
      "Iteration 1271: loss -1.9110850095748901\n",
      "Iteration 1272: loss -2.08528995513916\n",
      "Iteration 1273: loss -1.9531038999557495\n",
      "Iteration 1274: loss -2.022569179534912\n",
      "Iteration 1275: loss -1.9348821640014648\n",
      "Iteration 1276: loss -1.7882829904556274\n",
      "Iteration 1277: loss -1.7016842365264893\n",
      "Iteration 1278: loss -1.9777990579605103\n",
      "Iteration 1279: loss -1.9835643768310547\n",
      "Iteration 1280: loss -2.0172278881073\n",
      "Iteration 1281: loss -1.9934982061386108\n",
      "Iteration 1282: loss -1.8965495824813843\n",
      "Iteration 1283: loss -2.054948568344116\n",
      "Iteration 1284: loss -1.8736120462417603\n",
      "Iteration 1285: loss -2.00016188621521\n",
      "Iteration 1286: loss -1.9946720600128174\n",
      "Iteration 1287: loss -1.9109829664230347\n",
      "Iteration 1288: loss -1.8699560165405273\n",
      "Iteration 1289: loss -1.9566054344177246\n",
      "Iteration 1290: loss -2.138944387435913\n",
      "Iteration 1291: loss -1.744332194328308\n",
      "Iteration 1292: loss -1.9144456386566162\n",
      "Iteration 1293: loss -1.9802361726760864\n",
      "Iteration 1294: loss -1.9351390600204468\n",
      "Iteration 1295: loss -2.0126357078552246\n",
      "Iteration 1296: loss -1.8943699598312378\n",
      "Iteration 1297: loss -1.8080708980560303\n",
      "Iteration 1298: loss -2.072831630706787\n",
      "Iteration 1299: loss -1.9583795070648193\n",
      "Iteration 1300: loss -1.9409068822860718\n",
      "Iteration 1301: loss -2.0460731983184814\n",
      "Iteration 1302: loss -1.8638966083526611\n",
      "Iteration 1303: loss -2.071636438369751\n",
      "Iteration 1304: loss -1.9717161655426025\n",
      "Iteration 1305: loss -2.0390024185180664\n",
      "Iteration 1306: loss -2.0368378162384033\n",
      "Iteration 1307: loss -1.9959248304367065\n",
      "Iteration 1308: loss -1.724780559539795\n",
      "Iteration 1309: loss -1.8957315683364868\n",
      "Iteration 1310: loss -2.0660576820373535\n",
      "Iteration 1311: loss -1.9404397010803223\n",
      "Iteration 1312: loss -1.9446407556533813\n",
      "Iteration 1313: loss -2.113002300262451\n",
      "Iteration 1314: loss -2.0136256217956543\n",
      "Iteration 1315: loss -1.8817347288131714\n",
      "Iteration 1316: loss -1.9270871877670288\n",
      "Iteration 1317: loss -1.9030767679214478\n",
      "Iteration 1318: loss -2.0982682704925537\n",
      "Iteration 1319: loss -1.9708040952682495\n",
      "Iteration 1320: loss -2.005798578262329\n",
      "Iteration 1321: loss -2.044241189956665\n",
      "Iteration 1322: loss -1.9148681163787842\n",
      "Iteration 1323: loss -2.1872265338897705\n",
      "Iteration 1324: loss -1.9656308889389038\n",
      "Iteration 1325: loss -2.0469415187835693\n",
      "Iteration 1326: loss -1.9805949926376343\n",
      "Iteration 1327: loss -2.037599563598633\n",
      "Iteration 1328: loss -2.097321033477783\n",
      "Iteration 1329: loss -1.9800018072128296\n",
      "Iteration 1330: loss -1.9659743309020996\n",
      "Iteration 1331: loss -2.0733718872070312\n",
      "Iteration 1332: loss -1.8750442266464233\n",
      "Iteration 1333: loss -2.002139091491699\n",
      "Iteration 1334: loss -1.962607979774475\n",
      "Iteration 1335: loss -1.8655614852905273\n",
      "Iteration 1336: loss -2.006112813949585\n",
      "Iteration 1337: loss -1.930139422416687\n",
      "Iteration 1338: loss -1.9974310398101807\n",
      "Iteration 1339: loss -1.9621857404708862\n",
      "Iteration 1340: loss -2.0297021865844727\n",
      "Iteration 1341: loss -1.9416249990463257\n",
      "Iteration 1342: loss -2.098560094833374\n",
      "Iteration 1343: loss -2.0017995834350586\n",
      "Iteration 1344: loss -1.9650154113769531\n",
      "Iteration 1345: loss -2.1218345165252686\n",
      "Iteration 1346: loss -1.96773362159729\n",
      "Iteration 1347: loss -2.0238327980041504\n",
      "Iteration 1348: loss -2.0831618309020996\n",
      "Iteration 1349: loss -1.9042129516601562\n",
      "Iteration 1350: loss -2.102062702178955\n",
      "Iteration 1351: loss -1.9860665798187256\n",
      "Iteration 1352: loss -1.8290729522705078\n",
      "Iteration 1353: loss -1.8457205295562744\n",
      "Iteration 1354: loss -1.8194215297698975\n",
      "Iteration 1355: loss -1.8543624877929688\n",
      "Iteration 1356: loss -1.9649146795272827\n",
      "Iteration 1357: loss -1.842753291130066\n",
      "Iteration 1358: loss -1.8449904918670654\n",
      "Iteration 1359: loss -1.7853747606277466\n",
      "Iteration 1360: loss -1.7094488143920898\n",
      "Iteration 1361: loss -1.8699904680252075\n",
      "Iteration 1362: loss -1.9082903861999512\n",
      "Iteration 1363: loss -1.882930874824524\n",
      "Iteration 1364: loss -1.8566973209381104\n",
      "Iteration 1365: loss -1.8757872581481934\n",
      "Iteration 1366: loss -1.881034255027771\n",
      "Iteration 1367: loss -1.8069976568222046\n",
      "Iteration 1368: loss -1.932401418685913\n",
      "Iteration 1369: loss -1.9707467555999756\n",
      "Iteration 1370: loss -1.821283221244812\n",
      "Iteration 1371: loss -2.0396525859832764\n",
      "Iteration 1372: loss -1.991477608680725\n",
      "Iteration 1373: loss -1.9800171852111816\n",
      "Iteration 1374: loss -2.083386182785034\n",
      "Iteration 1375: loss -2.0283567905426025\n",
      "Iteration 1376: loss -1.925794005393982\n",
      "Iteration 1377: loss -1.9854109287261963\n",
      "Iteration 1378: loss -1.9541698694229126\n",
      "Iteration 1379: loss -1.9318593740463257\n",
      "Iteration 1380: loss -2.093709707260132\n",
      "Iteration 1381: loss -1.9936877489089966\n",
      "Iteration 1382: loss -1.9374167919158936\n",
      "Iteration 1383: loss -1.9907848834991455\n",
      "Iteration 1384: loss -2.018310308456421\n",
      "Iteration 1385: loss -2.006241798400879\n",
      "Iteration 1386: loss -1.8260761499404907\n",
      "Iteration 1387: loss -1.9733352661132812\n",
      "Iteration 1388: loss -1.9619265794754028\n",
      "Iteration 1389: loss -1.9857839345932007\n",
      "Iteration 1390: loss -1.8217588663101196\n",
      "Iteration 1391: loss -1.8970669507980347\n",
      "Iteration 1392: loss -2.0237557888031006\n",
      "Iteration 1393: loss -1.751115322113037\n",
      "Iteration 1394: loss -1.8005956411361694\n",
      "Iteration 1395: loss -1.9487545490264893\n",
      "Iteration 1396: loss -1.8766332864761353\n",
      "Iteration 1397: loss -1.7222728729248047\n",
      "Iteration 1398: loss -1.8689569234848022\n",
      "Iteration 1399: loss -2.081416368484497\n",
      "Iteration 1400: loss -1.9299657344818115\n",
      "Iteration 1401: loss -1.9814962148666382\n",
      "Iteration 1402: loss -1.9151335954666138\n",
      "Iteration 1403: loss -1.988649845123291\n",
      "Iteration 1404: loss -1.9104704856872559\n",
      "Iteration 1405: loss -1.797526240348816\n",
      "Iteration 1406: loss -1.98151695728302\n",
      "Iteration 1407: loss -2.0118165016174316\n",
      "Iteration 1408: loss -1.7929346561431885\n",
      "Iteration 1409: loss -2.077514886856079\n",
      "Iteration 1410: loss -1.8763580322265625\n",
      "Iteration 1411: loss -2.2158758640289307\n",
      "Iteration 1412: loss -1.8414056301116943\n",
      "Iteration 1413: loss -1.9389756917953491\n",
      "Iteration 1414: loss -1.8595633506774902\n",
      "Iteration 1415: loss -2.081305980682373\n",
      "Iteration 1416: loss -1.9891711473464966\n",
      "Iteration 1417: loss -2.054405689239502\n",
      "Iteration 1418: loss -2.0822417736053467\n",
      "Iteration 1419: loss -2.0290684700012207\n",
      "Iteration 1420: loss -1.9227324724197388\n",
      "Iteration 1421: loss -1.9127929210662842\n",
      "Iteration 1422: loss -1.9867526292800903\n",
      "Iteration 1423: loss -2.0036141872406006\n",
      "Iteration 1424: loss -1.911651611328125\n",
      "Iteration 1425: loss -2.0555131435394287\n",
      "Iteration 1426: loss -1.9206600189208984\n",
      "Iteration 1427: loss -1.9631866216659546\n",
      "Iteration 1428: loss -1.8987805843353271\n",
      "Iteration 1429: loss -1.9720901250839233\n",
      "Iteration 1430: loss -1.8694324493408203\n",
      "Iteration 1431: loss -2.0125911235809326\n",
      "Iteration 1432: loss -1.983781099319458\n",
      "Iteration 1433: loss -2.0008461475372314\n",
      "Iteration 1434: loss -2.2230165004730225\n",
      "Iteration 1435: loss -2.1083180904388428\n",
      "Iteration 1436: loss -1.99104642868042\n",
      "Iteration 1437: loss -1.8904627561569214\n",
      "Iteration 1438: loss -1.9669452905654907\n",
      "Iteration 1439: loss -1.9162859916687012\n",
      "Iteration 1440: loss -1.8832427263259888\n",
      "Iteration 1441: loss -2.057868003845215\n",
      "Iteration 1442: loss -2.1409432888031006\n",
      "Iteration 1443: loss -1.9258233308792114\n",
      "Iteration 1444: loss -1.8347054719924927\n",
      "Iteration 1445: loss -1.9414995908737183\n",
      "Iteration 1446: loss -1.931185007095337\n",
      "Iteration 1447: loss -1.7988864183425903\n",
      "Iteration 1448: loss -2.007249355316162\n",
      "Iteration 1449: loss -1.914282202720642\n",
      "Iteration 1450: loss -2.0060412883758545\n",
      "Iteration 1451: loss -1.9596971273422241\n",
      "Iteration 1452: loss -1.9445468187332153\n",
      "Iteration 1453: loss -1.947928786277771\n",
      "Iteration 1454: loss -2.0620198249816895\n",
      "Iteration 1455: loss -1.8664608001708984\n",
      "Iteration 1456: loss -2.056610107421875\n",
      "Iteration 1457: loss -2.039726495742798\n",
      "Iteration 1458: loss -1.9422903060913086\n",
      "Iteration 1459: loss -1.962930679321289\n",
      "Iteration 1460: loss -2.0433156490325928\n",
      "Iteration 1461: loss -1.9737151861190796\n",
      "Iteration 1462: loss -2.074381113052368\n",
      "Iteration 1463: loss -2.0452067852020264\n",
      "Iteration 1464: loss -1.8884868621826172\n",
      "Iteration 1465: loss -2.047159433364868\n",
      "Iteration 1466: loss -1.9921196699142456\n",
      "Iteration 1467: loss -2.0411272048950195\n",
      "Iteration 1468: loss -1.8609622716903687\n",
      "Iteration 1469: loss -1.9882415533065796\n",
      "Iteration 1470: loss -2.140598773956299\n",
      "Iteration 1471: loss -1.9838846921920776\n",
      "Iteration 1472: loss -1.9520083665847778\n",
      "Iteration 1473: loss -1.9393093585968018\n",
      "Iteration 1474: loss -1.9084734916687012\n",
      "Iteration 1475: loss -1.8152416944503784\n",
      "Iteration 1476: loss -2.0363709926605225\n",
      "Iteration 1477: loss -1.9803129434585571\n",
      "Iteration 1478: loss -1.892535924911499\n",
      "Iteration 1479: loss -1.930469274520874\n",
      "Iteration 1480: loss -2.011401414871216\n",
      "Iteration 1481: loss -2.023224353790283\n",
      "Iteration 1482: loss -1.9600013494491577\n",
      "Iteration 1483: loss -2.089655876159668\n",
      "Iteration 1484: loss -1.9939789772033691\n",
      "Iteration 1485: loss -2.0081570148468018\n",
      "Iteration 1486: loss -1.941605806350708\n",
      "Iteration 1487: loss -1.939048171043396\n",
      "Iteration 1488: loss -1.9467135667800903\n",
      "Iteration 1489: loss -1.95235013961792\n",
      "Iteration 1490: loss -1.9571765661239624\n",
      "Iteration 1491: loss -2.1222009658813477\n",
      "Iteration 1492: loss -1.9258462190628052\n",
      "Iteration 1493: loss -1.9594930410385132\n",
      "Iteration 1494: loss -1.8776763677597046\n",
      "Iteration 1495: loss -2.019622802734375\n",
      "Iteration 1496: loss -2.0588531494140625\n",
      "Iteration 1497: loss -2.0135457515716553\n",
      "Iteration 1498: loss -1.9230886697769165\n",
      "Iteration 1499: loss -1.952073574066162\n",
      "Iteration 1500: loss -2.0908915996551514\n",
      "Iteration 1501: loss -1.9354957342147827\n",
      "Iteration 1502: loss -1.87883722782135\n",
      "Iteration 1503: loss -1.9622653722763062\n",
      "Iteration 1504: loss -2.000748872756958\n",
      "Iteration 1505: loss -1.9289145469665527\n",
      "Iteration 1506: loss -1.9589457511901855\n",
      "Iteration 1507: loss -1.936179518699646\n",
      "Iteration 1508: loss -1.992021918296814\n",
      "Iteration 1509: loss -1.9329711198806763\n",
      "Iteration 1510: loss -2.011512517929077\n",
      "Iteration 1511: loss -1.97793447971344\n",
      "Iteration 1512: loss -2.0143868923187256\n",
      "Iteration 1513: loss -1.9662081003189087\n",
      "Iteration 1514: loss -1.9603867530822754\n",
      "Iteration 1515: loss -2.0511550903320312\n",
      "Iteration 1516: loss -2.0647456645965576\n",
      "Iteration 1517: loss -1.979989767074585\n",
      "Iteration 1518: loss -2.0263164043426514\n",
      "Iteration 1519: loss -1.9277257919311523\n",
      "Iteration 1520: loss -1.9275423288345337\n",
      "Iteration 1521: loss -1.8560725450515747\n",
      "Iteration 1522: loss -2.1302528381347656\n",
      "Iteration 1523: loss -2.0985963344573975\n",
      "Iteration 1524: loss -1.9964230060577393\n",
      "Iteration 1525: loss -2.183891534805298\n",
      "Iteration 1526: loss -1.9762755632400513\n",
      "Iteration 1527: loss -1.7723500728607178\n",
      "Iteration 1528: loss -2.1166536808013916\n",
      "Iteration 1529: loss -2.1631851196289062\n",
      "Iteration 1530: loss -1.8759560585021973\n",
      "Iteration 1531: loss -2.1190993785858154\n",
      "Iteration 1532: loss -1.9480656385421753\n",
      "Iteration 1533: loss -1.8794749975204468\n",
      "Iteration 1534: loss -1.8896430730819702\n",
      "Iteration 1535: loss -1.9268652200698853\n",
      "Iteration 1536: loss -1.8867087364196777\n",
      "Iteration 1537: loss -1.9672118425369263\n",
      "Iteration 1538: loss -1.8892478942871094\n",
      "Iteration 1539: loss -2.0838780403137207\n",
      "Iteration 1540: loss -1.9773775339126587\n",
      "Iteration 1541: loss -2.0804965496063232\n",
      "Iteration 1542: loss -2.051492929458618\n",
      "Iteration 1543: loss -1.934309959411621\n",
      "Iteration 1544: loss -1.8425747156143188\n",
      "Iteration 1545: loss -2.118783473968506\n",
      "Iteration 1546: loss -2.1408159732818604\n",
      "Iteration 1547: loss -2.008375406265259\n",
      "Iteration 1548: loss -1.939707636833191\n",
      "Iteration 1549: loss -1.9924976825714111\n",
      "Iteration 1550: loss -2.0424551963806152\n",
      "Iteration 1551: loss -1.939620018005371\n",
      "Iteration 1552: loss -1.978745698928833\n",
      "Iteration 1553: loss -2.0589840412139893\n",
      "Iteration 1554: loss -2.005939483642578\n",
      "Iteration 1555: loss -2.0196568965911865\n",
      "Iteration 1556: loss -2.0572755336761475\n",
      "Iteration 1557: loss -2.040475606918335\n",
      "Iteration 1558: loss -1.9306267499923706\n",
      "Iteration 1559: loss -2.074413776397705\n",
      "Iteration 1560: loss -1.903237223625183\n",
      "Iteration 1561: loss -1.949550986289978\n",
      "Iteration 1562: loss -2.0536048412323\n",
      "Iteration 1563: loss -2.0167174339294434\n",
      "Iteration 1564: loss -1.9858523607254028\n",
      "Iteration 1565: loss -2.094531774520874\n",
      "Iteration 1566: loss -2.0628721714019775\n",
      "Iteration 1567: loss -1.9890694618225098\n",
      "Iteration 1568: loss -2.0337722301483154\n",
      "Iteration 1569: loss -2.0395681858062744\n",
      "Iteration 1570: loss -2.004229784011841\n",
      "Iteration 1571: loss -1.895554780960083\n",
      "Iteration 1572: loss -1.8501778841018677\n",
      "Iteration 1573: loss -2.0627031326293945\n",
      "Iteration 1574: loss -1.9963527917861938\n",
      "Iteration 1575: loss -1.9252756834030151\n",
      "Iteration 1576: loss -2.163916826248169\n",
      "Iteration 1577: loss -2.084703207015991\n",
      "Iteration 1578: loss -2.1650304794311523\n",
      "Iteration 1579: loss -1.9348304271697998\n",
      "Iteration 1580: loss -2.012310743331909\n",
      "Iteration 1581: loss -1.880733847618103\n",
      "Iteration 1582: loss -2.0591840744018555\n",
      "Iteration 1583: loss -2.0349583625793457\n",
      "Iteration 1584: loss -1.9837228059768677\n",
      "Iteration 1585: loss -2.019179105758667\n",
      "Iteration 1586: loss -1.966978669166565\n",
      "Iteration 1587: loss -2.1009764671325684\n",
      "Iteration 1588: loss -2.094691753387451\n",
      "Iteration 1589: loss -1.9429742097854614\n",
      "Iteration 1590: loss -2.0019168853759766\n",
      "Iteration 1591: loss -2.0196352005004883\n",
      "Iteration 1592: loss -2.0292880535125732\n",
      "Iteration 1593: loss -2.0266923904418945\n",
      "Iteration 1594: loss -2.00129771232605\n",
      "Iteration 1595: loss -1.9414215087890625\n",
      "Iteration 1596: loss -1.966799259185791\n",
      "Iteration 1597: loss -1.9385579824447632\n",
      "Iteration 1598: loss -1.9938327074050903\n",
      "Iteration 1599: loss -2.0026021003723145\n",
      "Iteration 1600: loss -1.9846477508544922\n",
      "Iteration 1601: loss -2.0415639877319336\n",
      "Iteration 1602: loss -2.100994110107422\n",
      "Iteration 1603: loss -1.8935449123382568\n",
      "Iteration 1604: loss -1.9090818166732788\n",
      "Iteration 1605: loss -2.030266284942627\n",
      "Iteration 1606: loss -1.9046719074249268\n",
      "Iteration 1607: loss -2.1612753868103027\n",
      "Iteration 1608: loss -1.9988433122634888\n",
      "Iteration 1609: loss -2.008432626724243\n",
      "Iteration 1610: loss -2.0400054454803467\n",
      "Iteration 1611: loss -1.974364161491394\n",
      "Iteration 1612: loss -1.8930250406265259\n",
      "Iteration 1613: loss -1.997894525527954\n",
      "Iteration 1614: loss -2.1180553436279297\n",
      "Iteration 1615: loss -2.0287094116210938\n",
      "Iteration 1616: loss -1.9808666706085205\n",
      "Iteration 1617: loss -2.010586738586426\n",
      "Iteration 1618: loss -1.9717150926589966\n",
      "Iteration 1619: loss -1.92422354221344\n",
      "Iteration 1620: loss -2.0376248359680176\n",
      "Iteration 1621: loss -1.9357105493545532\n",
      "Iteration 1622: loss -1.9443061351776123\n",
      "Iteration 1623: loss -1.917556881904602\n",
      "Iteration 1624: loss -2.000704288482666\n",
      "Iteration 1625: loss -1.9837992191314697\n",
      "Iteration 1626: loss -2.1442668437957764\n",
      "Iteration 1627: loss -1.943987488746643\n",
      "Iteration 1628: loss -1.9666258096694946\n",
      "Iteration 1629: loss -1.9644798040390015\n",
      "Iteration 1630: loss -2.0665805339813232\n",
      "Iteration 1631: loss -1.8750616312026978\n",
      "Iteration 1632: loss -2.010984420776367\n",
      "Iteration 1633: loss -1.9599089622497559\n",
      "Iteration 1634: loss -1.945567011833191\n",
      "Iteration 1635: loss -2.068488121032715\n",
      "Iteration 1636: loss -1.9881325960159302\n",
      "Iteration 1637: loss -1.9258904457092285\n",
      "Iteration 1638: loss -2.1221673488616943\n",
      "Iteration 1639: loss -2.07126784324646\n",
      "Iteration 1640: loss -2.0027196407318115\n",
      "Iteration 1641: loss -2.0910234451293945\n",
      "Iteration 1642: loss -1.9640499353408813\n",
      "Iteration 1643: loss -1.9992742538452148\n",
      "Iteration 1644: loss -1.8919590711593628\n",
      "Iteration 1645: loss -1.9964232444763184\n",
      "Iteration 1646: loss -2.084986925125122\n",
      "Iteration 1647: loss -2.0743179321289062\n",
      "Iteration 1648: loss -1.964143991470337\n",
      "Iteration 1649: loss -1.7867993116378784\n",
      "Iteration 1650: loss -2.0836379528045654\n",
      "Iteration 1651: loss -1.958931803703308\n",
      "Iteration 1652: loss -1.8403010368347168\n",
      "Iteration 1653: loss -1.8611913919448853\n",
      "Iteration 1654: loss -2.010141134262085\n",
      "Iteration 1655: loss -2.1672801971435547\n",
      "Iteration 1656: loss -1.990395426750183\n",
      "Iteration 1657: loss -1.898417353630066\n",
      "Iteration 1658: loss -1.9989608526229858\n",
      "Iteration 1659: loss -2.026573896408081\n",
      "Iteration 1660: loss -1.8205392360687256\n",
      "Iteration 1661: loss -2.0522682666778564\n",
      "Iteration 1662: loss -2.130770206451416\n",
      "Iteration 1663: loss -2.038177251815796\n",
      "Iteration 1664: loss -2.0415565967559814\n",
      "Iteration 1665: loss -1.8874744176864624\n",
      "Iteration 1666: loss -2.042931318283081\n",
      "Iteration 1667: loss -2.0075056552886963\n",
      "Iteration 1668: loss -2.0833253860473633\n",
      "Iteration 1669: loss -2.0673816204071045\n",
      "Iteration 1670: loss -1.994809627532959\n",
      "Iteration 1671: loss -1.9189151525497437\n",
      "Iteration 1672: loss -1.9676350355148315\n",
      "Iteration 1673: loss -1.8800256252288818\n",
      "Iteration 1674: loss -1.890417456626892\n",
      "Iteration 1675: loss -1.8587844371795654\n",
      "Iteration 1676: loss -2.022676944732666\n",
      "Iteration 1677: loss -1.9322271347045898\n",
      "Iteration 1678: loss -1.8392449617385864\n",
      "Iteration 1679: loss -2.0572104454040527\n",
      "Iteration 1680: loss -2.0511114597320557\n",
      "Iteration 1681: loss -1.958778977394104\n",
      "Iteration 1682: loss -2.080742120742798\n",
      "Iteration 1683: loss -1.8901176452636719\n",
      "Iteration 1684: loss -1.9734117984771729\n",
      "Iteration 1685: loss -1.9041613340377808\n",
      "Iteration 1686: loss -1.928558588027954\n",
      "Iteration 1687: loss -1.8499505519866943\n",
      "Iteration 1688: loss -1.781463623046875\n",
      "Iteration 1689: loss -1.975251317024231\n",
      "Iteration 1690: loss -1.9704989194869995\n",
      "Iteration 1691: loss -1.866060495376587\n",
      "Iteration 1692: loss -2.0803680419921875\n",
      "Iteration 1693: loss -2.02016282081604\n",
      "Iteration 1694: loss -2.015049934387207\n",
      "Iteration 1695: loss -2.0487990379333496\n",
      "Iteration 1696: loss -2.0471060276031494\n",
      "Iteration 1697: loss -1.7359527349472046\n",
      "Iteration 1698: loss -1.9049218893051147\n",
      "Iteration 1699: loss -2.0363662242889404\n",
      "Iteration 1700: loss -1.9223719835281372\n",
      "Iteration 1701: loss -1.9349234104156494\n",
      "Iteration 1702: loss -2.0838308334350586\n",
      "Iteration 1703: loss -1.9574127197265625\n",
      "Iteration 1704: loss -1.9210948944091797\n",
      "Iteration 1705: loss -1.9902712106704712\n",
      "Iteration 1706: loss -1.8976796865463257\n",
      "Iteration 1707: loss -1.9830220937728882\n",
      "Iteration 1708: loss -1.939767837524414\n",
      "Iteration 1709: loss -2.0642852783203125\n",
      "Iteration 1710: loss -2.0310165882110596\n",
      "Iteration 1711: loss -2.026665687561035\n",
      "Iteration 1712: loss -1.938360571861267\n",
      "Iteration 1713: loss -2.064617872238159\n",
      "Iteration 1714: loss -2.0430638790130615\n",
      "Iteration 1715: loss -2.1154088973999023\n",
      "Iteration 1716: loss -1.907401442527771\n",
      "Iteration 1717: loss -2.0895421504974365\n",
      "Iteration 1718: loss -2.075050115585327\n",
      "Iteration 1719: loss -2.0101699829101562\n",
      "Iteration 1720: loss -1.9457623958587646\n",
      "Iteration 1721: loss -2.0113844871520996\n",
      "Iteration 1722: loss -1.987707495689392\n",
      "Iteration 1723: loss -1.9829469919204712\n",
      "Iteration 1724: loss -1.996549367904663\n",
      "Iteration 1725: loss -2.0217723846435547\n",
      "Iteration 1726: loss -1.9728094339370728\n",
      "Iteration 1727: loss -2.00053334236145\n",
      "Iteration 1728: loss -2.0522470474243164\n",
      "Iteration 1729: loss -2.03546142578125\n",
      "Iteration 1730: loss -2.1229162216186523\n",
      "Iteration 1731: loss -1.8187540769577026\n",
      "Iteration 1732: loss -1.9858559370040894\n",
      "Iteration 1733: loss -2.1615145206451416\n",
      "Iteration 1734: loss -2.1105599403381348\n",
      "Iteration 1735: loss -1.89915931224823\n",
      "Iteration 1736: loss -2.0255062580108643\n",
      "Iteration 1737: loss -2.1283745765686035\n",
      "Iteration 1738: loss -2.040363073348999\n",
      "Iteration 1739: loss -2.0058608055114746\n",
      "Iteration 1740: loss -1.9225902557373047\n",
      "Iteration 1741: loss -2.1227617263793945\n",
      "Iteration 1742: loss -2.0208613872528076\n",
      "Iteration 1743: loss -1.8578455448150635\n",
      "Iteration 1744: loss -2.1355998516082764\n",
      "Iteration 1745: loss -2.115410804748535\n",
      "Iteration 1746: loss -2.0764904022216797\n",
      "Iteration 1747: loss -2.049302577972412\n",
      "Iteration 1748: loss -2.2170398235321045\n",
      "Iteration 1749: loss -2.1695268154144287\n",
      "Iteration 1750: loss -1.9640239477157593\n",
      "Iteration 1751: loss -1.9426779747009277\n",
      "Iteration 1752: loss -1.9911221265792847\n",
      "Iteration 1753: loss -2.0144217014312744\n",
      "Iteration 1754: loss -1.9826244115829468\n",
      "Iteration 1755: loss -2.1837522983551025\n",
      "Iteration 1756: loss -2.0247676372528076\n",
      "Iteration 1757: loss -1.9320945739746094\n",
      "Iteration 1758: loss -2.120901107788086\n",
      "Iteration 1759: loss -2.1287295818328857\n",
      "Iteration 1760: loss -1.8840643167495728\n",
      "Iteration 1761: loss -2.0008630752563477\n",
      "Iteration 1762: loss -2.0732078552246094\n",
      "Iteration 1763: loss -2.0997231006622314\n",
      "Iteration 1764: loss -1.9797273874282837\n",
      "Iteration 1765: loss -2.1536383628845215\n",
      "Iteration 1766: loss -2.0287036895751953\n",
      "Iteration 1767: loss -1.8804214000701904\n",
      "Iteration 1768: loss -1.9255825281143188\n",
      "Iteration 1769: loss -1.9588807821273804\n",
      "Iteration 1770: loss -1.8447134494781494\n",
      "Iteration 1771: loss -1.9545822143554688\n",
      "Iteration 1772: loss -2.162132740020752\n",
      "Iteration 1773: loss -1.8865615129470825\n",
      "Iteration 1774: loss -1.965145230293274\n",
      "Iteration 1775: loss -2.1079530715942383\n",
      "Iteration 1776: loss -2.0650386810302734\n",
      "Iteration 1777: loss -2.0420472621917725\n",
      "Iteration 1778: loss -2.1483678817749023\n",
      "Iteration 1779: loss -1.987784743309021\n",
      "Iteration 1780: loss -2.0022761821746826\n",
      "Iteration 1781: loss -2.15531849861145\n",
      "Iteration 1782: loss -2.145418643951416\n",
      "Iteration 1783: loss -1.985239028930664\n",
      "Iteration 1784: loss -2.1999456882476807\n",
      "Iteration 1785: loss -2.2259459495544434\n",
      "Iteration 1786: loss -1.9131883382797241\n",
      "Iteration 1787: loss -1.9408599138259888\n",
      "Iteration 1788: loss -2.065342664718628\n",
      "Iteration 1789: loss -2.0833842754364014\n",
      "Iteration 1790: loss -2.0421624183654785\n",
      "Iteration 1791: loss -2.1109607219696045\n",
      "Iteration 1792: loss -1.9741030931472778\n",
      "Iteration 1793: loss -2.0846173763275146\n",
      "Iteration 1794: loss -1.9849752187728882\n",
      "Iteration 1795: loss -2.0579659938812256\n",
      "Iteration 1796: loss -1.9183225631713867\n",
      "Iteration 1797: loss -1.962085485458374\n",
      "Iteration 1798: loss -1.9763715267181396\n",
      "Iteration 1799: loss -2.0041098594665527\n",
      "Iteration 1800: loss -1.9478603601455688\n",
      "Iteration 1801: loss -1.9974344968795776\n",
      "Iteration 1802: loss -2.1003050804138184\n",
      "Iteration 1803: loss -2.1375832557678223\n",
      "Iteration 1804: loss -2.0968358516693115\n",
      "Iteration 1805: loss -2.1604433059692383\n",
      "Iteration 1806: loss -1.9953384399414062\n",
      "Iteration 1807: loss -1.8797005414962769\n",
      "Iteration 1808: loss -1.9854120016098022\n",
      "Iteration 1809: loss -1.9007232189178467\n",
      "Iteration 1810: loss -2.0732314586639404\n",
      "Iteration 1811: loss -2.095209836959839\n",
      "Iteration 1812: loss -1.8183969259262085\n",
      "Iteration 1813: loss -2.033475875854492\n",
      "Iteration 1814: loss -2.1168103218078613\n",
      "Iteration 1815: loss -2.245042562484741\n",
      "Iteration 1816: loss -1.9507900476455688\n",
      "Iteration 1817: loss -2.0641438961029053\n",
      "Iteration 1818: loss -1.9212877750396729\n",
      "Iteration 1819: loss -2.043131113052368\n",
      "Iteration 1820: loss -2.1642489433288574\n",
      "Iteration 1821: loss -2.1843080520629883\n",
      "Iteration 1822: loss -1.9561327695846558\n",
      "Iteration 1823: loss -2.079270362854004\n",
      "Iteration 1824: loss -1.9762290716171265\n",
      "Iteration 1825: loss -2.0197513103485107\n",
      "Iteration 1826: loss -1.971720576286316\n",
      "Iteration 1827: loss -2.125157356262207\n",
      "Iteration 1828: loss -2.0077316761016846\n",
      "Iteration 1829: loss -2.1043314933776855\n",
      "Iteration 1830: loss -2.083658456802368\n",
      "Iteration 1831: loss -2.0221476554870605\n",
      "Iteration 1832: loss -2.14787220954895\n",
      "Iteration 1833: loss -2.056786298751831\n",
      "Iteration 1834: loss -1.8598594665527344\n",
      "Iteration 1835: loss -2.0862059593200684\n",
      "Iteration 1836: loss -1.9826363325119019\n",
      "Iteration 1837: loss -2.0155653953552246\n",
      "Iteration 1838: loss -1.997426152229309\n",
      "Iteration 1839: loss -2.101665496826172\n",
      "Iteration 1840: loss -2.0811080932617188\n",
      "Iteration 1841: loss -2.042955160140991\n",
      "Iteration 1842: loss -2.1327388286590576\n",
      "Iteration 1843: loss -2.1747817993164062\n",
      "Iteration 1844: loss -2.016335964202881\n",
      "Iteration 1845: loss -2.055636405944824\n",
      "Iteration 1846: loss -1.8861581087112427\n",
      "Iteration 1847: loss -1.9363453388214111\n",
      "Iteration 1848: loss -2.0389602184295654\n",
      "Iteration 1849: loss -2.162839651107788\n",
      "Iteration 1850: loss -1.981360912322998\n",
      "Iteration 1851: loss -2.0965592861175537\n",
      "Iteration 1852: loss -1.8393421173095703\n",
      "Iteration 1853: loss -2.1116669178009033\n",
      "Iteration 1854: loss -1.9577242136001587\n",
      "Iteration 1855: loss -2.1613106727600098\n",
      "Iteration 1856: loss -2.0306546688079834\n",
      "Iteration 1857: loss -1.885462760925293\n",
      "Iteration 1858: loss -2.072909116744995\n",
      "Iteration 1859: loss -2.0493128299713135\n",
      "Iteration 1860: loss -1.856629729270935\n",
      "Iteration 1861: loss -1.7815220355987549\n",
      "Iteration 1862: loss -1.8434596061706543\n",
      "Iteration 1863: loss -1.882936954498291\n",
      "Iteration 1864: loss -2.0340895652770996\n",
      "Iteration 1865: loss -1.8331536054611206\n",
      "Iteration 1866: loss -1.9578112363815308\n",
      "Iteration 1867: loss -1.9272149801254272\n",
      "Iteration 1868: loss -1.998017430305481\n",
      "Iteration 1869: loss -2.050572156906128\n",
      "Iteration 1870: loss -2.015528917312622\n",
      "Iteration 1871: loss -1.9086283445358276\n",
      "Iteration 1872: loss -1.8041958808898926\n",
      "Iteration 1873: loss -1.9199559688568115\n",
      "Iteration 1874: loss -2.1010780334472656\n",
      "Iteration 1875: loss -1.8952134847640991\n",
      "Iteration 1876: loss -2.004490375518799\n",
      "Iteration 1877: loss -1.8776668310165405\n",
      "Iteration 1878: loss -1.9180883169174194\n",
      "Iteration 1879: loss -2.0032615661621094\n",
      "Iteration 1880: loss -2.094334840774536\n",
      "Iteration 1881: loss -1.9320214986801147\n",
      "Iteration 1882: loss -1.8542003631591797\n",
      "Iteration 1883: loss -1.8459663391113281\n",
      "Iteration 1884: loss -2.010359764099121\n",
      "Iteration 1885: loss -2.001582622528076\n",
      "Iteration 1886: loss -2.1717162132263184\n",
      "Iteration 1887: loss -1.957568883895874\n",
      "Iteration 1888: loss -1.9335771799087524\n",
      "Iteration 1889: loss -2.1111643314361572\n",
      "Iteration 1890: loss -2.0920181274414062\n",
      "Iteration 1891: loss -2.0881080627441406\n",
      "Iteration 1892: loss -2.010223150253296\n",
      "Iteration 1893: loss -2.1048529148101807\n",
      "Iteration 1894: loss -1.9833159446716309\n",
      "Iteration 1895: loss -2.0183539390563965\n",
      "Iteration 1896: loss -2.00004243850708\n",
      "Iteration 1897: loss -1.9681899547576904\n",
      "Iteration 1898: loss -2.0172882080078125\n",
      "Iteration 1899: loss -2.1545207500457764\n",
      "Iteration 1900: loss -1.9251166582107544\n",
      "Iteration 1901: loss -2.0228233337402344\n",
      "Iteration 1902: loss -2.00706148147583\n",
      "Iteration 1903: loss -2.1150639057159424\n",
      "Iteration 1904: loss -2.100989818572998\n",
      "Iteration 1905: loss -2.234379768371582\n",
      "Iteration 1906: loss -2.0353665351867676\n",
      "Iteration 1907: loss -1.9863742589950562\n",
      "Iteration 1908: loss -2.074882984161377\n",
      "Iteration 1909: loss -1.948099136352539\n",
      "Iteration 1910: loss -2.1098484992980957\n",
      "Iteration 1911: loss -1.964956283569336\n",
      "Iteration 1912: loss -1.9876046180725098\n",
      "Iteration 1913: loss -2.0205230712890625\n",
      "Iteration 1914: loss -2.0864837169647217\n",
      "Iteration 1915: loss -1.8613985776901245\n",
      "Iteration 1916: loss -2.1472201347351074\n",
      "Iteration 1917: loss -1.932930827140808\n",
      "Iteration 1918: loss -2.0641305446624756\n",
      "Iteration 1919: loss -2.1274478435516357\n",
      "Iteration 1920: loss -1.9315078258514404\n",
      "Iteration 1921: loss -2.0115718841552734\n",
      "Iteration 1922: loss -2.1171700954437256\n",
      "Iteration 1923: loss -2.0330147743225098\n",
      "Iteration 1924: loss -2.064091682434082\n",
      "Iteration 1925: loss -1.9489796161651611\n",
      "Iteration 1926: loss -1.8340481519699097\n",
      "Iteration 1927: loss -1.8990505933761597\n",
      "Iteration 1928: loss -2.0803301334381104\n",
      "Iteration 1929: loss -1.9902642965316772\n",
      "Iteration 1930: loss -1.9381033182144165\n",
      "Iteration 1931: loss -1.7350972890853882\n",
      "Iteration 1932: loss -2.042412281036377\n",
      "Iteration 1933: loss -1.8621305227279663\n",
      "Iteration 1934: loss -2.050785541534424\n",
      "Iteration 1935: loss -2.1782054901123047\n",
      "Iteration 1936: loss -2.0872247219085693\n",
      "Iteration 1937: loss -1.9663240909576416\n",
      "Iteration 1938: loss -2.0538995265960693\n",
      "Iteration 1939: loss -2.0461173057556152\n",
      "Iteration 1940: loss -1.9103732109069824\n",
      "Iteration 1941: loss -1.9003450870513916\n",
      "Iteration 1942: loss -1.9083508253097534\n",
      "Iteration 1943: loss -1.9222567081451416\n",
      "Iteration 1944: loss -2.0555403232574463\n",
      "Iteration 1945: loss -2.07151460647583\n",
      "Iteration 1946: loss -2.0123038291931152\n",
      "Iteration 1947: loss -2.0814554691314697\n",
      "Iteration 1948: loss -2.0472514629364014\n",
      "Iteration 1949: loss -2.0351638793945312\n",
      "Iteration 1950: loss -2.0781643390655518\n",
      "Iteration 1951: loss -1.87940514087677\n",
      "Iteration 1952: loss -2.0239686965942383\n",
      "Iteration 1953: loss -1.951934576034546\n",
      "Iteration 1954: loss -1.8504451513290405\n",
      "Iteration 1955: loss -2.0378944873809814\n",
      "Iteration 1956: loss -1.9771822690963745\n",
      "Iteration 1957: loss -2.2164573669433594\n",
      "Iteration 1958: loss -1.9467843770980835\n",
      "Iteration 1959: loss -2.1155800819396973\n",
      "Iteration 1960: loss -2.013247013092041\n",
      "Iteration 1961: loss -2.0587871074676514\n",
      "Iteration 1962: loss -2.202354907989502\n",
      "Iteration 1963: loss -2.149038076400757\n",
      "Iteration 1964: loss -1.9139289855957031\n",
      "Iteration 1965: loss -1.976637840270996\n",
      "Iteration 1966: loss -2.0614895820617676\n",
      "Iteration 1967: loss -2.0234525203704834\n",
      "Iteration 1968: loss -2.228828191757202\n",
      "Iteration 1969: loss -2.032334566116333\n",
      "Iteration 1970: loss -1.9050664901733398\n",
      "Iteration 1971: loss -1.9671707153320312\n",
      "Iteration 1972: loss -2.052030563354492\n",
      "Iteration 1973: loss -2.097071409225464\n",
      "Iteration 1974: loss -1.9612336158752441\n",
      "Iteration 1975: loss -2.0295968055725098\n",
      "Iteration 1976: loss -2.0194144248962402\n",
      "Iteration 1977: loss -2.067682981491089\n",
      "Iteration 1978: loss -2.013059616088867\n",
      "Iteration 1979: loss -2.046804666519165\n",
      "Iteration 1980: loss -2.1234426498413086\n",
      "Iteration 1981: loss -2.0359294414520264\n",
      "Iteration 1982: loss -2.0784623622894287\n",
      "Iteration 1983: loss -1.9297443628311157\n",
      "Iteration 1984: loss -1.9153019189834595\n",
      "Iteration 1985: loss -1.9421520233154297\n",
      "Iteration 1986: loss -1.750215768814087\n",
      "Iteration 1987: loss -2.0155086517333984\n",
      "Iteration 1988: loss -1.9713349342346191\n",
      "Iteration 1989: loss -1.739620327949524\n",
      "Iteration 1990: loss -1.9023486375808716\n",
      "Iteration 1991: loss -2.0555715560913086\n",
      "Iteration 1992: loss -1.979378342628479\n",
      "Iteration 1993: loss -1.9575164318084717\n",
      "Iteration 1994: loss -2.1062872409820557\n",
      "Iteration 1995: loss -1.958004355430603\n",
      "Iteration 1996: loss -1.8949731588363647\n",
      "Iteration 1997: loss -2.0093793869018555\n",
      "Iteration 1998: loss -2.1039164066314697\n",
      "Iteration 1999: loss -1.9421918392181396\n",
      "Iteration 2000: loss -2.0593695640563965\n",
      "Iteration 2001: loss -2.013639450073242\n",
      "Iteration 2002: loss -2.000270366668701\n",
      "Iteration 2003: loss -2.106977939605713\n",
      "Iteration 2004: loss -1.9402602910995483\n",
      "Iteration 2005: loss -1.946065068244934\n",
      "Iteration 2006: loss -1.9705228805541992\n",
      "Iteration 2007: loss -1.9991792440414429\n",
      "Iteration 2008: loss -2.029634475708008\n",
      "Iteration 2009: loss -1.983178973197937\n",
      "Iteration 2010: loss -2.0131096839904785\n",
      "Iteration 2011: loss -2.0413076877593994\n",
      "Iteration 2012: loss -2.2475743293762207\n",
      "Iteration 2013: loss -2.0229527950286865\n",
      "Iteration 2014: loss -1.999491572380066\n",
      "Iteration 2015: loss -1.9583086967468262\n",
      "Iteration 2016: loss -2.0744214057922363\n",
      "Iteration 2017: loss -2.1179420948028564\n",
      "Iteration 2018: loss -2.0929219722747803\n",
      "Iteration 2019: loss -2.034045696258545\n",
      "Iteration 2020: loss -1.995991826057434\n",
      "Iteration 2021: loss -2.110180377960205\n",
      "Iteration 2022: loss -2.1855251789093018\n",
      "Iteration 2023: loss -1.989302635192871\n",
      "Iteration 2024: loss -1.9713941812515259\n",
      "Iteration 2025: loss -2.0363869667053223\n",
      "Iteration 2026: loss -1.866193175315857\n",
      "Iteration 2027: loss -2.0900089740753174\n",
      "Iteration 2028: loss -2.026878833770752\n",
      "Iteration 2029: loss -2.1013829708099365\n",
      "Iteration 2030: loss -2.072815418243408\n",
      "Iteration 2031: loss -1.8032869100570679\n",
      "Iteration 2032: loss -1.9336358308792114\n",
      "Iteration 2033: loss -1.872439980506897\n",
      "Iteration 2034: loss -1.9289575815200806\n",
      "Iteration 2035: loss -2.1303110122680664\n",
      "Iteration 2036: loss -1.6664620637893677\n",
      "Iteration 2037: loss -1.9296624660491943\n",
      "Iteration 2038: loss -2.0237131118774414\n",
      "Iteration 2039: loss -1.8568215370178223\n",
      "Iteration 2040: loss -2.1152822971343994\n",
      "Iteration 2041: loss -2.0737709999084473\n",
      "Iteration 2042: loss -1.9531841278076172\n",
      "Iteration 2043: loss -1.95274817943573\n",
      "Iteration 2044: loss -1.8777209520339966\n",
      "Iteration 2045: loss -1.9638622999191284\n",
      "Iteration 2046: loss -1.7071982622146606\n",
      "Iteration 2047: loss -1.9918395280838013\n",
      "Iteration 2048: loss -2.0685112476348877\n",
      "Iteration 2049: loss -2.116532564163208\n",
      "Iteration 2050: loss -1.893654704093933\n",
      "Iteration 2051: loss -1.9408185482025146\n",
      "Iteration 2052: loss -1.9048802852630615\n",
      "Iteration 2053: loss -1.998515009880066\n",
      "Iteration 2054: loss -2.123380422592163\n",
      "Iteration 2055: loss -2.026998281478882\n",
      "Iteration 2056: loss -1.8972960710525513\n",
      "Iteration 2057: loss -1.8631536960601807\n",
      "Iteration 2058: loss -1.9327373504638672\n",
      "Iteration 2059: loss -2.015707015991211\n",
      "Iteration 2060: loss -2.130272626876831\n",
      "Iteration 2061: loss -2.0395944118499756\n",
      "Iteration 2062: loss -1.956375241279602\n",
      "Iteration 2063: loss -2.0054101943969727\n",
      "Iteration 2064: loss -1.910674810409546\n",
      "Iteration 2065: loss -1.9545841217041016\n",
      "Iteration 2066: loss -2.113246202468872\n",
      "Iteration 2067: loss -1.7536779642105103\n",
      "Iteration 2068: loss -1.9628695249557495\n",
      "Iteration 2069: loss -1.9690181016921997\n",
      "Iteration 2070: loss -1.7290905714035034\n",
      "Iteration 2071: loss -2.085071086883545\n",
      "Iteration 2072: loss -2.025113105773926\n",
      "Iteration 2073: loss -2.0170443058013916\n",
      "Iteration 2074: loss -1.781745433807373\n",
      "Iteration 2075: loss -1.997636318206787\n",
      "Iteration 2076: loss -1.9274325370788574\n",
      "Iteration 2077: loss -1.838934302330017\n",
      "Iteration 2078: loss -1.96797513961792\n",
      "Iteration 2079: loss -2.0749831199645996\n",
      "Iteration 2080: loss -1.905476689338684\n",
      "Iteration 2081: loss -1.8884049654006958\n",
      "Iteration 2082: loss -2.0555717945098877\n",
      "Iteration 2083: loss -2.096940040588379\n",
      "Iteration 2084: loss -1.9009337425231934\n",
      "Iteration 2085: loss -1.9483416080474854\n",
      "Iteration 2086: loss -2.0902926921844482\n",
      "Iteration 2087: loss -2.071063995361328\n",
      "Iteration 2088: loss -2.0006673336029053\n",
      "Iteration 2089: loss -2.021604061126709\n",
      "Iteration 2090: loss -2.0382373332977295\n",
      "Iteration 2091: loss -1.7609100341796875\n",
      "Iteration 2092: loss -1.8790556192398071\n",
      "Iteration 2093: loss -1.790120244026184\n",
      "Iteration 2094: loss -1.9064157009124756\n",
      "Iteration 2095: loss -2.1012778282165527\n",
      "Iteration 2096: loss -1.9669009447097778\n",
      "Iteration 2097: loss -1.9940694570541382\n",
      "Iteration 2098: loss -1.7340943813323975\n",
      "Iteration 2099: loss -2.2093911170959473\n",
      "Iteration 2100: loss -2.059089422225952\n",
      "Iteration 2101: loss -1.864593505859375\n",
      "Iteration 2102: loss -2.1067726612091064\n",
      "Iteration 2103: loss -2.024627685546875\n",
      "Iteration 2104: loss -1.9938403367996216\n",
      "Iteration 2105: loss -1.94020676612854\n",
      "Iteration 2106: loss -2.113182306289673\n",
      "Iteration 2107: loss -2.009549856185913\n",
      "Iteration 2108: loss -2.0009963512420654\n",
      "Iteration 2109: loss -2.055783271789551\n",
      "Iteration 2110: loss -2.1748037338256836\n",
      "Iteration 2111: loss -2.023103713989258\n",
      "Iteration 2112: loss -2.0181145668029785\n",
      "Iteration 2113: loss -2.0854268074035645\n",
      "Iteration 2114: loss -2.1218669414520264\n",
      "Iteration 2115: loss -2.0746278762817383\n",
      "Iteration 2116: loss -1.9800846576690674\n",
      "Iteration 2117: loss -2.1417551040649414\n",
      "Iteration 2118: loss -2.0201096534729004\n",
      "Iteration 2119: loss -2.0445778369903564\n",
      "Iteration 2120: loss -2.173793315887451\n",
      "Iteration 2121: loss -2.185368061065674\n",
      "Iteration 2122: loss -2.0828640460968018\n",
      "Iteration 2123: loss -1.9487249851226807\n",
      "Iteration 2124: loss -2.0268664360046387\n",
      "Iteration 2125: loss -2.0904297828674316\n",
      "Iteration 2126: loss -1.9863054752349854\n",
      "Iteration 2127: loss -1.8467659950256348\n",
      "Iteration 2128: loss -2.0169970989227295\n",
      "Iteration 2129: loss -2.1208603382110596\n",
      "Iteration 2130: loss -2.055116891860962\n",
      "Iteration 2131: loss -2.0997467041015625\n",
      "Iteration 2132: loss -2.0961265563964844\n",
      "Iteration 2133: loss -2.098579168319702\n",
      "Iteration 2134: loss -2.042261838912964\n",
      "Iteration 2135: loss -2.1042325496673584\n",
      "Iteration 2136: loss -1.9139056205749512\n",
      "Iteration 2137: loss -2.131631851196289\n",
      "Iteration 2138: loss -2.1158533096313477\n",
      "Iteration 2139: loss -1.929051160812378\n",
      "Iteration 2140: loss -1.9918456077575684\n",
      "Iteration 2141: loss -2.063652992248535\n",
      "Iteration 2142: loss -2.0305840969085693\n",
      "Iteration 2143: loss -2.039085865020752\n",
      "Iteration 2144: loss -1.720357060432434\n",
      "Iteration 2145: loss -1.9487611055374146\n",
      "Iteration 2146: loss -1.9941915273666382\n",
      "Iteration 2147: loss -2.1052260398864746\n",
      "Iteration 2148: loss -1.7510497570037842\n",
      "Iteration 2149: loss -2.174572467803955\n",
      "Iteration 2150: loss -1.9108153581619263\n",
      "Iteration 2151: loss -2.053448438644409\n",
      "Iteration 2152: loss -1.7921663522720337\n",
      "Iteration 2153: loss -1.9800317287445068\n",
      "Iteration 2154: loss -2.060580015182495\n",
      "Iteration 2155: loss -2.086836099624634\n",
      "Iteration 2156: loss -2.091846227645874\n",
      "Iteration 2157: loss -1.9223986864089966\n",
      "Iteration 2158: loss -1.9325916767120361\n",
      "Iteration 2159: loss -2.1518375873565674\n",
      "Iteration 2160: loss -2.1003916263580322\n",
      "Iteration 2161: loss -2.010788917541504\n",
      "Iteration 2162: loss -2.170159339904785\n",
      "Iteration 2163: loss -1.9934991598129272\n",
      "Iteration 2164: loss -2.148362398147583\n",
      "Iteration 2165: loss -2.1495611667633057\n",
      "Iteration 2166: loss -2.013739585876465\n",
      "Iteration 2167: loss -2.0116121768951416\n",
      "Iteration 2168: loss -2.0074503421783447\n",
      "Iteration 2169: loss -2.0881664752960205\n",
      "Iteration 2170: loss -2.0587210655212402\n",
      "Iteration 2171: loss -1.9549803733825684\n",
      "Iteration 2172: loss -1.9916595220565796\n",
      "Iteration 2173: loss -1.894838809967041\n",
      "Iteration 2174: loss -1.9898003339767456\n",
      "Iteration 2175: loss -1.8990442752838135\n",
      "Iteration 2176: loss -2.0587563514709473\n",
      "Iteration 2177: loss -1.888444423675537\n",
      "Iteration 2178: loss -1.971681833267212\n",
      "Iteration 2179: loss -2.134382963180542\n",
      "Iteration 2180: loss -1.889766812324524\n",
      "Iteration 2181: loss -2.078035831451416\n",
      "Iteration 2182: loss -2.082444667816162\n",
      "Iteration 2183: loss -2.135788917541504\n",
      "Iteration 2184: loss -1.945396065711975\n",
      "Iteration 2185: loss -2.076115131378174\n",
      "Iteration 2186: loss -1.9186893701553345\n",
      "Iteration 2187: loss -1.9356242418289185\n",
      "Iteration 2188: loss -2.1696150302886963\n",
      "Iteration 2189: loss -2.1291520595550537\n",
      "Iteration 2190: loss -2.0666301250457764\n",
      "Iteration 2191: loss -2.176600217819214\n",
      "Iteration 2192: loss -1.9281307458877563\n",
      "Iteration 2193: loss -2.0902650356292725\n",
      "Iteration 2194: loss -2.029292583465576\n",
      "Iteration 2195: loss -2.0065829753875732\n",
      "Iteration 2196: loss -1.8713114261627197\n",
      "Iteration 2197: loss -1.7695693969726562\n",
      "Iteration 2198: loss -2.082869052886963\n",
      "Iteration 2199: loss -1.8747990131378174\n",
      "Iteration 2200: loss -1.8457788228988647\n",
      "Iteration 2201: loss -2.1158864498138428\n",
      "Iteration 2202: loss -2.023653745651245\n",
      "Iteration 2203: loss -1.9518990516662598\n",
      "Iteration 2204: loss -2.0035905838012695\n",
      "Iteration 2205: loss -2.0812759399414062\n",
      "Iteration 2206: loss -1.8950408697128296\n",
      "Iteration 2207: loss -2.0406079292297363\n",
      "Iteration 2208: loss -2.0252339839935303\n",
      "Iteration 2209: loss -1.9685958623886108\n",
      "Iteration 2210: loss -2.075207233428955\n",
      "Iteration 2211: loss -1.8958953619003296\n",
      "Iteration 2212: loss -1.8765288591384888\n",
      "Iteration 2213: loss -2.1331534385681152\n",
      "Iteration 2214: loss -1.7773972749710083\n",
      "Iteration 2215: loss -2.000243902206421\n",
      "Iteration 2216: loss -2.0189754962921143\n",
      "Iteration 2217: loss -1.9500762224197388\n",
      "Iteration 2218: loss -2.079058885574341\n",
      "Iteration 2219: loss -1.9419677257537842\n",
      "Iteration 2220: loss -2.03369402885437\n",
      "Iteration 2221: loss -2.107727289199829\n",
      "Iteration 2222: loss -2.026120185852051\n",
      "Iteration 2223: loss -2.1466963291168213\n",
      "Iteration 2224: loss -2.060056686401367\n",
      "Iteration 2225: loss -1.9613380432128906\n",
      "Iteration 2226: loss -2.052870035171509\n",
      "Iteration 2227: loss -1.964021921157837\n",
      "Iteration 2228: loss -2.0769124031066895\n",
      "Iteration 2229: loss -2.1046316623687744\n",
      "Iteration 2230: loss -2.0591275691986084\n",
      "Iteration 2231: loss -2.0334413051605225\n",
      "Iteration 2232: loss -2.102792978286743\n",
      "Iteration 2233: loss -2.0890047550201416\n",
      "Iteration 2234: loss -1.8836098909378052\n",
      "Iteration 2235: loss -2.122748374938965\n",
      "Iteration 2236: loss -1.8643548488616943\n",
      "Iteration 2237: loss -2.021662950515747\n",
      "Iteration 2238: loss -2.025498390197754\n",
      "Iteration 2239: loss -2.0448050498962402\n",
      "Iteration 2240: loss -2.176727056503296\n",
      "Iteration 2241: loss -1.9224419593811035\n",
      "Iteration 2242: loss -2.0004806518554688\n",
      "Iteration 2243: loss -1.8251818418502808\n",
      "Iteration 2244: loss -2.027569055557251\n",
      "Iteration 2245: loss -1.9787392616271973\n",
      "Iteration 2246: loss -2.1145284175872803\n",
      "Iteration 2247: loss -2.1275269985198975\n",
      "Iteration 2248: loss -1.9434716701507568\n",
      "Iteration 2249: loss -1.9535033702850342\n",
      "Iteration 2250: loss -2.0342979431152344\n",
      "Iteration 2251: loss -2.0543391704559326\n",
      "Iteration 2252: loss -2.0631659030914307\n",
      "Iteration 2253: loss -2.157184362411499\n",
      "Iteration 2254: loss -2.0847907066345215\n",
      "Iteration 2255: loss -1.9780023097991943\n",
      "Iteration 2256: loss -2.019573211669922\n",
      "Iteration 2257: loss -2.1364591121673584\n",
      "Iteration 2258: loss -2.2101891040802\n",
      "Iteration 2259: loss -2.0599114894866943\n",
      "Iteration 2260: loss -2.0402026176452637\n",
      "Iteration 2261: loss -1.9126601219177246\n",
      "Iteration 2262: loss -2.2059566974639893\n",
      "Iteration 2263: loss -2.056598424911499\n",
      "Iteration 2264: loss -2.038918972015381\n",
      "Iteration 2265: loss -1.985211730003357\n",
      "Iteration 2266: loss -1.8809714317321777\n",
      "Iteration 2267: loss -1.8996731042861938\n",
      "Iteration 2268: loss -2.1307904720306396\n",
      "Iteration 2269: loss -2.1406610012054443\n",
      "Iteration 2270: loss -1.8046177625656128\n",
      "Iteration 2271: loss -1.9552018642425537\n",
      "Iteration 2272: loss -1.8783553838729858\n",
      "Iteration 2273: loss -2.0366482734680176\n",
      "Iteration 2274: loss -1.9328234195709229\n",
      "Iteration 2275: loss -2.0878024101257324\n",
      "Iteration 2276: loss -2.120126962661743\n",
      "Iteration 2277: loss -2.0494651794433594\n",
      "Iteration 2278: loss -2.152031898498535\n",
      "Iteration 2279: loss -2.1387505531311035\n",
      "Iteration 2280: loss -2.038996934890747\n",
      "Iteration 2281: loss -1.8060024976730347\n",
      "Iteration 2282: loss -2.1266400814056396\n",
      "Iteration 2283: loss -2.017357349395752\n",
      "Iteration 2284: loss -2.000765085220337\n",
      "Iteration 2285: loss -2.1571505069732666\n",
      "Iteration 2286: loss -1.9951741695404053\n",
      "Iteration 2287: loss -2.139129638671875\n",
      "Iteration 2288: loss -1.9807780981063843\n",
      "Iteration 2289: loss -2.048535108566284\n",
      "Iteration 2290: loss -1.9844063520431519\n",
      "Iteration 2291: loss -1.9170080423355103\n",
      "Iteration 2292: loss -1.9518117904663086\n",
      "Iteration 2293: loss -2.031633138656616\n",
      "Iteration 2294: loss -1.9743493795394897\n",
      "Iteration 2295: loss -2.1008100509643555\n",
      "Iteration 2296: loss -1.9014687538146973\n",
      "Iteration 2297: loss -2.0762763023376465\n",
      "Iteration 2298: loss -2.0968639850616455\n",
      "Iteration 2299: loss -1.9796351194381714\n",
      "Iteration 2300: loss -2.2453715801239014\n",
      "Iteration 2301: loss -2.0861566066741943\n",
      "Iteration 2302: loss -2.153878688812256\n",
      "Iteration 2303: loss -1.9632155895233154\n",
      "Iteration 2304: loss -2.047985076904297\n",
      "Iteration 2305: loss -2.1338913440704346\n",
      "Iteration 2306: loss -2.1105473041534424\n",
      "Iteration 2307: loss -2.1718239784240723\n",
      "Iteration 2308: loss -1.9229607582092285\n",
      "Iteration 2309: loss -1.8309367895126343\n",
      "Iteration 2310: loss -2.0954861640930176\n",
      "Iteration 2311: loss -2.0223286151885986\n",
      "Iteration 2312: loss -1.8821096420288086\n",
      "Iteration 2313: loss -1.943381905555725\n",
      "Iteration 2314: loss -1.9639203548431396\n",
      "Iteration 2315: loss -2.0798044204711914\n",
      "Iteration 2316: loss -2.0495529174804688\n",
      "Iteration 2317: loss -2.0762691497802734\n",
      "Iteration 2318: loss -1.8144387006759644\n",
      "Iteration 2319: loss -2.0636961460113525\n",
      "Iteration 2320: loss -1.969254732131958\n",
      "Iteration 2321: loss -2.0244739055633545\n",
      "Iteration 2322: loss -1.9520530700683594\n",
      "Iteration 2323: loss -2.0486950874328613\n",
      "Iteration 2324: loss -2.142936944961548\n",
      "Iteration 2325: loss -2.0449271202087402\n",
      "Iteration 2326: loss -1.973799705505371\n",
      "Iteration 2327: loss -1.8691630363464355\n",
      "Iteration 2328: loss -2.1462574005126953\n",
      "Iteration 2329: loss -1.9591981172561646\n",
      "Iteration 2330: loss -2.0618584156036377\n",
      "Iteration 2331: loss -2.0514299869537354\n",
      "Iteration 2332: loss -1.8753849267959595\n",
      "Iteration 2333: loss -2.007343292236328\n",
      "Iteration 2334: loss -2.0273396968841553\n",
      "Iteration 2335: loss -2.089566707611084\n",
      "Iteration 2336: loss -2.1160728931427\n",
      "Iteration 2337: loss -1.920060634613037\n",
      "Iteration 2338: loss -2.103691577911377\n",
      "Iteration 2339: loss -1.9192533493041992\n",
      "Iteration 2340: loss -2.126924753189087\n",
      "Iteration 2341: loss -2.0817630290985107\n",
      "Iteration 2342: loss -1.9679930210113525\n",
      "Iteration 2343: loss -2.0422298908233643\n",
      "Iteration 2344: loss -2.1185877323150635\n",
      "Iteration 2345: loss -1.9392485618591309\n",
      "Iteration 2346: loss -2.0258443355560303\n",
      "Iteration 2347: loss -2.1199769973754883\n",
      "Iteration 2348: loss -1.872022032737732\n",
      "Iteration 2349: loss -1.9515151977539062\n",
      "Iteration 2350: loss -1.9102518558502197\n",
      "Iteration 2351: loss -1.9331482648849487\n",
      "Iteration 2352: loss -1.9824126958847046\n",
      "Iteration 2353: loss -1.9000344276428223\n",
      "Iteration 2354: loss -2.128649950027466\n",
      "Iteration 2355: loss -1.9762789011001587\n",
      "Iteration 2356: loss -2.2217319011688232\n",
      "Iteration 2357: loss -1.9710012674331665\n",
      "Iteration 2358: loss -1.9710224866867065\n",
      "Iteration 2359: loss -1.864107608795166\n",
      "Iteration 2360: loss -2.1215384006500244\n",
      "Iteration 2361: loss -1.9101660251617432\n",
      "Iteration 2362: loss -1.711814045906067\n",
      "Iteration 2363: loss -1.8744131326675415\n",
      "Iteration 2364: loss -1.937058687210083\n",
      "Iteration 2365: loss -1.742631435394287\n",
      "Iteration 2366: loss -2.088890552520752\n",
      "Iteration 2367: loss -2.0343141555786133\n",
      "Iteration 2368: loss -1.880027174949646\n",
      "Iteration 2369: loss -1.9397094249725342\n",
      "Iteration 2370: loss -1.9568301439285278\n",
      "Iteration 2371: loss -1.9549282789230347\n",
      "Iteration 2372: loss -1.853763222694397\n",
      "Iteration 2373: loss -2.0134286880493164\n",
      "Iteration 2374: loss -1.9678335189819336\n",
      "Iteration 2375: loss -1.9653855562210083\n",
      "Iteration 2376: loss -1.9073487520217896\n",
      "Iteration 2377: loss -2.109790563583374\n",
      "Iteration 2378: loss -1.9448280334472656\n",
      "Iteration 2379: loss -1.995004653930664\n",
      "Iteration 2380: loss -1.9910918474197388\n",
      "Iteration 2381: loss -2.051597833633423\n",
      "Iteration 2382: loss -2.0740907192230225\n",
      "Iteration 2383: loss -2.022580862045288\n",
      "Iteration 2384: loss -2.0856571197509766\n",
      "Iteration 2385: loss -2.1150171756744385\n",
      "Iteration 2386: loss -2.079416275024414\n",
      "Iteration 2387: loss -1.8999778032302856\n",
      "Iteration 2388: loss -1.9942318201065063\n",
      "Iteration 2389: loss -1.9578438997268677\n",
      "Iteration 2390: loss -1.9940894842147827\n",
      "Iteration 2391: loss -2.0660812854766846\n",
      "Iteration 2392: loss -2.0352728366851807\n",
      "Iteration 2393: loss -1.9349199533462524\n",
      "Iteration 2394: loss -2.0484447479248047\n",
      "Iteration 2395: loss -1.9681941270828247\n",
      "Iteration 2396: loss -1.958823561668396\n",
      "Iteration 2397: loss -1.9962056875228882\n",
      "Iteration 2398: loss -2.0538127422332764\n",
      "Iteration 2399: loss -2.147040605545044\n",
      "Iteration 2400: loss -2.037168025970459\n",
      "Iteration 2401: loss -2.0524890422821045\n",
      "Iteration 2402: loss -2.0692031383514404\n",
      "Iteration 2403: loss -2.102109909057617\n",
      "Iteration 2404: loss -2.0899219512939453\n",
      "Iteration 2405: loss -2.140198230743408\n",
      "Iteration 2406: loss -2.1656596660614014\n",
      "Iteration 2407: loss -2.1002590656280518\n",
      "Iteration 2408: loss -1.9153497219085693\n",
      "Iteration 2409: loss -1.8621206283569336\n",
      "Iteration 2410: loss -2.053842782974243\n",
      "Iteration 2411: loss -1.9843993186950684\n",
      "Iteration 2412: loss -2.039898157119751\n",
      "Iteration 2413: loss -1.990332007408142\n",
      "Iteration 2414: loss -1.8985971212387085\n",
      "Iteration 2415: loss -1.890297770500183\n",
      "Iteration 2416: loss -2.0107524394989014\n",
      "Iteration 2417: loss -1.9230077266693115\n",
      "Iteration 2418: loss -1.913136601448059\n",
      "Iteration 2419: loss -2.042205810546875\n",
      "Iteration 2420: loss -2.1944520473480225\n",
      "Iteration 2421: loss -1.9440898895263672\n",
      "Iteration 2422: loss -2.0234131813049316\n",
      "Iteration 2423: loss -1.9749683141708374\n",
      "Iteration 2424: loss -1.848925232887268\n",
      "Iteration 2425: loss -1.9656846523284912\n",
      "Iteration 2426: loss -2.073946714401245\n",
      "Iteration 2427: loss -2.0743069648742676\n",
      "Iteration 2428: loss -2.012078285217285\n",
      "Iteration 2429: loss -2.2252442836761475\n",
      "Iteration 2430: loss -2.137653350830078\n",
      "Iteration 2431: loss -1.902815818786621\n",
      "Iteration 2432: loss -2.0825772285461426\n",
      "Iteration 2433: loss -2.00126314163208\n",
      "Iteration 2434: loss -1.992401361465454\n",
      "Iteration 2435: loss -2.1053407192230225\n",
      "Iteration 2436: loss -2.092637538909912\n",
      "Iteration 2437: loss -2.0289065837860107\n",
      "Iteration 2438: loss -2.0858685970306396\n",
      "Iteration 2439: loss -2.134220838546753\n",
      "Iteration 2440: loss -2.1583988666534424\n",
      "Iteration 2441: loss -2.033517360687256\n",
      "Iteration 2442: loss -2.0211780071258545\n",
      "Iteration 2443: loss -2.0337204933166504\n",
      "Iteration 2444: loss -2.103424310684204\n",
      "Iteration 2445: loss -2.0540599822998047\n",
      "Iteration 2446: loss -1.9766265153884888\n",
      "Iteration 2447: loss -2.141380786895752\n",
      "Iteration 2448: loss -2.0651698112487793\n",
      "Iteration 2449: loss -2.081284999847412\n",
      "Iteration 2450: loss -2.115680456161499\n",
      "Iteration 2451: loss -1.9869424104690552\n",
      "Iteration 2452: loss -2.144273519515991\n",
      "Iteration 2453: loss -2.0833423137664795\n",
      "Iteration 2454: loss -2.240535020828247\n",
      "Iteration 2455: loss -2.163205146789551\n",
      "Iteration 2456: loss -2.068622350692749\n",
      "Iteration 2457: loss -2.0043540000915527\n",
      "Iteration 2458: loss -1.8924094438552856\n",
      "Iteration 2459: loss -2.085063934326172\n",
      "Iteration 2460: loss -2.1172726154327393\n",
      "Iteration 2461: loss -2.049431800842285\n",
      "Iteration 2462: loss -2.0590176582336426\n",
      "Iteration 2463: loss -2.0951156616210938\n",
      "Iteration 2464: loss -2.1409411430358887\n",
      "Iteration 2465: loss -2.063774585723877\n",
      "Iteration 2466: loss -2.0239579677581787\n",
      "Iteration 2467: loss -2.0882163047790527\n",
      "Iteration 2468: loss -2.146470785140991\n",
      "Iteration 2469: loss -2.071516275405884\n",
      "Iteration 2470: loss -1.964449405670166\n",
      "Iteration 2471: loss -2.1294708251953125\n",
      "Iteration 2472: loss -2.0774717330932617\n",
      "Iteration 2473: loss -2.0942795276641846\n",
      "Iteration 2474: loss -2.246044635772705\n",
      "Iteration 2475: loss -1.9215295314788818\n",
      "Iteration 2476: loss -2.0065500736236572\n",
      "Iteration 2477: loss -2.0387282371520996\n",
      "Iteration 2478: loss -1.8074184656143188\n",
      "Iteration 2479: loss -1.8902508020401\n",
      "Iteration 2480: loss -1.999880075454712\n",
      "Iteration 2481: loss -2.0241410732269287\n",
      "Iteration 2482: loss -1.902174949645996\n",
      "Iteration 2483: loss -1.99960196018219\n",
      "Iteration 2484: loss -1.9730970859527588\n",
      "Iteration 2485: loss -1.950181484222412\n",
      "Iteration 2486: loss -2.071934223175049\n",
      "Iteration 2487: loss -2.079028367996216\n",
      "Iteration 2488: loss -1.939700722694397\n",
      "Iteration 2489: loss -2.0493571758270264\n",
      "Iteration 2490: loss -1.9717713594436646\n",
      "Iteration 2491: loss -2.0460145473480225\n",
      "Iteration 2492: loss -1.9856685400009155\n",
      "Iteration 2493: loss -2.01143741607666\n",
      "Iteration 2494: loss -2.112978219985962\n",
      "Iteration 2495: loss -1.9076879024505615\n",
      "Iteration 2496: loss -1.9950194358825684\n",
      "Iteration 2497: loss -2.0524487495422363\n",
      "Iteration 2498: loss -2.017394781112671\n",
      "Iteration 2499: loss -1.9522497653961182\n",
      "Iteration 2500: loss -1.8931865692138672\n",
      "Iteration 2501: loss -1.8152974843978882\n",
      "Iteration 2502: loss -2.082362174987793\n",
      "Iteration 2503: loss -2.058781147003174\n",
      "Iteration 2504: loss -1.786489725112915\n",
      "Iteration 2505: loss -1.8351187705993652\n",
      "Iteration 2506: loss -2.1638433933258057\n",
      "Iteration 2507: loss -1.923964500427246\n",
      "Iteration 2508: loss -2.0639398097991943\n",
      "Iteration 2509: loss -2.040865898132324\n",
      "Iteration 2510: loss -1.916986346244812\n",
      "Iteration 2511: loss -1.920715570449829\n",
      "Iteration 2512: loss -2.091404676437378\n",
      "Iteration 2513: loss -2.071320056915283\n",
      "Iteration 2514: loss -2.0885655879974365\n",
      "Iteration 2515: loss -1.9560023546218872\n",
      "Iteration 2516: loss -2.1171276569366455\n",
      "Iteration 2517: loss -2.1030378341674805\n",
      "Iteration 2518: loss -2.003148078918457\n",
      "Iteration 2519: loss -1.9898830652236938\n",
      "Iteration 2520: loss -2.164609432220459\n",
      "Iteration 2521: loss -2.080559253692627\n",
      "Iteration 2522: loss -1.9941344261169434\n",
      "Iteration 2523: loss -1.9898492097854614\n",
      "Iteration 2524: loss -2.1517677307128906\n",
      "Iteration 2525: loss -2.129838228225708\n",
      "Iteration 2526: loss -2.031538963317871\n",
      "Iteration 2527: loss -2.0698606967926025\n",
      "Iteration 2528: loss -2.1333084106445312\n",
      "Iteration 2529: loss -2.210177183151245\n",
      "Iteration 2530: loss -2.0841174125671387\n",
      "Iteration 2531: loss -2.115312337875366\n",
      "Iteration 2532: loss -2.089289426803589\n",
      "Iteration 2533: loss -2.0135743618011475\n",
      "Iteration 2534: loss -2.0291478633880615\n",
      "Iteration 2535: loss -1.8536652326583862\n",
      "Iteration 2536: loss -2.103302001953125\n",
      "Iteration 2537: loss -2.0988881587982178\n",
      "Iteration 2538: loss -2.0597290992736816\n",
      "Iteration 2539: loss -2.12819766998291\n",
      "Iteration 2540: loss -2.0421769618988037\n",
      "Iteration 2541: loss -1.8736944198608398\n",
      "Iteration 2542: loss -2.052401065826416\n",
      "Iteration 2543: loss -2.0015411376953125\n",
      "Iteration 2544: loss -1.9051934480667114\n",
      "Iteration 2545: loss -2.1150615215301514\n",
      "Iteration 2546: loss -2.0349230766296387\n",
      "Iteration 2547: loss -1.9725924730300903\n",
      "Iteration 2548: loss -1.8970943689346313\n",
      "Iteration 2549: loss -2.0124614238739014\n",
      "Iteration 2550: loss -2.0564868450164795\n",
      "Iteration 2551: loss -2.048670768737793\n",
      "Iteration 2552: loss -2.153242349624634\n",
      "Iteration 2553: loss -1.9551787376403809\n",
      "Iteration 2554: loss -2.0030510425567627\n",
      "Iteration 2555: loss -1.9169052839279175\n",
      "Iteration 2556: loss -2.0453972816467285\n",
      "Iteration 2557: loss -1.970633864402771\n",
      "Iteration 2558: loss -1.8839114904403687\n",
      "Iteration 2559: loss -2.0907769203186035\n",
      "Iteration 2560: loss -2.012529134750366\n",
      "Iteration 2561: loss -2.0358808040618896\n",
      "Iteration 2562: loss -2.0685391426086426\n",
      "Iteration 2563: loss -2.0362629890441895\n",
      "Iteration 2564: loss -2.0424227714538574\n",
      "Iteration 2565: loss -2.0861737728118896\n",
      "Iteration 2566: loss -1.9506034851074219\n",
      "Iteration 2567: loss -1.802280068397522\n",
      "Iteration 2568: loss -2.0602872371673584\n",
      "Iteration 2569: loss -2.238170623779297\n",
      "Iteration 2570: loss -1.9448952674865723\n",
      "Iteration 2571: loss -2.021547317504883\n",
      "Iteration 2572: loss -1.9114080667495728\n",
      "Iteration 2573: loss -1.9858226776123047\n",
      "Iteration 2574: loss -1.9982188940048218\n",
      "Iteration 2575: loss -1.9999477863311768\n",
      "Iteration 2576: loss -2.099752187728882\n",
      "Iteration 2577: loss -1.8358204364776611\n",
      "Iteration 2578: loss -1.9858392477035522\n",
      "Iteration 2579: loss -2.1540002822875977\n",
      "Iteration 2580: loss -1.9514275789260864\n",
      "Iteration 2581: loss -1.9407153129577637\n",
      "Iteration 2582: loss -1.9947832822799683\n",
      "Iteration 2583: loss -2.0053818225860596\n",
      "Iteration 2584: loss -2.0200304985046387\n",
      "Iteration 2585: loss -2.000441312789917\n",
      "Iteration 2586: loss -2.01528000831604\n",
      "Iteration 2587: loss -1.829083800315857\n",
      "Iteration 2588: loss -2.146320343017578\n",
      "Iteration 2589: loss -2.0585014820098877\n",
      "Iteration 2590: loss -1.958415150642395\n",
      "Iteration 2591: loss -1.9466460943222046\n",
      "Iteration 2592: loss -1.884002685546875\n",
      "Iteration 2593: loss -2.0070180892944336\n",
      "Iteration 2594: loss -1.808174729347229\n",
      "Iteration 2595: loss -2.0462749004364014\n",
      "Iteration 2596: loss -2.1057345867156982\n",
      "Iteration 2597: loss -2.083836793899536\n",
      "Iteration 2598: loss -2.009734869003296\n",
      "Iteration 2599: loss -2.037261486053467\n",
      "Iteration 2600: loss -2.003147840499878\n",
      "Iteration 2601: loss -2.0933220386505127\n",
      "Iteration 2602: loss -2.0303189754486084\n",
      "Iteration 2603: loss -2.064849615097046\n",
      "Iteration 2604: loss -2.092456340789795\n",
      "Iteration 2605: loss -1.9673216342926025\n",
      "Iteration 2606: loss -2.1896955966949463\n",
      "Iteration 2607: loss -2.1527891159057617\n",
      "Iteration 2608: loss -2.0456109046936035\n",
      "Iteration 2609: loss -2.0462417602539062\n",
      "Iteration 2610: loss -2.000938653945923\n",
      "Iteration 2611: loss -2.14522385597229\n",
      "Iteration 2612: loss -1.8122503757476807\n",
      "Iteration 2613: loss -2.081904649734497\n",
      "Iteration 2614: loss -2.1829299926757812\n",
      "Iteration 2615: loss -2.173215389251709\n",
      "Iteration 2616: loss -2.1886966228485107\n",
      "Iteration 2617: loss -2.110246181488037\n",
      "Iteration 2618: loss -1.979370355606079\n",
      "Iteration 2619: loss -1.9708788394927979\n",
      "Iteration 2620: loss -1.9793293476104736\n",
      "Iteration 2621: loss -2.120009422302246\n",
      "Iteration 2622: loss -1.8867168426513672\n",
      "Iteration 2623: loss -2.082582712173462\n",
      "Iteration 2624: loss -2.1828043460845947\n",
      "Iteration 2625: loss -2.0152316093444824\n",
      "Iteration 2626: loss -2.0344085693359375\n",
      "Iteration 2627: loss -2.1812570095062256\n",
      "Iteration 2628: loss -2.158233642578125\n",
      "Iteration 2629: loss -2.0487825870513916\n",
      "Iteration 2630: loss -2.1224260330200195\n",
      "Iteration 2631: loss -1.9790968894958496\n",
      "Iteration 2632: loss -1.8254048824310303\n",
      "Iteration 2633: loss -2.1436285972595215\n",
      "Iteration 2634: loss -2.0811383724212646\n",
      "Iteration 2635: loss -2.018758535385132\n",
      "Iteration 2636: loss -2.000750780105591\n",
      "Iteration 2637: loss -2.0200183391571045\n",
      "Iteration 2638: loss -2.0476269721984863\n",
      "Iteration 2639: loss -2.186267614364624\n",
      "Iteration 2640: loss -2.112985372543335\n",
      "Iteration 2641: loss -2.003012180328369\n",
      "Iteration 2642: loss -2.082385540008545\n",
      "Iteration 2643: loss -2.0449059009552\n",
      "Iteration 2644: loss -1.8326821327209473\n",
      "Iteration 2645: loss -2.035999059677124\n",
      "Iteration 2646: loss -2.1077542304992676\n",
      "Iteration 2647: loss -1.8940130472183228\n",
      "Iteration 2648: loss -2.0089242458343506\n",
      "Iteration 2649: loss -2.108596086502075\n",
      "Iteration 2650: loss -1.9743016958236694\n",
      "Iteration 2651: loss -2.2515976428985596\n",
      "Iteration 2652: loss -2.1223480701446533\n",
      "Iteration 2653: loss -2.1214258670806885\n",
      "Iteration 2654: loss -2.0241239070892334\n",
      "Iteration 2655: loss -2.103691577911377\n",
      "Iteration 2656: loss -2.079698085784912\n",
      "Iteration 2657: loss -2.154876708984375\n",
      "Iteration 2658: loss -2.0206844806671143\n",
      "Iteration 2659: loss -2.2143166065216064\n",
      "Iteration 2660: loss -1.9754269123077393\n",
      "Iteration 2661: loss -2.0029473304748535\n",
      "Iteration 2662: loss -1.91487455368042\n",
      "Iteration 2663: loss -2.0338284969329834\n",
      "Iteration 2664: loss -2.1047451496124268\n",
      "Iteration 2665: loss -1.8274203538894653\n",
      "Iteration 2666: loss -2.0488407611846924\n",
      "Iteration 2667: loss -2.127260446548462\n",
      "Iteration 2668: loss -1.8934365510940552\n",
      "Iteration 2669: loss -1.9070708751678467\n",
      "Iteration 2670: loss -1.957175374031067\n",
      "Iteration 2671: loss -2.006502866744995\n",
      "Iteration 2672: loss -1.841025710105896\n",
      "Iteration 2673: loss -2.1196248531341553\n",
      "Iteration 2674: loss -1.9774774312973022\n",
      "Iteration 2675: loss -2.099396228790283\n",
      "Iteration 2676: loss -1.9154284000396729\n",
      "Iteration 2677: loss -2.071030855178833\n",
      "Iteration 2678: loss -2.097198963165283\n",
      "Iteration 2679: loss -2.1513211727142334\n",
      "Iteration 2680: loss -2.098524332046509\n",
      "Iteration 2681: loss -2.0845208168029785\n",
      "Iteration 2682: loss -2.0148708820343018\n",
      "Iteration 2683: loss -2.0491726398468018\n",
      "Iteration 2684: loss -2.0899107456207275\n",
      "Iteration 2685: loss -2.048828125\n",
      "Iteration 2686: loss -1.9366490840911865\n",
      "Iteration 2687: loss -1.9824423789978027\n",
      "Iteration 2688: loss -1.9603899717330933\n",
      "Iteration 2689: loss -2.052272081375122\n",
      "Iteration 2690: loss -2.1689281463623047\n",
      "Iteration 2691: loss -2.170621871948242\n",
      "Iteration 2692: loss -2.09187912940979\n",
      "Iteration 2693: loss -2.1091854572296143\n",
      "Iteration 2694: loss -1.9994230270385742\n",
      "Iteration 2695: loss -2.0629849433898926\n",
      "Iteration 2696: loss -2.1331801414489746\n",
      "Iteration 2697: loss -2.0596072673797607\n",
      "Iteration 2698: loss -1.8827416896820068\n",
      "Iteration 2699: loss -2.000676155090332\n",
      "Iteration 2700: loss -2.4060165882110596\n",
      "Iteration 2701: loss -2.051274299621582\n",
      "Iteration 2702: loss -2.0686147212982178\n",
      "Iteration 2703: loss -2.0797510147094727\n",
      "Iteration 2704: loss -1.8182367086410522\n",
      "Iteration 2705: loss -2.0850276947021484\n",
      "Iteration 2706: loss -2.0908141136169434\n",
      "Iteration 2707: loss -2.0945639610290527\n",
      "Iteration 2708: loss -2.1733789443969727\n",
      "Iteration 2709: loss -2.101426362991333\n",
      "Iteration 2710: loss -2.09723162651062\n",
      "Iteration 2711: loss -1.9782525300979614\n",
      "Iteration 2712: loss -1.9748622179031372\n",
      "Iteration 2713: loss -2.077084541320801\n",
      "Iteration 2714: loss -2.0214340686798096\n",
      "Iteration 2715: loss -2.1405415534973145\n",
      "Iteration 2716: loss -2.0628106594085693\n",
      "Iteration 2717: loss -2.140594244003296\n",
      "Iteration 2718: loss -2.185079574584961\n",
      "Iteration 2719: loss -2.0507969856262207\n",
      "Iteration 2720: loss -1.9833604097366333\n",
      "Iteration 2721: loss -1.8856561183929443\n",
      "Iteration 2722: loss -2.105661153793335\n",
      "Iteration 2723: loss -2.0484578609466553\n",
      "Iteration 2724: loss -2.1488194465637207\n",
      "Iteration 2725: loss -2.1036970615386963\n",
      "Iteration 2726: loss -2.0919008255004883\n",
      "Iteration 2727: loss -2.064020872116089\n",
      "Iteration 2728: loss -2.1983070373535156\n",
      "Iteration 2729: loss -1.9639681577682495\n",
      "Iteration 2730: loss -2.0249240398406982\n",
      "Iteration 2731: loss -2.0521271228790283\n",
      "Iteration 2732: loss -2.05045485496521\n",
      "Iteration 2733: loss -2.0612471103668213\n",
      "Iteration 2734: loss -2.0951144695281982\n",
      "Iteration 2735: loss -2.1250805854797363\n",
      "Iteration 2736: loss -2.0984914302825928\n",
      "Iteration 2737: loss -2.203068733215332\n",
      "Iteration 2738: loss -2.117379665374756\n",
      "Iteration 2739: loss -2.087024688720703\n",
      "Iteration 2740: loss -2.035907030105591\n",
      "Iteration 2741: loss -2.0169568061828613\n",
      "Iteration 2742: loss -2.184319019317627\n",
      "Iteration 2743: loss -2.097170829772949\n",
      "Iteration 2744: loss -2.0794780254364014\n",
      "Iteration 2745: loss -2.0158517360687256\n",
      "Iteration 2746: loss -1.9454699754714966\n",
      "Iteration 2747: loss -2.0481386184692383\n",
      "Iteration 2748: loss -2.12166428565979\n",
      "Iteration 2749: loss -2.1098005771636963\n",
      "Iteration 2750: loss -2.253007173538208\n",
      "Iteration 2751: loss -2.136765718460083\n",
      "Iteration 2752: loss -1.9428619146347046\n",
      "Iteration 2753: loss -1.9955925941467285\n",
      "Iteration 2754: loss -2.0093934535980225\n",
      "Iteration 2755: loss -2.0931003093719482\n",
      "Iteration 2756: loss -2.055325984954834\n",
      "Iteration 2757: loss -2.0810797214508057\n",
      "Iteration 2758: loss -2.1078763008117676\n",
      "Iteration 2759: loss -2.189934253692627\n",
      "Iteration 2760: loss -2.181546926498413\n",
      "Iteration 2761: loss -2.0497887134552\n",
      "Iteration 2762: loss -1.999451994895935\n",
      "Iteration 2763: loss -2.02392578125\n",
      "Iteration 2764: loss -2.073709011077881\n",
      "Iteration 2765: loss -2.018548011779785\n",
      "Iteration 2766: loss -2.0794520378112793\n",
      "Iteration 2767: loss -2.1456820964813232\n",
      "Iteration 2768: loss -1.8747069835662842\n",
      "Iteration 2769: loss -2.163686990737915\n",
      "Iteration 2770: loss -2.0597457885742188\n",
      "Iteration 2771: loss -1.8798497915267944\n",
      "Iteration 2772: loss -2.13554048538208\n",
      "Iteration 2773: loss -2.099930763244629\n",
      "Iteration 2774: loss -2.0664889812469482\n",
      "Iteration 2775: loss -1.8193453550338745\n",
      "Iteration 2776: loss -1.9277981519699097\n",
      "Iteration 2777: loss -2.04526948928833\n",
      "Iteration 2778: loss -1.8720378875732422\n",
      "Iteration 2779: loss -1.8544000387191772\n",
      "Iteration 2780: loss -2.0938544273376465\n",
      "Iteration 2781: loss -2.003826856613159\n",
      "Iteration 2782: loss -1.8562802076339722\n",
      "Iteration 2783: loss -2.2121899127960205\n",
      "Iteration 2784: loss -1.8397040367126465\n",
      "Iteration 2785: loss -1.995784878730774\n",
      "Iteration 2786: loss -1.9382565021514893\n",
      "Iteration 2787: loss -2.0055055618286133\n",
      "Iteration 2788: loss -2.1004114151000977\n",
      "Iteration 2789: loss -1.9314547777175903\n",
      "Iteration 2790: loss -2.198939561843872\n",
      "Iteration 2791: loss -2.0557138919830322\n",
      "Iteration 2792: loss -2.117750883102417\n",
      "Iteration 2793: loss -2.01670503616333\n",
      "Iteration 2794: loss -2.1968612670898438\n",
      "Iteration 2795: loss -1.9488067626953125\n",
      "Iteration 2796: loss -2.041806697845459\n",
      "Iteration 2797: loss -2.044567584991455\n",
      "Iteration 2798: loss -2.067836284637451\n",
      "Iteration 2799: loss -2.027923822402954\n",
      "Iteration 2800: loss -1.9772683382034302\n",
      "Iteration 2801: loss -2.0491130352020264\n",
      "Iteration 2802: loss -2.1951913833618164\n",
      "Iteration 2803: loss -2.0346481800079346\n",
      "Iteration 2804: loss -2.066237211227417\n",
      "Iteration 2805: loss -2.1492748260498047\n",
      "Iteration 2806: loss -2.094303846359253\n",
      "Iteration 2807: loss -2.0111944675445557\n",
      "Iteration 2808: loss -2.1537070274353027\n",
      "Iteration 2809: loss -2.1658895015716553\n",
      "Iteration 2810: loss -2.0487804412841797\n",
      "Iteration 2811: loss -2.134148120880127\n",
      "Iteration 2812: loss -2.140310525894165\n",
      "Iteration 2813: loss -2.1412482261657715\n",
      "Iteration 2814: loss -2.184807300567627\n",
      "Iteration 2815: loss -2.0532829761505127\n",
      "Iteration 2816: loss -1.8815265893936157\n",
      "Iteration 2817: loss -2.1319310665130615\n",
      "Iteration 2818: loss -2.0061275959014893\n",
      "Iteration 2819: loss -2.0124435424804688\n",
      "Iteration 2820: loss -1.9832006692886353\n",
      "Iteration 2821: loss -2.0595004558563232\n",
      "Iteration 2822: loss -2.16184139251709\n",
      "Iteration 2823: loss -2.083418130874634\n",
      "Iteration 2824: loss -2.084042549133301\n",
      "Iteration 2825: loss -1.8601429462432861\n",
      "Iteration 2826: loss -2.0745484828948975\n",
      "Iteration 2827: loss -2.125105142593384\n",
      "Iteration 2828: loss -2.0196754932403564\n",
      "Iteration 2829: loss -2.1899547576904297\n",
      "Iteration 2830: loss -1.8360590934753418\n",
      "Iteration 2831: loss -2.0900893211364746\n",
      "Iteration 2832: loss -1.9759212732315063\n",
      "Iteration 2833: loss -2.0454580783843994\n",
      "Iteration 2834: loss -2.054664373397827\n",
      "Iteration 2835: loss -2.1583406925201416\n",
      "Iteration 2836: loss -2.0587193965911865\n",
      "Iteration 2837: loss -2.0392911434173584\n",
      "Iteration 2838: loss -2.1742260456085205\n",
      "Iteration 2839: loss -1.8902517557144165\n",
      "Iteration 2840: loss -2.00180983543396\n",
      "Iteration 2841: loss -2.01066255569458\n",
      "Iteration 2842: loss -2.105628728866577\n",
      "Iteration 2843: loss -1.9741853475570679\n",
      "Iteration 2844: loss -2.1892611980438232\n",
      "Iteration 2845: loss -2.1054089069366455\n",
      "Iteration 2846: loss -1.976139783859253\n",
      "Iteration 2847: loss -1.9523893594741821\n",
      "Iteration 2848: loss -1.9310964345932007\n",
      "Iteration 2849: loss -2.0407755374908447\n",
      "Iteration 2850: loss -2.175739049911499\n",
      "Iteration 2851: loss -2.1175103187561035\n",
      "Iteration 2852: loss -2.2133474349975586\n",
      "Iteration 2853: loss -1.9384814500808716\n",
      "Iteration 2854: loss -2.1583354473114014\n",
      "Iteration 2855: loss -2.155362367630005\n",
      "Iteration 2856: loss -2.0596158504486084\n",
      "Iteration 2857: loss -2.211336374282837\n",
      "Iteration 2858: loss -2.1138510704040527\n",
      "Iteration 2859: loss -2.1691501140594482\n",
      "Iteration 2860: loss -2.1383955478668213\n",
      "Iteration 2861: loss -2.0628623962402344\n",
      "Iteration 2862: loss -2.2376699447631836\n",
      "Iteration 2863: loss -1.927812099456787\n",
      "Iteration 2864: loss -2.0431113243103027\n",
      "Iteration 2865: loss -1.9360625743865967\n",
      "Iteration 2866: loss -2.2132513523101807\n",
      "Iteration 2867: loss -1.8861452341079712\n",
      "Iteration 2868: loss -1.9549816846847534\n",
      "Iteration 2869: loss -2.0863358974456787\n",
      "Iteration 2870: loss -2.181002616882324\n",
      "Iteration 2871: loss -2.167066812515259\n",
      "Iteration 2872: loss -1.929220199584961\n",
      "Iteration 2873: loss -1.9883733987808228\n",
      "Iteration 2874: loss -2.0781753063201904\n",
      "Iteration 2875: loss -2.171128273010254\n",
      "Iteration 2876: loss -2.0884861946105957\n",
      "Iteration 2877: loss -2.17244291305542\n",
      "Iteration 2878: loss -2.1597049236297607\n",
      "Iteration 2879: loss -1.9926007986068726\n",
      "Iteration 2880: loss -2.0857138633728027\n",
      "Iteration 2881: loss -2.2148141860961914\n",
      "Iteration 2882: loss -2.129673719406128\n",
      "Iteration 2883: loss -2.113051652908325\n",
      "Iteration 2884: loss -2.1082828044891357\n",
      "Iteration 2885: loss -2.164254903793335\n",
      "Iteration 2886: loss -2.189495086669922\n",
      "Iteration 2887: loss -1.8875281810760498\n",
      "Iteration 2888: loss -2.193925619125366\n",
      "Iteration 2889: loss -2.11578369140625\n",
      "Iteration 2890: loss -2.1660077571868896\n",
      "Iteration 2891: loss -2.061724901199341\n",
      "Iteration 2892: loss -2.05604887008667\n",
      "Iteration 2893: loss -2.108919620513916\n",
      "Iteration 2894: loss -2.0236525535583496\n",
      "Iteration 2895: loss -1.9090347290039062\n",
      "Iteration 2896: loss -2.038332223892212\n",
      "Iteration 2897: loss -2.111175537109375\n",
      "Iteration 2898: loss -1.9184163808822632\n",
      "Iteration 2899: loss -2.0542969703674316\n",
      "Iteration 2900: loss -2.144618511199951\n",
      "Iteration 2901: loss -1.8898464441299438\n",
      "Iteration 2902: loss -1.9121630191802979\n",
      "Iteration 2903: loss -1.8755251169204712\n",
      "Iteration 2904: loss -2.1064112186431885\n",
      "Iteration 2905: loss -2.0180881023406982\n",
      "Iteration 2906: loss -1.7635762691497803\n",
      "Iteration 2907: loss -2.0336055755615234\n",
      "Iteration 2908: loss -1.956844449043274\n",
      "Iteration 2909: loss -2.007202625274658\n",
      "Iteration 2910: loss -2.1625137329101562\n",
      "Iteration 2911: loss -2.1184799671173096\n",
      "Iteration 2912: loss -2.103729724884033\n",
      "Iteration 2913: loss -2.188382625579834\n",
      "Iteration 2914: loss -2.2611916065216064\n",
      "Iteration 2915: loss -2.216343402862549\n",
      "Iteration 2916: loss -2.0948870182037354\n",
      "Iteration 2917: loss -2.1050353050231934\n",
      "Iteration 2918: loss -2.00934100151062\n",
      "Iteration 2919: loss -2.2340869903564453\n",
      "Iteration 2920: loss -1.836165428161621\n",
      "Iteration 2921: loss -2.1166181564331055\n",
      "Iteration 2922: loss -2.108095645904541\n",
      "Iteration 2923: loss -2.1016845703125\n",
      "Iteration 2924: loss -2.130915403366089\n",
      "Iteration 2925: loss -2.183195114135742\n",
      "Iteration 2926: loss -2.1737308502197266\n",
      "Iteration 2927: loss -1.8986421823501587\n",
      "Iteration 2928: loss -2.157827138900757\n",
      "Iteration 2929: loss -2.023392677307129\n",
      "Iteration 2930: loss -1.9963237047195435\n",
      "Iteration 2931: loss -2.138559579849243\n",
      "Iteration 2932: loss -2.0490975379943848\n",
      "Iteration 2933: loss -1.9900944232940674\n",
      "Iteration 2934: loss -2.223580837249756\n",
      "Iteration 2935: loss -2.116281270980835\n",
      "Iteration 2936: loss -2.0011203289031982\n",
      "Iteration 2937: loss -2.1359899044036865\n",
      "Iteration 2938: loss -2.142620086669922\n",
      "Iteration 2939: loss -1.8129470348358154\n",
      "Iteration 2940: loss -2.0880658626556396\n",
      "Iteration 2941: loss -2.0052525997161865\n",
      "Iteration 2942: loss -2.0333731174468994\n",
      "Iteration 2943: loss -1.9471116065979004\n",
      "Iteration 2944: loss -2.002040386199951\n",
      "Iteration 2945: loss -2.141627788543701\n",
      "Iteration 2946: loss -2.2213950157165527\n",
      "Iteration 2947: loss -2.109586715698242\n",
      "Iteration 2948: loss -2.037682056427002\n",
      "Iteration 2949: loss -2.1012673377990723\n",
      "Iteration 2950: loss -2.1786632537841797\n",
      "Iteration 2951: loss -2.1232454776763916\n",
      "Iteration 2952: loss -2.108618974685669\n",
      "Iteration 2953: loss -2.0124571323394775\n",
      "Iteration 2954: loss -2.102790594100952\n",
      "Iteration 2955: loss -2.1227927207946777\n",
      "Iteration 2956: loss -2.0630810260772705\n",
      "Iteration 2957: loss -2.144181728363037\n",
      "Iteration 2958: loss -2.0614840984344482\n",
      "Iteration 2959: loss -2.08201265335083\n",
      "Iteration 2960: loss -2.156914234161377\n",
      "Iteration 2961: loss -2.200869083404541\n",
      "Iteration 2962: loss -2.0851972103118896\n",
      "Iteration 2963: loss -2.146028757095337\n",
      "Iteration 2964: loss -2.043121814727783\n",
      "Iteration 2965: loss -2.0451183319091797\n",
      "Iteration 2966: loss -2.097759246826172\n",
      "Iteration 2967: loss -2.0745303630828857\n",
      "Iteration 2968: loss -2.026787042617798\n",
      "Iteration 2969: loss -2.242767572402954\n",
      "Iteration 2970: loss -2.037111520767212\n",
      "Iteration 2971: loss -1.9064855575561523\n",
      "Iteration 2972: loss -2.208954334259033\n",
      "Iteration 2973: loss -2.149369716644287\n",
      "Iteration 2974: loss -2.0292482376098633\n",
      "Iteration 2975: loss -2.101675033569336\n",
      "Iteration 2976: loss -1.8659534454345703\n",
      "Iteration 2977: loss -2.078308582305908\n",
      "Iteration 2978: loss -1.9346871376037598\n",
      "Iteration 2979: loss -1.9269479513168335\n",
      "Iteration 2980: loss -2.0270073413848877\n",
      "Iteration 2981: loss -1.9921517372131348\n",
      "Iteration 2982: loss -1.870453119277954\n",
      "Iteration 2983: loss -1.9595763683319092\n",
      "Iteration 2984: loss -2.0395004749298096\n",
      "Iteration 2985: loss -2.18351411819458\n",
      "Iteration 2986: loss -2.0006260871887207\n",
      "Iteration 2987: loss -2.012389898300171\n",
      "Iteration 2988: loss -2.0822908878326416\n",
      "Iteration 2989: loss -2.028285503387451\n",
      "Iteration 2990: loss -1.8632975816726685\n",
      "Iteration 2991: loss -2.0593419075012207\n",
      "Iteration 2992: loss -2.083681344985962\n",
      "Iteration 2993: loss -2.048063278198242\n",
      "Iteration 2994: loss -2.1687796115875244\n",
      "Iteration 2995: loss -2.034430503845215\n",
      "Iteration 2996: loss -2.1151716709136963\n",
      "Iteration 2997: loss -2.1456663608551025\n",
      "Iteration 2998: loss -2.1807823181152344\n",
      "Iteration 2999: loss -2.105376958847046\n",
      "Iteration 3000: loss -2.0699281692504883\n",
      "Iteration 3001: loss -2.116861581802368\n",
      "Iteration 3002: loss -2.0300679206848145\n",
      "Iteration 3003: loss -1.9523080587387085\n",
      "Iteration 3004: loss -2.128955125808716\n",
      "Iteration 3005: loss -2.1827385425567627\n",
      "Iteration 3006: loss -2.1906394958496094\n",
      "Iteration 3007: loss -2.058779239654541\n",
      "Iteration 3008: loss -2.1082632541656494\n",
      "Iteration 3009: loss -1.9416335821151733\n",
      "Iteration 3010: loss -2.2161669731140137\n",
      "Iteration 3011: loss -2.01393985748291\n",
      "Iteration 3012: loss -2.0126535892486572\n",
      "Iteration 3013: loss -2.0186595916748047\n",
      "Iteration 3014: loss -1.9827007055282593\n",
      "Iteration 3015: loss -2.2065067291259766\n",
      "Iteration 3016: loss -1.972058653831482\n",
      "Iteration 3017: loss -2.144318103790283\n",
      "Iteration 3018: loss -2.1267666816711426\n",
      "Iteration 3019: loss -2.033674478530884\n",
      "Iteration 3020: loss -2.0888609886169434\n",
      "Iteration 3021: loss -1.891487956047058\n",
      "Iteration 3022: loss -2.117933511734009\n",
      "Iteration 3023: loss -1.9728997945785522\n",
      "Iteration 3024: loss -2.0540263652801514\n",
      "Iteration 3025: loss -2.1682052612304688\n",
      "Iteration 3026: loss -2.139744281768799\n",
      "Iteration 3027: loss -2.001570463180542\n",
      "Iteration 3028: loss -1.8899307250976562\n",
      "Iteration 3029: loss -2.072144031524658\n",
      "Iteration 3030: loss -2.124735116958618\n",
      "Iteration 3031: loss -2.112053871154785\n",
      "Iteration 3032: loss -2.2313857078552246\n",
      "Iteration 3033: loss -2.0366427898406982\n",
      "Iteration 3034: loss -2.1513419151306152\n",
      "Iteration 3035: loss -2.017698049545288\n",
      "Iteration 3036: loss -2.072910785675049\n",
      "Iteration 3037: loss -2.041107416152954\n",
      "Iteration 3038: loss -2.129664182662964\n",
      "Iteration 3039: loss -2.0395331382751465\n",
      "Iteration 3040: loss -2.029597282409668\n",
      "Iteration 3041: loss -2.2476084232330322\n",
      "Iteration 3042: loss -2.0460808277130127\n",
      "Iteration 3043: loss -2.1648824214935303\n",
      "Iteration 3044: loss -2.1952104568481445\n",
      "Iteration 3045: loss -1.9842251539230347\n",
      "Iteration 3046: loss -2.069833517074585\n",
      "Iteration 3047: loss -2.0066261291503906\n",
      "Iteration 3048: loss -2.1831014156341553\n",
      "Iteration 3049: loss -2.2346603870391846\n",
      "Iteration 3050: loss -2.0410516262054443\n",
      "Iteration 3051: loss -2.266097068786621\n",
      "Iteration 3052: loss -1.9155516624450684\n",
      "Iteration 3053: loss -2.05812931060791\n",
      "Iteration 3054: loss -2.0207767486572266\n",
      "Iteration 3055: loss -2.0155606269836426\n",
      "Iteration 3056: loss -2.0782971382141113\n",
      "Iteration 3057: loss -1.8893204927444458\n",
      "Iteration 3058: loss -2.0376412868499756\n",
      "Iteration 3059: loss -2.0863351821899414\n",
      "Iteration 3060: loss -2.064122200012207\n",
      "Iteration 3061: loss -2.1241071224212646\n",
      "Iteration 3062: loss -2.0781006813049316\n",
      "Iteration 3063: loss -1.8977117538452148\n",
      "Iteration 3064: loss -2.0258913040161133\n",
      "Iteration 3065: loss -2.159318208694458\n",
      "Iteration 3066: loss -2.0293030738830566\n",
      "Iteration 3067: loss -2.0315027236938477\n",
      "Iteration 3068: loss -2.0350732803344727\n",
      "Iteration 3069: loss -2.243682622909546\n",
      "Iteration 3070: loss -2.1592013835906982\n",
      "Iteration 3071: loss -2.225407123565674\n",
      "Iteration 3072: loss -2.1814799308776855\n",
      "Iteration 3073: loss -2.1086061000823975\n",
      "Iteration 3074: loss -2.062467098236084\n",
      "Iteration 3075: loss -2.00083327293396\n",
      "Iteration 3076: loss -2.0058584213256836\n",
      "Iteration 3077: loss -2.1616291999816895\n",
      "Iteration 3078: loss -2.0043883323669434\n",
      "Iteration 3079: loss -2.2433621883392334\n",
      "Iteration 3080: loss -1.98281991481781\n",
      "Iteration 3081: loss -2.140368938446045\n",
      "Iteration 3082: loss -2.135021924972534\n",
      "Iteration 3083: loss -2.1452653408050537\n",
      "Iteration 3084: loss -1.984090805053711\n",
      "Iteration 3085: loss -2.101410388946533\n",
      "Iteration 3086: loss -1.9880627393722534\n",
      "Iteration 3087: loss -1.9755873680114746\n",
      "Iteration 3088: loss -2.0772337913513184\n",
      "Iteration 3089: loss -2.0558135509490967\n",
      "Iteration 3090: loss -1.9507070779800415\n",
      "Iteration 3091: loss -2.110579252243042\n",
      "Iteration 3092: loss -1.891103744506836\n",
      "Iteration 3093: loss -2.2543721199035645\n",
      "Iteration 3094: loss -2.097965955734253\n",
      "Iteration 3095: loss -1.939925193786621\n",
      "Iteration 3096: loss -2.051244020462036\n",
      "Iteration 3097: loss -2.2731142044067383\n",
      "Iteration 3098: loss -2.0677406787872314\n",
      "Iteration 3099: loss -1.981494426727295\n",
      "Iteration 3100: loss -2.041224956512451\n",
      "Iteration 3101: loss -2.1672418117523193\n",
      "Iteration 3102: loss -2.0202343463897705\n",
      "Iteration 3103: loss -2.0875470638275146\n",
      "Iteration 3104: loss -2.0533974170684814\n",
      "Iteration 3105: loss -1.9641305208206177\n",
      "Iteration 3106: loss -2.0031721591949463\n",
      "Iteration 3107: loss -2.0321125984191895\n",
      "Iteration 3108: loss -2.1321089267730713\n",
      "Iteration 3109: loss -1.9790281057357788\n",
      "Iteration 3110: loss -2.002770185470581\n",
      "Iteration 3111: loss -2.0367965698242188\n",
      "Iteration 3112: loss -2.0026865005493164\n",
      "Iteration 3113: loss -2.0512421131134033\n",
      "Iteration 3114: loss -2.1119985580444336\n",
      "Iteration 3115: loss -2.0194168090820312\n",
      "Iteration 3116: loss -2.101471424102783\n",
      "Iteration 3117: loss -2.1771926879882812\n",
      "Iteration 3118: loss -2.1980202198028564\n",
      "Iteration 3119: loss -2.0953712463378906\n",
      "Iteration 3120: loss -2.050966501235962\n",
      "Iteration 3121: loss -2.091766119003296\n",
      "Iteration 3122: loss -2.059217691421509\n",
      "Iteration 3123: loss -2.1738362312316895\n",
      "Iteration 3124: loss -2.0326929092407227\n",
      "Iteration 3125: loss -2.0298988819122314\n",
      "Iteration 3126: loss -2.1239101886749268\n",
      "Iteration 3127: loss -2.135141611099243\n",
      "Iteration 3128: loss -2.1069276332855225\n",
      "Iteration 3129: loss -2.2421340942382812\n",
      "Iteration 3130: loss -2.171329975128174\n",
      "Iteration 3131: loss -2.0953776836395264\n",
      "Iteration 3132: loss -2.015977144241333\n",
      "Iteration 3133: loss -2.115102529525757\n",
      "Iteration 3134: loss -2.0524864196777344\n",
      "Iteration 3135: loss -2.0548558235168457\n",
      "Iteration 3136: loss -2.0148026943206787\n",
      "Iteration 3137: loss -1.9770047664642334\n",
      "Iteration 3138: loss -2.0890722274780273\n",
      "Iteration 3139: loss -1.936277985572815\n",
      "Iteration 3140: loss -2.3130898475646973\n",
      "Iteration 3141: loss -2.0391197204589844\n",
      "Iteration 3142: loss -2.1561191082000732\n",
      "Iteration 3143: loss -2.1600592136383057\n",
      "Iteration 3144: loss -2.1364212036132812\n",
      "Iteration 3145: loss -2.0752649307250977\n",
      "Iteration 3146: loss -2.139705181121826\n",
      "Iteration 3147: loss -2.2029500007629395\n",
      "Iteration 3148: loss -2.2236838340759277\n",
      "Iteration 3149: loss -2.240462303161621\n",
      "Iteration 3150: loss -1.9693783521652222\n",
      "Iteration 3151: loss -2.0007967948913574\n",
      "Iteration 3152: loss -2.09256911277771\n",
      "Iteration 3153: loss -2.038487672805786\n",
      "Iteration 3154: loss -1.7373956441879272\n",
      "Iteration 3155: loss -2.260718584060669\n",
      "Iteration 3156: loss -2.080482244491577\n",
      "Iteration 3157: loss -1.9734660387039185\n",
      "Iteration 3158: loss -1.8661293983459473\n",
      "Iteration 3159: loss -1.8751293420791626\n",
      "Iteration 3160: loss -1.950429916381836\n",
      "Iteration 3161: loss -1.847402572631836\n",
      "Iteration 3162: loss -2.1056933403015137\n",
      "Iteration 3163: loss -1.9272505044937134\n",
      "Iteration 3164: loss -2.069586753845215\n",
      "Iteration 3165: loss -1.8647878170013428\n",
      "Iteration 3166: loss -1.9478774070739746\n",
      "Iteration 3167: loss -1.7484939098358154\n",
      "Iteration 3168: loss -1.8510210514068604\n",
      "Iteration 3169: loss -2.039295196533203\n",
      "Iteration 3170: loss -1.9949897527694702\n",
      "Iteration 3171: loss -1.884034276008606\n",
      "Iteration 3172: loss -1.8589669466018677\n",
      "Iteration 3173: loss -2.0984084606170654\n",
      "Iteration 3174: loss -2.0844900608062744\n",
      "Iteration 3175: loss -1.8085824251174927\n",
      "Iteration 3176: loss -1.8109811544418335\n",
      "Iteration 3177: loss -2.095646381378174\n",
      "Iteration 3178: loss -1.9790581464767456\n",
      "Iteration 3179: loss -1.8845632076263428\n",
      "Iteration 3180: loss -1.9385313987731934\n",
      "Iteration 3181: loss -2.0251879692077637\n",
      "Iteration 3182: loss -1.9491103887557983\n",
      "Iteration 3183: loss -1.9581621885299683\n",
      "Iteration 3184: loss -1.8519350290298462\n",
      "Iteration 3185: loss -1.9431084394454956\n",
      "Iteration 3186: loss -1.8989675045013428\n",
      "Iteration 3187: loss -1.8573092222213745\n",
      "Iteration 3188: loss -1.9831807613372803\n",
      "Iteration 3189: loss -2.1623833179473877\n",
      "Iteration 3190: loss -2.1363799571990967\n",
      "Iteration 3191: loss -2.0744411945343018\n",
      "Iteration 3192: loss -2.1632280349731445\n",
      "Iteration 3193: loss -2.02783465385437\n",
      "Iteration 3194: loss -2.0900654792785645\n",
      "Iteration 3195: loss -2.0350394248962402\n",
      "Iteration 3196: loss -2.054368734359741\n",
      "Iteration 3197: loss -2.010282039642334\n",
      "Iteration 3198: loss -1.9481502771377563\n",
      "Iteration 3199: loss -2.2214059829711914\n",
      "Iteration 3200: loss -2.043703556060791\n",
      "Iteration 3201: loss -2.0178382396698\n",
      "Iteration 3202: loss -2.2077693939208984\n",
      "Iteration 3203: loss -1.975311040878296\n",
      "Iteration 3204: loss -2.0705153942108154\n",
      "Iteration 3205: loss -2.043670415878296\n",
      "Iteration 3206: loss -2.1776931285858154\n",
      "Iteration 3207: loss -2.0867605209350586\n",
      "Iteration 3208: loss -2.077235460281372\n",
      "Iteration 3209: loss -2.063812494277954\n",
      "Iteration 3210: loss -2.152357578277588\n",
      "Iteration 3211: loss -1.9231383800506592\n",
      "Iteration 3212: loss -2.026581048965454\n",
      "Iteration 3213: loss -2.1281485557556152\n",
      "Iteration 3214: loss -2.137937068939209\n",
      "Iteration 3215: loss -2.203425884246826\n",
      "Iteration 3216: loss -2.0989577770233154\n",
      "Iteration 3217: loss -2.1357502937316895\n",
      "Iteration 3218: loss -2.1225662231445312\n",
      "Iteration 3219: loss -2.1946091651916504\n",
      "Iteration 3220: loss -2.1611952781677246\n",
      "Iteration 3221: loss -2.1554791927337646\n",
      "Iteration 3222: loss -2.1153347492218018\n",
      "Iteration 3223: loss -2.0549120903015137\n",
      "Iteration 3224: loss -2.2500805854797363\n",
      "Iteration 3225: loss -2.110822916030884\n",
      "Iteration 3226: loss -2.0035009384155273\n",
      "Iteration 3227: loss -1.9840627908706665\n",
      "Iteration 3228: loss -2.0225377082824707\n",
      "Iteration 3229: loss -2.1243832111358643\n",
      "Iteration 3230: loss -2.2068920135498047\n",
      "Iteration 3231: loss -2.1936614513397217\n",
      "Iteration 3232: loss -1.9719512462615967\n",
      "Iteration 3233: loss -2.0383269786834717\n",
      "Iteration 3234: loss -1.9418509006500244\n",
      "Iteration 3235: loss -1.9961196184158325\n",
      "Iteration 3236: loss -2.1462786197662354\n",
      "Iteration 3237: loss -2.0617740154266357\n",
      "Iteration 3238: loss -2.162776231765747\n",
      "Iteration 3239: loss -2.137829065322876\n",
      "Iteration 3240: loss -2.1359801292419434\n",
      "Iteration 3241: loss -2.0915679931640625\n",
      "Iteration 3242: loss -2.051999807357788\n",
      "Iteration 3243: loss -2.1572604179382324\n",
      "Iteration 3244: loss -2.1384787559509277\n",
      "Iteration 3245: loss -2.0058233737945557\n",
      "Iteration 3246: loss -1.978926658630371\n",
      "Iteration 3247: loss -2.238401174545288\n",
      "Iteration 3248: loss -2.0968964099884033\n",
      "Iteration 3249: loss -2.079895496368408\n",
      "Iteration 3250: loss -2.146311044692993\n",
      "Iteration 3251: loss -1.8550045490264893\n",
      "Iteration 3252: loss -2.094888210296631\n",
      "Iteration 3253: loss -2.0566811561584473\n",
      "Iteration 3254: loss -2.2530651092529297\n",
      "Iteration 3255: loss -2.1015684604644775\n",
      "Iteration 3256: loss -2.166616201400757\n",
      "Iteration 3257: loss -2.061065673828125\n",
      "Iteration 3258: loss -2.189528465270996\n",
      "Iteration 3259: loss -1.9986933469772339\n",
      "Iteration 3260: loss -2.216948986053467\n",
      "Iteration 3261: loss -2.0903308391571045\n",
      "Iteration 3262: loss -2.041674852371216\n",
      "Iteration 3263: loss -2.0906081199645996\n",
      "Iteration 3264: loss -2.0474789142608643\n",
      "Iteration 3265: loss -2.1810965538024902\n",
      "Iteration 3266: loss -2.1660008430480957\n",
      "Iteration 3267: loss -1.9402625560760498\n",
      "Iteration 3268: loss -2.1285529136657715\n",
      "Iteration 3269: loss -2.2342610359191895\n",
      "Iteration 3270: loss -2.104724168777466\n",
      "Iteration 3271: loss -2.2043676376342773\n",
      "Iteration 3272: loss -2.187290906906128\n",
      "Iteration 3273: loss -2.196847677230835\n",
      "Iteration 3274: loss -2.0145509243011475\n",
      "Iteration 3275: loss -2.2052834033966064\n",
      "Iteration 3276: loss -2.1038429737091064\n",
      "Iteration 3277: loss -2.0445191860198975\n",
      "Iteration 3278: loss -2.0242302417755127\n",
      "Iteration 3279: loss -2.0634524822235107\n",
      "Iteration 3280: loss -2.056873083114624\n",
      "Iteration 3281: loss -1.930464744567871\n",
      "Iteration 3282: loss -2.0647032260894775\n",
      "Iteration 3283: loss -2.07954740524292\n",
      "Iteration 3284: loss -1.8916845321655273\n",
      "Iteration 3285: loss -2.190857410430908\n",
      "Iteration 3286: loss -2.206890106201172\n",
      "Iteration 3287: loss -2.0523712635040283\n",
      "Iteration 3288: loss -2.0797767639160156\n",
      "Iteration 3289: loss -1.9735616445541382\n",
      "Iteration 3290: loss -2.0652551651000977\n",
      "Iteration 3291: loss -2.129629373550415\n",
      "Iteration 3292: loss -1.912754774093628\n",
      "Iteration 3293: loss -1.9353779554367065\n",
      "Iteration 3294: loss -1.9457238912582397\n",
      "Iteration 3295: loss -2.1643691062927246\n",
      "Iteration 3296: loss -2.061225414276123\n",
      "Iteration 3297: loss -2.075754404067993\n",
      "Iteration 3298: loss -2.109100818634033\n",
      "Iteration 3299: loss -1.9482468366622925\n",
      "Iteration 3300: loss -2.0056750774383545\n",
      "Iteration 3301: loss -2.0956926345825195\n",
      "Iteration 3302: loss -1.8862882852554321\n",
      "Iteration 3303: loss -2.0034711360931396\n",
      "Iteration 3304: loss -2.129611015319824\n",
      "Iteration 3305: loss -2.059598445892334\n",
      "Iteration 3306: loss -2.1173274517059326\n",
      "Iteration 3307: loss -1.8975863456726074\n",
      "Iteration 3308: loss -2.06484317779541\n",
      "Iteration 3309: loss -2.1667401790618896\n",
      "Iteration 3310: loss -2.1424059867858887\n",
      "Iteration 3311: loss -2.0200183391571045\n",
      "Iteration 3312: loss -2.085970640182495\n",
      "Iteration 3313: loss -2.0107388496398926\n",
      "Iteration 3314: loss -2.160567283630371\n",
      "Iteration 3315: loss -2.1271536350250244\n",
      "Iteration 3316: loss -2.023930788040161\n",
      "Iteration 3317: loss -2.148148775100708\n",
      "Iteration 3318: loss -2.11281681060791\n",
      "Iteration 3319: loss -2.3326380252838135\n",
      "Iteration 3320: loss -1.9418777227401733\n",
      "Iteration 3321: loss -2.0737721920013428\n",
      "Iteration 3322: loss -1.9893736839294434\n",
      "Iteration 3323: loss -1.971219539642334\n",
      "Iteration 3324: loss -2.176485776901245\n",
      "Iteration 3325: loss -2.153496742248535\n",
      "Iteration 3326: loss -1.9271246194839478\n",
      "Iteration 3327: loss -2.0148746967315674\n",
      "Iteration 3328: loss -1.9412728548049927\n",
      "Iteration 3329: loss -2.1211557388305664\n",
      "Iteration 3330: loss -2.06193470954895\n",
      "Iteration 3331: loss -2.237715005874634\n",
      "Iteration 3332: loss -2.1064820289611816\n",
      "Iteration 3333: loss -2.1715784072875977\n",
      "Iteration 3334: loss -1.9938445091247559\n",
      "Iteration 3335: loss -2.041968822479248\n",
      "Iteration 3336: loss -2.05618953704834\n",
      "Iteration 3337: loss -2.096149206161499\n",
      "Iteration 3338: loss -2.03163743019104\n",
      "Iteration 3339: loss -2.145338535308838\n",
      "Iteration 3340: loss -2.0276644229888916\n",
      "Iteration 3341: loss -2.0585267543792725\n",
      "Iteration 3342: loss -2.144956111907959\n",
      "Iteration 3343: loss -2.1055798530578613\n",
      "Iteration 3344: loss -2.0487070083618164\n",
      "Iteration 3345: loss -1.8826419115066528\n",
      "Iteration 3346: loss -1.9877735376358032\n",
      "Iteration 3347: loss -2.0616955757141113\n",
      "Iteration 3348: loss -2.0707077980041504\n",
      "Iteration 3349: loss -1.930696964263916\n",
      "Iteration 3350: loss -2.035182476043701\n",
      "Iteration 3351: loss -2.1244890689849854\n",
      "Iteration 3352: loss -1.76410973072052\n",
      "Iteration 3353: loss -2.0316615104675293\n",
      "Iteration 3354: loss -1.9885717630386353\n",
      "Iteration 3355: loss -1.8753854036331177\n",
      "Iteration 3356: loss -2.059783935546875\n",
      "Iteration 3357: loss -1.8582125902175903\n",
      "Iteration 3358: loss -2.0064215660095215\n",
      "Iteration 3359: loss -1.9178589582443237\n",
      "Iteration 3360: loss -2.06811785697937\n",
      "Iteration 3361: loss -2.082521915435791\n",
      "Iteration 3362: loss -1.8966203927993774\n",
      "Iteration 3363: loss -2.0306596755981445\n",
      "Iteration 3364: loss -2.0049455165863037\n",
      "Iteration 3365: loss -1.965746521949768\n",
      "Iteration 3366: loss -1.993467092514038\n",
      "Iteration 3367: loss -2.0823123455047607\n",
      "Iteration 3368: loss -1.9494969844818115\n",
      "Iteration 3369: loss -1.9732154607772827\n",
      "Iteration 3370: loss -2.0157952308654785\n",
      "Iteration 3371: loss -2.131155252456665\n",
      "Iteration 3372: loss -2.1169416904449463\n",
      "Iteration 3373: loss -1.9343994855880737\n",
      "Iteration 3374: loss -2.0643348693847656\n",
      "Iteration 3375: loss -2.016494035720825\n",
      "Iteration 3376: loss -2.0985236167907715\n",
      "Iteration 3377: loss -2.1538307666778564\n",
      "Iteration 3378: loss -2.118299722671509\n",
      "Iteration 3379: loss -2.1152477264404297\n",
      "Iteration 3380: loss -2.037829637527466\n",
      "Iteration 3381: loss -2.2381343841552734\n",
      "Iteration 3382: loss -2.1114022731781006\n",
      "Iteration 3383: loss -2.0583136081695557\n",
      "Iteration 3384: loss -1.9232267141342163\n",
      "Iteration 3385: loss -2.210979461669922\n",
      "Iteration 3386: loss -2.1495728492736816\n",
      "Iteration 3387: loss -2.037034749984741\n",
      "Iteration 3388: loss -2.1448190212249756\n",
      "Iteration 3389: loss -2.121969223022461\n",
      "Iteration 3390: loss -2.051900863647461\n",
      "Iteration 3391: loss -2.058018922805786\n",
      "Iteration 3392: loss -2.0370945930480957\n",
      "Iteration 3393: loss -2.15615177154541\n",
      "Iteration 3394: loss -2.0483949184417725\n",
      "Iteration 3395: loss -2.076070547103882\n",
      "Iteration 3396: loss -2.146144390106201\n",
      "Iteration 3397: loss -2.115244150161743\n",
      "Iteration 3398: loss -2.077728748321533\n",
      "Iteration 3399: loss -1.9796062707901\n",
      "Iteration 3400: loss -2.0490798950195312\n",
      "Iteration 3401: loss -2.265442132949829\n",
      "Iteration 3402: loss -1.794628620147705\n",
      "Iteration 3403: loss -2.192373514175415\n",
      "Iteration 3404: loss -2.084578275680542\n",
      "Iteration 3405: loss -2.412654399871826\n",
      "Iteration 3406: loss -2.032599687576294\n",
      "Iteration 3407: loss -2.1462788581848145\n",
      "Iteration 3408: loss -2.1413981914520264\n",
      "Iteration 3409: loss -2.1350514888763428\n",
      "Iteration 3410: loss -1.8912479877471924\n",
      "Iteration 3411: loss -2.1791534423828125\n",
      "Iteration 3412: loss -1.9177576303482056\n",
      "Iteration 3413: loss -2.2459428310394287\n",
      "Iteration 3414: loss -2.065274715423584\n",
      "Iteration 3415: loss -1.9818719625473022\n",
      "Iteration 3416: loss -2.0072357654571533\n",
      "Iteration 3417: loss -2.051645040512085\n",
      "Iteration 3418: loss -2.1363513469696045\n",
      "Iteration 3419: loss -2.1793549060821533\n",
      "Iteration 3420: loss -2.0241613388061523\n",
      "Iteration 3421: loss -2.24721622467041\n",
      "Iteration 3422: loss -2.0467920303344727\n",
      "Iteration 3423: loss -2.1226391792297363\n",
      "Iteration 3424: loss -2.069836378097534\n",
      "Iteration 3425: loss -2.1361420154571533\n",
      "Iteration 3426: loss -2.1530017852783203\n",
      "Iteration 3427: loss -1.9534580707550049\n",
      "Iteration 3428: loss -2.144815444946289\n",
      "Iteration 3429: loss -2.09568190574646\n",
      "Iteration 3430: loss -2.071535587310791\n",
      "Iteration 3431: loss -2.182222366333008\n",
      "Iteration 3432: loss -2.1206438541412354\n",
      "Iteration 3433: loss -2.0458455085754395\n",
      "Iteration 3434: loss -2.02677321434021\n",
      "Iteration 3435: loss -2.0779337882995605\n",
      "Iteration 3436: loss -1.8769285678863525\n",
      "Iteration 3437: loss -2.226759672164917\n",
      "Iteration 3438: loss -2.192399263381958\n",
      "Iteration 3439: loss -2.0815353393554688\n",
      "Iteration 3440: loss -2.1195244789123535\n",
      "Iteration 3441: loss -1.9369797706604004\n",
      "Iteration 3442: loss -2.0388472080230713\n",
      "Iteration 3443: loss -2.1306276321411133\n",
      "Iteration 3444: loss -2.071592092514038\n",
      "Iteration 3445: loss -2.1166563034057617\n",
      "Iteration 3446: loss -2.08512544631958\n",
      "Iteration 3447: loss -2.0630605220794678\n",
      "Iteration 3448: loss -2.070672035217285\n",
      "Iteration 3449: loss -2.174417734146118\n",
      "Iteration 3450: loss -2.042017936706543\n",
      "Iteration 3451: loss -2.029702663421631\n",
      "Iteration 3452: loss -2.0211288928985596\n",
      "Iteration 3453: loss -2.1581761837005615\n",
      "Iteration 3454: loss -2.2171733379364014\n",
      "Iteration 3455: loss -2.118439197540283\n",
      "Iteration 3456: loss -2.2563414573669434\n",
      "Iteration 3457: loss -2.172464609146118\n",
      "Iteration 3458: loss -2.165506601333618\n",
      "Iteration 3459: loss -2.2730777263641357\n",
      "Iteration 3460: loss -1.93862783908844\n",
      "Iteration 3461: loss -2.032315969467163\n",
      "Iteration 3462: loss -2.1640384197235107\n",
      "Iteration 3463: loss -2.142592668533325\n",
      "Iteration 3464: loss -1.9942641258239746\n",
      "Iteration 3465: loss -2.0506961345672607\n",
      "Iteration 3466: loss -2.1066548824310303\n",
      "Iteration 3467: loss -2.203012704849243\n",
      "Iteration 3468: loss -2.155632734298706\n",
      "Iteration 3469: loss -2.169900417327881\n",
      "Iteration 3470: loss -2.227553129196167\n",
      "Iteration 3471: loss -2.283543109893799\n",
      "Iteration 3472: loss -2.150981903076172\n",
      "Iteration 3473: loss -2.284773111343384\n",
      "Iteration 3474: loss -2.065356492996216\n",
      "Iteration 3475: loss -2.175448179244995\n",
      "Iteration 3476: loss -2.0925307273864746\n",
      "Iteration 3477: loss -1.9576737880706787\n",
      "Iteration 3478: loss -2.07885479927063\n",
      "Iteration 3479: loss -2.216919422149658\n",
      "Iteration 3480: loss -2.06837797164917\n",
      "Iteration 3481: loss -2.0177767276763916\n",
      "Iteration 3482: loss -2.3236310482025146\n",
      "Iteration 3483: loss -2.226332426071167\n",
      "Iteration 3484: loss -2.1534194946289062\n",
      "Iteration 3485: loss -2.1346182823181152\n",
      "Iteration 3486: loss -2.0294017791748047\n",
      "Iteration 3487: loss -2.193300247192383\n",
      "Iteration 3488: loss -2.0839638710021973\n",
      "Iteration 3489: loss -2.232825517654419\n",
      "Iteration 3490: loss -2.1440041065216064\n",
      "Iteration 3491: loss -2.053341865539551\n",
      "Iteration 3492: loss -2.187488317489624\n",
      "Iteration 3493: loss -2.022634744644165\n",
      "Iteration 3494: loss -2.136125087738037\n",
      "Iteration 3495: loss -2.104206085205078\n",
      "Iteration 3496: loss -2.1609091758728027\n",
      "Iteration 3497: loss -2.254694700241089\n",
      "Iteration 3498: loss -2.105149745941162\n",
      "Iteration 3499: loss -2.225762367248535\n",
      "Iteration 3500: loss -1.958320140838623\n",
      "Iteration 3501: loss -2.117518663406372\n",
      "Iteration 3502: loss -1.9724012613296509\n",
      "Iteration 3503: loss -1.9248716831207275\n",
      "Iteration 3504: loss -2.001500129699707\n",
      "Iteration 3505: loss -2.020097494125366\n",
      "Iteration 3506: loss -2.0166845321655273\n",
      "Iteration 3507: loss -2.1631948947906494\n",
      "Iteration 3508: loss -2.156826972961426\n",
      "Iteration 3509: loss -2.0346693992614746\n",
      "Iteration 3510: loss -2.1987171173095703\n",
      "Iteration 3511: loss -2.1149330139160156\n",
      "Iteration 3512: loss -2.011141300201416\n",
      "Iteration 3513: loss -2.1216650009155273\n",
      "Iteration 3514: loss -2.188448190689087\n",
      "Iteration 3515: loss -2.0682904720306396\n",
      "Iteration 3516: loss -2.181600332260132\n",
      "Iteration 3517: loss -2.186091184616089\n",
      "Iteration 3518: loss -1.8942670822143555\n",
      "Iteration 3519: loss -1.9246691465377808\n",
      "Iteration 3520: loss -2.028164863586426\n",
      "Iteration 3521: loss -2.1212971210479736\n",
      "Iteration 3522: loss -2.126417398452759\n",
      "Iteration 3523: loss -2.135890245437622\n",
      "Iteration 3524: loss -2.1295242309570312\n",
      "Iteration 3525: loss -2.1071617603302\n",
      "Iteration 3526: loss -2.159747362136841\n",
      "Iteration 3527: loss -1.9195606708526611\n",
      "Iteration 3528: loss -2.019453763961792\n",
      "Iteration 3529: loss -2.0896010398864746\n",
      "Iteration 3530: loss -2.098466634750366\n",
      "Iteration 3531: loss -2.0524163246154785\n",
      "Iteration 3532: loss -2.1021900177001953\n",
      "Iteration 3533: loss -1.9982116222381592\n",
      "Iteration 3534: loss -2.0681583881378174\n",
      "Iteration 3535: loss -2.105354070663452\n",
      "Iteration 3536: loss -2.1469740867614746\n",
      "Iteration 3537: loss -2.043339967727661\n",
      "Iteration 3538: loss -2.1667025089263916\n",
      "Iteration 3539: loss -2.0901248455047607\n",
      "Iteration 3540: loss -2.027384042739868\n",
      "Iteration 3541: loss -2.0160717964172363\n",
      "Iteration 3542: loss -2.0701489448547363\n",
      "Iteration 3543: loss -1.9151241779327393\n",
      "Iteration 3544: loss -1.9278537034988403\n",
      "Iteration 3545: loss -2.0948524475097656\n",
      "Iteration 3546: loss -2.0665440559387207\n",
      "Iteration 3547: loss -2.316192626953125\n",
      "Iteration 3548: loss -2.160748243331909\n",
      "Iteration 3549: loss -2.0555386543273926\n",
      "Iteration 3550: loss -2.257983446121216\n",
      "Iteration 3551: loss -2.289088010787964\n",
      "Iteration 3552: loss -2.239015817642212\n",
      "Iteration 3553: loss -2.117429733276367\n",
      "Iteration 3554: loss -2.2387828826904297\n",
      "Iteration 3555: loss -2.1393234729766846\n",
      "Iteration 3556: loss -2.261683702468872\n",
      "Iteration 3557: loss -2.178243398666382\n",
      "Iteration 3558: loss -2.093940496444702\n",
      "Iteration 3559: loss -2.110548257827759\n",
      "Iteration 3560: loss -2.182594060897827\n",
      "Iteration 3561: loss -2.1460936069488525\n",
      "Iteration 3562: loss -1.8491721153259277\n",
      "Iteration 3563: loss -2.1595711708068848\n",
      "Iteration 3564: loss -2.177476406097412\n",
      "Iteration 3565: loss -2.0137741565704346\n",
      "Iteration 3566: loss -2.052861213684082\n",
      "Iteration 3567: loss -2.143361806869507\n",
      "Iteration 3568: loss -2.086667776107788\n",
      "Iteration 3569: loss -2.1242549419403076\n",
      "Iteration 3570: loss -2.0713157653808594\n",
      "Iteration 3571: loss -2.137014865875244\n",
      "Iteration 3572: loss -2.109168767929077\n",
      "Iteration 3573: loss -2.0879433155059814\n",
      "Iteration 3574: loss -1.9955965280532837\n",
      "Iteration 3575: loss -2.2626302242279053\n",
      "Iteration 3576: loss -2.0195958614349365\n",
      "Iteration 3577: loss -2.051297903060913\n",
      "Iteration 3578: loss -2.1550416946411133\n",
      "Iteration 3579: loss -2.142322540283203\n",
      "Iteration 3580: loss -2.2263922691345215\n",
      "Iteration 3581: loss -2.1300981044769287\n",
      "Iteration 3582: loss -2.0602355003356934\n",
      "Iteration 3583: loss -2.110460042953491\n",
      "Iteration 3584: loss -2.1871392726898193\n",
      "Iteration 3585: loss -2.209381580352783\n",
      "Iteration 3586: loss -2.1345772743225098\n",
      "Iteration 3587: loss -2.215198040008545\n",
      "Iteration 3588: loss -2.033229351043701\n",
      "Iteration 3589: loss -2.2178430557250977\n",
      "Iteration 3590: loss -2.060577630996704\n",
      "Iteration 3591: loss -2.057349443435669\n",
      "Iteration 3592: loss -2.2307024002075195\n",
      "Iteration 3593: loss -1.9787453413009644\n",
      "Iteration 3594: loss -2.127136468887329\n",
      "Iteration 3595: loss -2.097923755645752\n",
      "Iteration 3596: loss -2.0008938312530518\n",
      "Iteration 3597: loss -2.2563672065734863\n",
      "Iteration 3598: loss -2.2450432777404785\n",
      "Iteration 3599: loss -1.9921941757202148\n",
      "Iteration 3600: loss -2.225348711013794\n",
      "Iteration 3601: loss -2.1793372631073\n",
      "Iteration 3602: loss -2.1694934368133545\n",
      "Iteration 3603: loss -2.0667166709899902\n",
      "Iteration 3604: loss -2.168154239654541\n",
      "Iteration 3605: loss -2.0154004096984863\n",
      "Iteration 3606: loss -2.1354315280914307\n",
      "Iteration 3607: loss -2.1057515144348145\n",
      "Iteration 3608: loss -2.0016069412231445\n",
      "Iteration 3609: loss -2.093268394470215\n",
      "Iteration 3610: loss -2.1009459495544434\n",
      "Iteration 3611: loss -1.7875694036483765\n",
      "Iteration 3612: loss -2.1484320163726807\n",
      "Iteration 3613: loss -2.03806471824646\n",
      "Iteration 3614: loss -2.04217791557312\n",
      "Iteration 3615: loss -1.9626998901367188\n",
      "Iteration 3616: loss -2.232050657272339\n",
      "Iteration 3617: loss -2.225071907043457\n",
      "Iteration 3618: loss -1.9486446380615234\n",
      "Iteration 3619: loss -2.0039031505584717\n",
      "Iteration 3620: loss -1.996888279914856\n",
      "Iteration 3621: loss -2.1675450801849365\n",
      "Iteration 3622: loss -1.9105480909347534\n",
      "Iteration 3623: loss -2.004835367202759\n",
      "Iteration 3624: loss -2.162280797958374\n",
      "Iteration 3625: loss -2.057263135910034\n",
      "Iteration 3626: loss -2.1754677295684814\n",
      "Iteration 3627: loss -2.1642227172851562\n",
      "Iteration 3628: loss -2.1283628940582275\n",
      "Iteration 3629: loss -2.0832581520080566\n",
      "Iteration 3630: loss -2.1659653186798096\n",
      "Iteration 3631: loss -1.93878173828125\n",
      "Iteration 3632: loss -2.18776535987854\n",
      "Iteration 3633: loss -2.2128124237060547\n",
      "Iteration 3634: loss -2.0161616802215576\n",
      "Iteration 3635: loss -2.2150495052337646\n",
      "Iteration 3636: loss -1.9583890438079834\n",
      "Iteration 3637: loss -2.0516209602355957\n",
      "Iteration 3638: loss -2.101269006729126\n",
      "Iteration 3639: loss -2.2693395614624023\n",
      "Iteration 3640: loss -1.991007924079895\n",
      "Iteration 3641: loss -2.087066650390625\n",
      "Iteration 3642: loss -2.184154510498047\n",
      "Iteration 3643: loss -2.114041328430176\n",
      "Iteration 3644: loss -2.1962838172912598\n",
      "Iteration 3645: loss -2.038637399673462\n",
      "Iteration 3646: loss -2.157139778137207\n",
      "Iteration 3647: loss -2.0483005046844482\n",
      "Iteration 3648: loss -2.126096487045288\n",
      "Iteration 3649: loss -1.9280489683151245\n",
      "Iteration 3650: loss -2.0596060752868652\n",
      "Iteration 3651: loss -1.8541017770767212\n",
      "Iteration 3652: loss -2.2156286239624023\n",
      "Iteration 3653: loss -1.9602810144424438\n",
      "Iteration 3654: loss -1.9945451021194458\n",
      "Iteration 3655: loss -2.2142903804779053\n",
      "Iteration 3656: loss -2.0244569778442383\n",
      "Iteration 3657: loss -2.1024489402770996\n",
      "Iteration 3658: loss -1.9750287532806396\n",
      "Iteration 3659: loss -2.1807048320770264\n",
      "Iteration 3660: loss -2.1717636585235596\n",
      "Iteration 3661: loss -2.11777400970459\n",
      "Iteration 3662: loss -2.1668007373809814\n",
      "Iteration 3663: loss -2.112626791000366\n",
      "Iteration 3664: loss -2.144902467727661\n",
      "Iteration 3665: loss -2.1820340156555176\n",
      "Iteration 3666: loss -2.0673811435699463\n",
      "Iteration 3667: loss -1.9995253086090088\n",
      "Iteration 3668: loss -2.2128448486328125\n",
      "Iteration 3669: loss -1.7576169967651367\n",
      "Iteration 3670: loss -2.105980157852173\n",
      "Iteration 3671: loss -2.1813762187957764\n",
      "Iteration 3672: loss -2.1542809009552\n",
      "Iteration 3673: loss -1.9577021598815918\n",
      "Iteration 3674: loss -1.9749257564544678\n",
      "Iteration 3675: loss -2.019153594970703\n",
      "Iteration 3676: loss -1.9140197038650513\n",
      "Iteration 3677: loss -2.079069137573242\n",
      "Iteration 3678: loss -2.247663736343384\n",
      "Iteration 3679: loss -2.1299796104431152\n",
      "Iteration 3680: loss -2.21631121635437\n",
      "Iteration 3681: loss -2.0859086513519287\n",
      "Iteration 3682: loss -2.1490886211395264\n",
      "Iteration 3683: loss -2.1212446689605713\n",
      "Iteration 3684: loss -2.178719997406006\n",
      "Iteration 3685: loss -1.9617855548858643\n",
      "Iteration 3686: loss -2.133455753326416\n",
      "Iteration 3687: loss -2.1054298877716064\n",
      "Iteration 3688: loss -2.112910747528076\n",
      "Iteration 3689: loss -2.1416070461273193\n",
      "Iteration 3690: loss -1.906874656677246\n",
      "Iteration 3691: loss -2.166351079940796\n",
      "Iteration 3692: loss -2.1907804012298584\n",
      "Iteration 3693: loss -2.174842596054077\n",
      "Iteration 3694: loss -2.048546075820923\n",
      "Iteration 3695: loss -2.1141083240509033\n",
      "Iteration 3696: loss -2.103527784347534\n",
      "Iteration 3697: loss -2.043268918991089\n",
      "Iteration 3698: loss -2.171189785003662\n",
      "Iteration 3699: loss -2.0813984870910645\n",
      "Iteration 3700: loss -2.2497551441192627\n",
      "Iteration 3701: loss -2.1999435424804688\n",
      "Iteration 3702: loss -2.164544105529785\n",
      "Iteration 3703: loss -2.022716999053955\n",
      "Iteration 3704: loss -1.928000807762146\n",
      "Iteration 3705: loss -2.115657091140747\n",
      "Iteration 3706: loss -2.178880453109741\n",
      "Iteration 3707: loss -1.8977068662643433\n",
      "Iteration 3708: loss -2.0633299350738525\n",
      "Iteration 3709: loss -2.146892786026001\n",
      "Iteration 3710: loss -2.0072479248046875\n",
      "Iteration 3711: loss -2.0645532608032227\n",
      "Iteration 3712: loss -2.1048855781555176\n",
      "Iteration 3713: loss -2.242558717727661\n",
      "Iteration 3714: loss -2.263725757598877\n",
      "Iteration 3715: loss -2.2546441555023193\n",
      "Iteration 3716: loss -2.0756561756134033\n",
      "Iteration 3717: loss -2.1654775142669678\n",
      "Iteration 3718: loss -2.032621383666992\n",
      "Iteration 3719: loss -2.003654956817627\n",
      "Iteration 3720: loss -2.1702752113342285\n",
      "Iteration 3721: loss -2.0091261863708496\n",
      "Iteration 3722: loss -2.069716453552246\n",
      "Iteration 3723: loss -2.147367000579834\n",
      "Iteration 3724: loss -2.1092686653137207\n",
      "Iteration 3725: loss -1.9626907110214233\n",
      "Iteration 3726: loss -2.2067854404449463\n",
      "Iteration 3727: loss -1.9875164031982422\n",
      "Iteration 3728: loss -2.032013416290283\n",
      "Iteration 3729: loss -2.1909449100494385\n",
      "Iteration 3730: loss -1.9348326921463013\n",
      "Iteration 3731: loss -2.0620479583740234\n",
      "Iteration 3732: loss -1.9340581893920898\n",
      "Iteration 3733: loss -1.9891420602798462\n",
      "Iteration 3734: loss -1.9753353595733643\n",
      "Iteration 3735: loss -2.032090902328491\n",
      "Iteration 3736: loss -2.087968349456787\n",
      "Iteration 3737: loss -2.0763165950775146\n",
      "Iteration 3738: loss -1.991458535194397\n",
      "Iteration 3739: loss -2.022770643234253\n",
      "Iteration 3740: loss -1.957727313041687\n",
      "Iteration 3741: loss -2.004227876663208\n",
      "Iteration 3742: loss -2.214370012283325\n",
      "Iteration 3743: loss -1.866794228553772\n",
      "Iteration 3744: loss -2.0938010215759277\n",
      "Iteration 3745: loss -2.289144277572632\n",
      "Iteration 3746: loss -2.0926828384399414\n",
      "Iteration 3747: loss -2.0931859016418457\n",
      "Iteration 3748: loss -2.1662802696228027\n",
      "Iteration 3749: loss -2.117757558822632\n",
      "Iteration 3750: loss -2.0101587772369385\n",
      "Iteration 3751: loss -2.1962039470672607\n",
      "Iteration 3752: loss -2.1029186248779297\n",
      "Iteration 3753: loss -2.262603521347046\n",
      "Iteration 3754: loss -2.2221975326538086\n",
      "Iteration 3755: loss -2.1993227005004883\n",
      "Iteration 3756: loss -2.1517627239227295\n",
      "Iteration 3757: loss -2.1475844383239746\n",
      "Iteration 3758: loss -2.24214768409729\n",
      "Iteration 3759: loss -2.1034677028656006\n",
      "Iteration 3760: loss -2.0533413887023926\n",
      "Iteration 3761: loss -2.1695492267608643\n",
      "Iteration 3762: loss -1.983095407485962\n",
      "Iteration 3763: loss -2.010678291320801\n",
      "Iteration 3764: loss -1.9484658241271973\n",
      "Iteration 3765: loss -2.164553642272949\n",
      "Iteration 3766: loss -2.116068124771118\n",
      "Iteration 3767: loss -2.0882794857025146\n",
      "Iteration 3768: loss -1.95555579662323\n",
      "Iteration 3769: loss -2.0770678520202637\n",
      "Iteration 3770: loss -2.104001522064209\n",
      "Iteration 3771: loss -2.0793588161468506\n",
      "Iteration 3772: loss -2.164076566696167\n",
      "Iteration 3773: loss -2.0172297954559326\n",
      "Iteration 3774: loss -2.2265067100524902\n",
      "Iteration 3775: loss -2.1785190105438232\n",
      "Iteration 3776: loss -1.9557206630706787\n",
      "Iteration 3777: loss -1.9936232566833496\n",
      "Iteration 3778: loss -2.137958526611328\n",
      "Iteration 3779: loss -2.1575868129730225\n",
      "Iteration 3780: loss -2.0851688385009766\n",
      "Iteration 3781: loss -2.125142812728882\n",
      "Iteration 3782: loss -2.0948076248168945\n",
      "Iteration 3783: loss -2.0941505432128906\n",
      "Iteration 3784: loss -2.082186222076416\n",
      "Iteration 3785: loss -2.059011220932007\n",
      "Iteration 3786: loss -2.0970077514648438\n",
      "Iteration 3787: loss -2.072324752807617\n",
      "Iteration 3788: loss -2.186588764190674\n",
      "Iteration 3789: loss -1.9894704818725586\n",
      "Iteration 3790: loss -2.2842464447021484\n",
      "Iteration 3791: loss -2.0128183364868164\n",
      "Iteration 3792: loss -2.1454570293426514\n",
      "Iteration 3793: loss -2.0587575435638428\n",
      "Iteration 3794: loss -2.1032283306121826\n",
      "Iteration 3795: loss -1.9349547624588013\n",
      "Iteration 3796: loss -2.1093289852142334\n",
      "Iteration 3797: loss -2.1785576343536377\n",
      "Iteration 3798: loss -2.13769268989563\n",
      "Iteration 3799: loss -1.950537085533142\n",
      "Iteration 3800: loss -2.141242027282715\n",
      "Iteration 3801: loss -2.102315902709961\n",
      "Iteration 3802: loss -2.023773431777954\n",
      "Iteration 3803: loss -2.1589856147766113\n",
      "Iteration 3804: loss -2.0060436725616455\n",
      "Iteration 3805: loss -2.218862771987915\n",
      "Iteration 3806: loss -2.2064621448516846\n",
      "Iteration 3807: loss -2.176414966583252\n",
      "Iteration 3808: loss -2.1237242221832275\n",
      "Iteration 3809: loss -2.0555500984191895\n",
      "Iteration 3810: loss -2.054103136062622\n",
      "Iteration 3811: loss -2.0676522254943848\n",
      "Iteration 3812: loss -2.0626378059387207\n",
      "Iteration 3813: loss -2.10552978515625\n",
      "Iteration 3814: loss -2.1774001121520996\n",
      "Iteration 3815: loss -2.2273688316345215\n",
      "Iteration 3816: loss -2.130629301071167\n",
      "Iteration 3817: loss -2.3014400005340576\n",
      "Iteration 3818: loss -2.196221351623535\n",
      "Iteration 3819: loss -2.1908702850341797\n",
      "Iteration 3820: loss -1.9979592561721802\n",
      "Iteration 3821: loss -2.1051573753356934\n",
      "Iteration 3822: loss -2.1009905338287354\n",
      "Iteration 3823: loss -2.171144723892212\n",
      "Iteration 3824: loss -2.2349095344543457\n",
      "Iteration 3825: loss -2.1892192363739014\n",
      "Iteration 3826: loss -2.1996593475341797\n",
      "Iteration 3827: loss -2.1549746990203857\n",
      "Iteration 3828: loss -2.158169746398926\n",
      "Iteration 3829: loss -2.040415048599243\n",
      "Iteration 3830: loss -2.09246826171875\n",
      "Iteration 3831: loss -2.184584856033325\n",
      "Iteration 3832: loss -2.103681802749634\n",
      "Iteration 3833: loss -2.2161641120910645\n",
      "Iteration 3834: loss -1.978988766670227\n",
      "Iteration 3835: loss -2.1544249057769775\n",
      "Iteration 3836: loss -2.136711359024048\n",
      "Iteration 3837: loss -2.0549604892730713\n",
      "Iteration 3838: loss -2.105161190032959\n",
      "Iteration 3839: loss -2.0933284759521484\n",
      "Iteration 3840: loss -1.9786666631698608\n",
      "Iteration 3841: loss -2.2282447814941406\n",
      "Iteration 3842: loss -2.0643882751464844\n",
      "Iteration 3843: loss -2.0632312297821045\n",
      "Iteration 3844: loss -2.1851634979248047\n",
      "Iteration 3845: loss -2.2703006267547607\n",
      "Iteration 3846: loss -2.09513258934021\n",
      "Iteration 3847: loss -1.9997941255569458\n",
      "Iteration 3848: loss -2.121182918548584\n",
      "Iteration 3849: loss -2.130197286605835\n",
      "Iteration 3850: loss -1.9772096872329712\n",
      "Iteration 3851: loss -2.0737733840942383\n",
      "Iteration 3852: loss -1.9525099992752075\n",
      "Iteration 3853: loss -2.0114283561706543\n",
      "Iteration 3854: loss -2.114765167236328\n",
      "Iteration 3855: loss -2.0886757373809814\n",
      "Iteration 3856: loss -2.027076005935669\n",
      "Iteration 3857: loss -2.0383923053741455\n",
      "Iteration 3858: loss -2.071617364883423\n",
      "Iteration 3859: loss -2.0176749229431152\n",
      "Iteration 3860: loss -2.0784764289855957\n",
      "Iteration 3861: loss -1.905827283859253\n",
      "Iteration 3862: loss -2.037097692489624\n",
      "Iteration 3863: loss -2.2112603187561035\n",
      "Iteration 3864: loss -2.12540340423584\n",
      "Iteration 3865: loss -2.1151437759399414\n",
      "Iteration 3866: loss -2.177370548248291\n",
      "Iteration 3867: loss -2.1171939373016357\n",
      "Iteration 3868: loss -2.1904029846191406\n",
      "Iteration 3869: loss -2.1335253715515137\n",
      "Iteration 3870: loss -2.161280393600464\n",
      "Iteration 3871: loss -2.171207904815674\n",
      "Iteration 3872: loss -2.2538082599639893\n",
      "Iteration 3873: loss -2.04939866065979\n",
      "Iteration 3874: loss -2.0608761310577393\n",
      "Iteration 3875: loss -2.253114938735962\n",
      "Iteration 3876: loss -2.03576922416687\n",
      "Iteration 3877: loss -2.1310949325561523\n",
      "Iteration 3878: loss -1.9926540851593018\n",
      "Iteration 3879: loss -2.343388080596924\n",
      "Iteration 3880: loss -2.1092617511749268\n",
      "Iteration 3881: loss -2.1021008491516113\n",
      "Iteration 3882: loss -1.9929882287979126\n",
      "Iteration 3883: loss -2.147156238555908\n",
      "Iteration 3884: loss -2.2680273056030273\n",
      "Iteration 3885: loss -2.1815805435180664\n",
      "Iteration 3886: loss -2.098465919494629\n",
      "Iteration 3887: loss -2.097123146057129\n",
      "Iteration 3888: loss -2.117295503616333\n",
      "Iteration 3889: loss -2.062534809112549\n",
      "Iteration 3890: loss -2.056182861328125\n",
      "Iteration 3891: loss -2.195772409439087\n",
      "Iteration 3892: loss -2.1850976943969727\n",
      "Iteration 3893: loss -2.0106468200683594\n",
      "Iteration 3894: loss -2.0954556465148926\n",
      "Iteration 3895: loss -1.9153536558151245\n",
      "Iteration 3896: loss -2.1716976165771484\n",
      "Iteration 3897: loss -2.0906777381896973\n",
      "Iteration 3898: loss -2.098341226577759\n",
      "Iteration 3899: loss -2.0856032371520996\n",
      "Iteration 3900: loss -2.0202629566192627\n",
      "Iteration 3901: loss -1.9716895818710327\n",
      "Iteration 3902: loss -2.1609344482421875\n",
      "Iteration 3903: loss -1.9896137714385986\n",
      "Iteration 3904: loss -2.1651132106781006\n",
      "Iteration 3905: loss -2.1048059463500977\n",
      "Iteration 3906: loss -2.1532623767852783\n",
      "Iteration 3907: loss -2.1045472621917725\n",
      "Iteration 3908: loss -2.1553876399993896\n",
      "Iteration 3909: loss -2.246403217315674\n",
      "Iteration 3910: loss -2.2139570713043213\n",
      "Iteration 3911: loss -2.149869441986084\n",
      "Iteration 3912: loss -2.1296167373657227\n",
      "Iteration 3913: loss -2.24924898147583\n",
      "Iteration 3914: loss -2.17840576171875\n",
      "Iteration 3915: loss -1.9338674545288086\n",
      "Iteration 3916: loss -2.029690980911255\n",
      "Iteration 3917: loss -2.08335542678833\n",
      "Iteration 3918: loss -2.097348213195801\n",
      "Iteration 3919: loss -2.2148220539093018\n",
      "Iteration 3920: loss -2.094296455383301\n",
      "Iteration 3921: loss -2.0538933277130127\n",
      "Iteration 3922: loss -2.228731393814087\n",
      "Iteration 3923: loss -2.2001266479492188\n",
      "Iteration 3924: loss -2.053859233856201\n",
      "Iteration 3925: loss -2.1695215702056885\n",
      "Iteration 3926: loss -2.079723596572876\n",
      "Iteration 3927: loss -2.1462912559509277\n",
      "Iteration 3928: loss -2.158036231994629\n",
      "Iteration 3929: loss -2.0753095149993896\n",
      "Iteration 3930: loss -2.185577392578125\n",
      "Iteration 3931: loss -2.235574960708618\n",
      "Iteration 3932: loss -2.0549120903015137\n",
      "Iteration 3933: loss -1.934136152267456\n",
      "Iteration 3934: loss -2.2117257118225098\n",
      "Iteration 3935: loss -2.19340443611145\n",
      "Iteration 3936: loss -2.119640588760376\n",
      "Iteration 3937: loss -2.0870723724365234\n",
      "Iteration 3938: loss -2.092740058898926\n",
      "Iteration 3939: loss -2.3087079524993896\n",
      "Iteration 3940: loss -2.0063352584838867\n",
      "Iteration 3941: loss -2.265820264816284\n",
      "Iteration 3942: loss -2.0600366592407227\n",
      "Iteration 3943: loss -2.1155693531036377\n",
      "Iteration 3944: loss -2.1271274089813232\n",
      "Iteration 3945: loss -2.1233913898468018\n",
      "Iteration 3946: loss -2.1664915084838867\n",
      "Iteration 3947: loss -2.052082061767578\n",
      "Iteration 3948: loss -2.322618246078491\n",
      "Iteration 3949: loss -1.932545781135559\n",
      "Iteration 3950: loss -2.104356050491333\n",
      "Iteration 3951: loss -1.9955573081970215\n",
      "Iteration 3952: loss -2.276289939880371\n",
      "Iteration 3953: loss -2.2120323181152344\n",
      "Iteration 3954: loss -2.0953691005706787\n",
      "Iteration 3955: loss -2.21142578125\n",
      "Iteration 3956: loss -1.9520870447158813\n",
      "Iteration 3957: loss -2.1015231609344482\n",
      "Iteration 3958: loss -2.025628089904785\n",
      "Iteration 3959: loss -2.12247896194458\n",
      "Iteration 3960: loss -2.083972454071045\n",
      "Iteration 3961: loss -2.059112548828125\n",
      "Iteration 3962: loss -2.2030930519104004\n",
      "Iteration 3963: loss -1.912965178489685\n",
      "Iteration 3964: loss -2.0087523460388184\n",
      "Iteration 3965: loss -2.0393359661102295\n",
      "Iteration 3966: loss -2.0945773124694824\n",
      "Iteration 3967: loss -2.092060089111328\n",
      "Iteration 3968: loss -1.9444242715835571\n",
      "Iteration 3969: loss -2.0711748600006104\n",
      "Iteration 3970: loss -2.1110806465148926\n",
      "Iteration 3971: loss -2.117424249649048\n",
      "Iteration 3972: loss -2.3234143257141113\n",
      "Iteration 3973: loss -2.2585527896881104\n",
      "Iteration 3974: loss -2.1779274940490723\n",
      "Iteration 3975: loss -2.0940396785736084\n",
      "Iteration 3976: loss -2.0680224895477295\n",
      "Iteration 3977: loss -2.0431790351867676\n",
      "Iteration 3978: loss -2.064962387084961\n",
      "Iteration 3979: loss -2.142853021621704\n",
      "Iteration 3980: loss -2.1343328952789307\n",
      "Iteration 3981: loss -2.1003079414367676\n",
      "Iteration 3982: loss -2.120389223098755\n",
      "Iteration 3983: loss -2.214984178543091\n",
      "Iteration 3984: loss -2.026505470275879\n",
      "Iteration 3985: loss -2.2118313312530518\n",
      "Iteration 3986: loss -2.0845539569854736\n",
      "Iteration 3987: loss -1.9136213064193726\n",
      "Iteration 3988: loss -2.063387632369995\n",
      "Iteration 3989: loss -2.1908340454101562\n",
      "Iteration 3990: loss -2.0998237133026123\n",
      "Iteration 3991: loss -2.011326789855957\n",
      "Iteration 3992: loss -2.197835922241211\n",
      "Iteration 3993: loss -2.0500035285949707\n",
      "Iteration 3994: loss -2.084789752960205\n",
      "Iteration 3995: loss -1.9779729843139648\n",
      "Iteration 3996: loss -2.0970189571380615\n",
      "Iteration 3997: loss -2.0000665187835693\n",
      "Iteration 3998: loss -2.198108673095703\n",
      "Iteration 3999: loss -1.9526094198226929\n",
      "Iteration 4000: loss -2.0290045738220215\n",
      "Iteration 4001: loss -2.0508334636688232\n",
      "Iteration 4002: loss -2.12522554397583\n",
      "Iteration 4003: loss -2.044797897338867\n",
      "Iteration 4004: loss -2.0358903408050537\n",
      "Iteration 4005: loss -2.229065179824829\n",
      "Iteration 4006: loss -2.0350656509399414\n",
      "Iteration 4007: loss -2.1090898513793945\n",
      "Iteration 4008: loss -2.1181366443634033\n",
      "Iteration 4009: loss -2.186957597732544\n",
      "Iteration 4010: loss -2.123542070388794\n",
      "Iteration 4011: loss -2.1825218200683594\n",
      "Iteration 4012: loss -2.262141227722168\n",
      "Iteration 4013: loss -2.212092876434326\n",
      "Iteration 4014: loss -2.2307918071746826\n",
      "Iteration 4015: loss -2.217818260192871\n",
      "Iteration 4016: loss -2.1911370754241943\n",
      "Iteration 4017: loss -2.1741652488708496\n",
      "Iteration 4018: loss -2.1867048740386963\n",
      "Iteration 4019: loss -2.0750274658203125\n",
      "Iteration 4020: loss -2.1141934394836426\n",
      "Iteration 4021: loss -2.0586764812469482\n",
      "Iteration 4022: loss -2.2671027183532715\n",
      "Iteration 4023: loss -1.9855114221572876\n",
      "Iteration 4024: loss -2.207887887954712\n",
      "Iteration 4025: loss -2.127070188522339\n",
      "Iteration 4026: loss -2.0904176235198975\n",
      "Iteration 4027: loss -2.0997252464294434\n",
      "Iteration 4028: loss -2.0760135650634766\n",
      "Iteration 4029: loss -2.0020763874053955\n",
      "Iteration 4030: loss -1.9056015014648438\n",
      "Iteration 4031: loss -2.102947950363159\n",
      "Iteration 4032: loss -2.161029100418091\n",
      "Iteration 4033: loss -2.1864616870880127\n",
      "Iteration 4034: loss -2.1312947273254395\n",
      "Iteration 4035: loss -2.097095489501953\n",
      "Iteration 4036: loss -2.112093687057495\n",
      "Iteration 4037: loss -2.048933982849121\n",
      "Iteration 4038: loss -2.1502532958984375\n",
      "Iteration 4039: loss -2.2205519676208496\n",
      "Iteration 4040: loss -2.305187702178955\n",
      "Iteration 4041: loss -2.1666369438171387\n",
      "Iteration 4042: loss -2.022913694381714\n",
      "Iteration 4043: loss -2.2277259826660156\n",
      "Iteration 4044: loss -2.1010358333587646\n",
      "Iteration 4045: loss -2.0850112438201904\n",
      "Iteration 4046: loss -2.2048091888427734\n",
      "Iteration 4047: loss -2.019765615463257\n",
      "Iteration 4048: loss -2.296745777130127\n",
      "Iteration 4049: loss -2.1440539360046387\n",
      "Iteration 4050: loss -2.2913811206817627\n",
      "Iteration 4051: loss -2.258836269378662\n",
      "Iteration 4052: loss -2.1937358379364014\n",
      "Iteration 4053: loss -2.004828691482544\n",
      "Iteration 4054: loss -2.2802391052246094\n",
      "Iteration 4055: loss -2.177499771118164\n",
      "Iteration 4056: loss -2.0272014141082764\n",
      "Iteration 4057: loss -2.222658395767212\n",
      "Iteration 4058: loss -2.1510467529296875\n",
      "Iteration 4059: loss -2.249579429626465\n",
      "Iteration 4060: loss -2.0944581031799316\n",
      "Iteration 4061: loss -2.1812145709991455\n",
      "Iteration 4062: loss -2.0552141666412354\n",
      "Iteration 4063: loss -2.157010793685913\n",
      "Iteration 4064: loss -2.0376052856445312\n",
      "Iteration 4065: loss -2.125079393386841\n",
      "Iteration 4066: loss -2.1967663764953613\n",
      "Iteration 4067: loss -1.9517871141433716\n",
      "Iteration 4068: loss -2.0509376525878906\n",
      "Iteration 4069: loss -2.2150063514709473\n",
      "Iteration 4070: loss -2.110748529434204\n",
      "Iteration 4071: loss -2.3330953121185303\n",
      "Iteration 4072: loss -2.251185178756714\n",
      "Iteration 4073: loss -2.2065248489379883\n",
      "Iteration 4074: loss -2.168497323989868\n",
      "Iteration 4075: loss -2.2087771892547607\n",
      "Iteration 4076: loss -2.1339356899261475\n",
      "Iteration 4077: loss -2.3119897842407227\n",
      "Iteration 4078: loss -2.2930943965911865\n",
      "Iteration 4079: loss -2.473410129547119\n",
      "Iteration 4080: loss -2.016187906265259\n",
      "Iteration 4081: loss -2.181809902191162\n",
      "Iteration 4082: loss -2.1545941829681396\n",
      "Iteration 4083: loss -2.217841386795044\n",
      "Iteration 4084: loss -2.376821279525757\n",
      "Iteration 4085: loss -2.049198627471924\n",
      "Iteration 4086: loss -2.267430067062378\n",
      "Iteration 4087: loss -2.1047420501708984\n",
      "Iteration 4088: loss -2.044889211654663\n",
      "Iteration 4089: loss -1.9093352556228638\n",
      "Iteration 4090: loss -2.090047597885132\n",
      "Iteration 4091: loss -2.0107905864715576\n",
      "Iteration 4092: loss -2.042405843734741\n",
      "Iteration 4093: loss -2.051643133163452\n",
      "Iteration 4094: loss -2.2857611179351807\n",
      "Iteration 4095: loss -2.128013849258423\n",
      "Iteration 4096: loss -2.0960893630981445\n",
      "Iteration 4097: loss -2.275515079498291\n",
      "Iteration 4098: loss -2.0769777297973633\n",
      "Iteration 4099: loss -2.322274923324585\n",
      "Iteration 4100: loss -2.136373996734619\n",
      "Iteration 4101: loss -2.2772064208984375\n",
      "Iteration 4102: loss -2.1190736293792725\n",
      "Iteration 4103: loss -2.0957448482513428\n",
      "Iteration 4104: loss -2.1059067249298096\n",
      "Iteration 4105: loss -2.1102850437164307\n",
      "Iteration 4106: loss -2.1335339546203613\n",
      "Iteration 4107: loss -2.1455559730529785\n",
      "Iteration 4108: loss -2.1465089321136475\n",
      "Iteration 4109: loss -2.0014326572418213\n",
      "Iteration 4110: loss -2.0942094326019287\n",
      "Iteration 4111: loss -2.0811307430267334\n",
      "Iteration 4112: loss -2.190682888031006\n",
      "Iteration 4113: loss -2.2008492946624756\n",
      "Iteration 4114: loss -2.29579758644104\n",
      "Iteration 4115: loss -2.1283273696899414\n",
      "Iteration 4116: loss -2.1161789894104004\n",
      "Iteration 4117: loss -2.1674304008483887\n",
      "Iteration 4118: loss -2.192685127258301\n",
      "Iteration 4119: loss -2.3902838230133057\n",
      "Iteration 4120: loss -2.1706318855285645\n",
      "Iteration 4121: loss -2.146253824234009\n",
      "Iteration 4122: loss -2.1776793003082275\n",
      "Iteration 4123: loss -2.197144031524658\n",
      "Iteration 4124: loss -2.228564739227295\n",
      "Iteration 4125: loss -2.034999370574951\n",
      "Iteration 4126: loss -1.9776278734207153\n",
      "Iteration 4127: loss -2.2988290786743164\n",
      "Iteration 4128: loss -2.168269395828247\n",
      "Iteration 4129: loss -2.1048948764801025\n",
      "Iteration 4130: loss -2.231327533721924\n",
      "Iteration 4131: loss -2.1461896896362305\n",
      "Iteration 4132: loss -2.0386900901794434\n",
      "Iteration 4133: loss -2.213080644607544\n",
      "Iteration 4134: loss -2.152040719985962\n",
      "Iteration 4135: loss -2.0892693996429443\n",
      "Iteration 4136: loss -1.946907639503479\n",
      "Iteration 4137: loss -2.0756852626800537\n",
      "Iteration 4138: loss -2.0378847122192383\n",
      "Iteration 4139: loss -2.0987565517425537\n",
      "Iteration 4140: loss -2.0052292346954346\n",
      "Iteration 4141: loss -2.183901071548462\n",
      "Iteration 4142: loss -2.0935750007629395\n",
      "Iteration 4143: loss -2.233532428741455\n",
      "Iteration 4144: loss -2.2890028953552246\n",
      "Iteration 4145: loss -2.076423406600952\n",
      "Iteration 4146: loss -2.068537950515747\n",
      "Iteration 4147: loss -2.0500195026397705\n",
      "Iteration 4148: loss -2.04254412651062\n",
      "Iteration 4149: loss -2.224984645843506\n",
      "Iteration 4150: loss -2.0342535972595215\n",
      "Iteration 4151: loss -1.9794107675552368\n",
      "Iteration 4152: loss -2.1539463996887207\n",
      "Iteration 4153: loss -2.3257625102996826\n",
      "Iteration 4154: loss -1.988924264907837\n",
      "Iteration 4155: loss -2.1275224685668945\n",
      "Iteration 4156: loss -2.1964468955993652\n",
      "Iteration 4157: loss -2.124504566192627\n",
      "Iteration 4158: loss -2.1381235122680664\n",
      "Iteration 4159: loss -2.221280813217163\n",
      "Iteration 4160: loss -2.2186315059661865\n",
      "Iteration 4161: loss -2.01322603225708\n",
      "Iteration 4162: loss -2.189840078353882\n",
      "Iteration 4163: loss -2.1988015174865723\n",
      "Iteration 4164: loss -2.1658010482788086\n",
      "Iteration 4165: loss -2.1103105545043945\n",
      "Iteration 4166: loss -1.9798333644866943\n",
      "Iteration 4167: loss -2.1002190113067627\n",
      "Iteration 4168: loss -2.15136981010437\n",
      "Iteration 4169: loss -2.240567207336426\n",
      "Iteration 4170: loss -2.1412651538848877\n",
      "Iteration 4171: loss -2.093045949935913\n",
      "Iteration 4172: loss -2.094003677368164\n",
      "Iteration 4173: loss -2.1031646728515625\n",
      "Iteration 4174: loss -2.2498762607574463\n",
      "Iteration 4175: loss -2.319730520248413\n",
      "Iteration 4176: loss -2.0875604152679443\n",
      "Iteration 4177: loss -2.1196701526641846\n",
      "Iteration 4178: loss -2.1418895721435547\n",
      "Iteration 4179: loss -1.9997152090072632\n",
      "Iteration 4180: loss -2.0543885231018066\n",
      "Iteration 4181: loss -2.02850604057312\n",
      "Iteration 4182: loss -2.0879335403442383\n",
      "Iteration 4183: loss -2.1249334812164307\n",
      "Iteration 4184: loss -2.305590867996216\n",
      "Iteration 4185: loss -2.077528715133667\n",
      "Iteration 4186: loss -2.1970627307891846\n",
      "Iteration 4187: loss -2.1213629245758057\n",
      "Iteration 4188: loss -1.9713175296783447\n",
      "Iteration 4189: loss -2.2972896099090576\n",
      "Iteration 4190: loss -1.8249413967132568\n",
      "Iteration 4191: loss -2.0038275718688965\n",
      "Iteration 4192: loss -2.288329839706421\n",
      "Iteration 4193: loss -2.006181240081787\n",
      "Iteration 4194: loss -2.106339931488037\n",
      "Iteration 4195: loss -2.1459579467773438\n",
      "Iteration 4196: loss -2.180976629257202\n",
      "Iteration 4197: loss -1.985937476158142\n",
      "Iteration 4198: loss -2.1162447929382324\n",
      "Iteration 4199: loss -2.19905948638916\n",
      "Iteration 4200: loss -2.1250054836273193\n",
      "Iteration 4201: loss -1.9520803689956665\n",
      "Iteration 4202: loss -2.1428723335266113\n",
      "Iteration 4203: loss -2.2403013706207275\n",
      "Iteration 4204: loss -2.0835161209106445\n",
      "Iteration 4205: loss -2.087629556655884\n",
      "Iteration 4206: loss -2.2109100818634033\n",
      "Iteration 4207: loss -2.067992925643921\n",
      "Iteration 4208: loss -2.0878396034240723\n",
      "Iteration 4209: loss -2.2973554134368896\n",
      "Iteration 4210: loss -2.1517388820648193\n",
      "Iteration 4211: loss -2.2610971927642822\n",
      "Iteration 4212: loss -2.1956498622894287\n",
      "Iteration 4213: loss -2.206178903579712\n",
      "Iteration 4214: loss -2.154404640197754\n",
      "Iteration 4215: loss -2.1849238872528076\n",
      "Iteration 4216: loss -2.1819615364074707\n",
      "Iteration 4217: loss -2.140469551086426\n",
      "Iteration 4218: loss -2.022641181945801\n",
      "Iteration 4219: loss -2.039332628250122\n",
      "Iteration 4220: loss -2.167123556137085\n",
      "Iteration 4221: loss -2.1675493717193604\n",
      "Iteration 4222: loss -2.2079765796661377\n",
      "Iteration 4223: loss -2.2061378955841064\n",
      "Iteration 4224: loss -2.1708083152770996\n",
      "Iteration 4225: loss -2.265110731124878\n",
      "Iteration 4226: loss -2.094252824783325\n",
      "Iteration 4227: loss -2.152649402618408\n",
      "Iteration 4228: loss -2.2080271244049072\n",
      "Iteration 4229: loss -2.1412336826324463\n",
      "Iteration 4230: loss -2.0383832454681396\n",
      "Iteration 4231: loss -2.265393018722534\n",
      "Iteration 4232: loss -2.1384923458099365\n",
      "Iteration 4233: loss -2.052431344985962\n",
      "Iteration 4234: loss -2.2513439655303955\n",
      "Iteration 4235: loss -2.1271932125091553\n",
      "Iteration 4236: loss -2.0922658443450928\n",
      "Iteration 4237: loss -2.068903684616089\n",
      "Iteration 4238: loss -2.082038640975952\n",
      "Iteration 4239: loss -2.1270265579223633\n",
      "Iteration 4240: loss -2.2065138816833496\n",
      "Iteration 4241: loss -2.2870306968688965\n",
      "Iteration 4242: loss -2.1600308418273926\n",
      "Iteration 4243: loss -2.0948212146759033\n",
      "Iteration 4244: loss -1.9693169593811035\n",
      "Iteration 4245: loss -2.079312801361084\n",
      "Iteration 4246: loss -2.137526273727417\n",
      "Iteration 4247: loss -2.1628775596618652\n",
      "Iteration 4248: loss -2.1538591384887695\n",
      "Iteration 4249: loss -1.9946136474609375\n",
      "Iteration 4250: loss -1.9760189056396484\n",
      "Iteration 4251: loss -2.142648458480835\n",
      "Iteration 4252: loss -2.1029927730560303\n",
      "Iteration 4253: loss -2.0767767429351807\n",
      "Iteration 4254: loss -2.219602108001709\n",
      "Iteration 4255: loss -2.2310101985931396\n",
      "Iteration 4256: loss -2.1059956550598145\n",
      "Iteration 4257: loss -2.1094772815704346\n",
      "Iteration 4258: loss -2.0533108711242676\n",
      "Iteration 4259: loss -2.1232352256774902\n",
      "Iteration 4260: loss -2.0938425064086914\n",
      "Iteration 4261: loss -2.2040860652923584\n",
      "Iteration 4262: loss -2.202899694442749\n",
      "Iteration 4263: loss -2.2108519077301025\n",
      "Iteration 4264: loss -2.069018602371216\n",
      "Iteration 4265: loss -2.1662862300872803\n",
      "Iteration 4266: loss -2.1353771686553955\n",
      "Iteration 4267: loss -1.9935195446014404\n",
      "Iteration 4268: loss -2.3307888507843018\n",
      "Iteration 4269: loss -2.1404950618743896\n",
      "Iteration 4270: loss -2.1629178524017334\n",
      "Iteration 4271: loss -2.067009687423706\n",
      "Iteration 4272: loss -2.0838582515716553\n",
      "Iteration 4273: loss -2.2047460079193115\n",
      "Iteration 4274: loss -2.226987838745117\n",
      "Iteration 4275: loss -2.2099344730377197\n",
      "Iteration 4276: loss -2.172086238861084\n",
      "Iteration 4277: loss -2.145799398422241\n",
      "Iteration 4278: loss -2.2835583686828613\n",
      "Iteration 4279: loss -2.256606340408325\n",
      "Iteration 4280: loss -2.1630196571350098\n",
      "Iteration 4281: loss -2.163257360458374\n",
      "Iteration 4282: loss -2.077924966812134\n",
      "Iteration 4283: loss -2.0794146060943604\n",
      "Iteration 4284: loss -2.1627793312072754\n",
      "Iteration 4285: loss -2.2110066413879395\n",
      "Iteration 4286: loss -2.173262357711792\n",
      "Iteration 4287: loss -2.039973497390747\n",
      "Iteration 4288: loss -2.179863929748535\n",
      "Iteration 4289: loss -2.0391745567321777\n",
      "Iteration 4290: loss -2.3112051486968994\n",
      "Iteration 4291: loss -2.1014785766601562\n",
      "Iteration 4292: loss -2.2896881103515625\n",
      "Iteration 4293: loss -2.1251721382141113\n",
      "Iteration 4294: loss -2.2072720527648926\n",
      "Iteration 4295: loss -2.0408871173858643\n",
      "Iteration 4296: loss -2.1788012981414795\n",
      "Iteration 4297: loss -2.1683690547943115\n",
      "Iteration 4298: loss -2.136199712753296\n",
      "Iteration 4299: loss -2.1628830432891846\n",
      "Iteration 4300: loss -2.1303224563598633\n",
      "Iteration 4301: loss -2.272416591644287\n",
      "Iteration 4302: loss -2.1522624492645264\n",
      "Iteration 4303: loss -2.094067096710205\n",
      "Iteration 4304: loss -2.281083106994629\n",
      "Iteration 4305: loss -2.1087775230407715\n",
      "Iteration 4306: loss -2.2518320083618164\n",
      "Iteration 4307: loss -2.0387866497039795\n",
      "Iteration 4308: loss -2.2318270206451416\n",
      "Iteration 4309: loss -2.074491500854492\n",
      "Iteration 4310: loss -2.190833330154419\n",
      "Iteration 4311: loss -1.9534045457839966\n",
      "Iteration 4312: loss -1.9820494651794434\n",
      "Iteration 4313: loss -2.2100062370300293\n",
      "Iteration 4314: loss -2.0249249935150146\n",
      "Iteration 4315: loss -2.0835936069488525\n",
      "Iteration 4316: loss -2.0323991775512695\n",
      "Iteration 4317: loss -2.047227382659912\n",
      "Iteration 4318: loss -2.1491124629974365\n",
      "Iteration 4319: loss -2.091705799102783\n",
      "Iteration 4320: loss -1.9632381200790405\n",
      "Iteration 4321: loss -2.139272451400757\n",
      "Iteration 4322: loss -2.080880641937256\n",
      "Iteration 4323: loss -2.2133188247680664\n",
      "Iteration 4324: loss -2.28987193107605\n",
      "Iteration 4325: loss -2.1294753551483154\n",
      "Iteration 4326: loss -2.0089962482452393\n",
      "Iteration 4327: loss -2.117417812347412\n",
      "Iteration 4328: loss -1.9405263662338257\n",
      "Iteration 4329: loss -2.1212594509124756\n",
      "Iteration 4330: loss -2.0004613399505615\n",
      "Iteration 4331: loss -2.0230374336242676\n",
      "Iteration 4332: loss -2.1614668369293213\n",
      "Iteration 4333: loss -2.2238147258758545\n",
      "Iteration 4334: loss -2.148784637451172\n",
      "Iteration 4335: loss -2.0987844467163086\n",
      "Iteration 4336: loss -2.1786305904388428\n",
      "Iteration 4337: loss -2.1658716201782227\n",
      "Iteration 4338: loss -2.0654661655426025\n",
      "Iteration 4339: loss -2.2557268142700195\n",
      "Iteration 4340: loss -2.084777593612671\n",
      "Iteration 4341: loss -1.9957244396209717\n",
      "Iteration 4342: loss -2.1624596118927\n",
      "Iteration 4343: loss -2.0078554153442383\n",
      "Iteration 4344: loss -2.1206045150756836\n",
      "Iteration 4345: loss -1.9595431089401245\n",
      "Iteration 4346: loss -2.005887508392334\n",
      "Iteration 4347: loss -2.159122943878174\n",
      "Iteration 4348: loss -1.9517431259155273\n",
      "Iteration 4349: loss -2.041029930114746\n",
      "Iteration 4350: loss -2.2150137424468994\n",
      "Iteration 4351: loss -2.281512975692749\n",
      "Iteration 4352: loss -2.115262269973755\n",
      "Iteration 4353: loss -1.9708086252212524\n",
      "Iteration 4354: loss -2.1472063064575195\n",
      "Iteration 4355: loss -2.103461503982544\n",
      "Iteration 4356: loss -2.2609877586364746\n",
      "Iteration 4357: loss -2.0537915229797363\n",
      "Iteration 4358: loss -1.9323397874832153\n",
      "Iteration 4359: loss -1.9942468404769897\n",
      "Iteration 4360: loss -2.0791354179382324\n",
      "Iteration 4361: loss -2.12211012840271\n",
      "Iteration 4362: loss -2.0902700424194336\n",
      "Iteration 4363: loss -2.140763282775879\n",
      "Iteration 4364: loss -2.0734450817108154\n",
      "Iteration 4365: loss -2.2361016273498535\n",
      "Iteration 4366: loss -2.119832992553711\n",
      "Iteration 4367: loss -2.3739938735961914\n",
      "Iteration 4368: loss -2.2461471557617188\n",
      "Iteration 4369: loss -2.039630174636841\n",
      "Iteration 4370: loss -2.147064685821533\n",
      "Iteration 4371: loss -2.2319939136505127\n",
      "Iteration 4372: loss -2.0298280715942383\n",
      "Iteration 4373: loss -2.2970988750457764\n",
      "Iteration 4374: loss -2.1356728076934814\n",
      "Iteration 4375: loss -2.20156192779541\n",
      "Iteration 4376: loss -2.1885528564453125\n",
      "Iteration 4377: loss -2.1116719245910645\n",
      "Iteration 4378: loss -2.2769463062286377\n",
      "Iteration 4379: loss -2.135674476623535\n",
      "Iteration 4380: loss -2.1359753608703613\n",
      "Iteration 4381: loss -2.1456520557403564\n",
      "Iteration 4382: loss -2.1457550525665283\n",
      "Iteration 4383: loss -2.231903553009033\n",
      "Iteration 4384: loss -2.06058931350708\n",
      "Iteration 4385: loss -2.148974657058716\n",
      "Iteration 4386: loss -2.130209445953369\n",
      "Iteration 4387: loss -2.1906566619873047\n",
      "Iteration 4388: loss -2.2138259410858154\n",
      "Iteration 4389: loss -2.142971992492676\n",
      "Iteration 4390: loss -2.152078866958618\n",
      "Iteration 4391: loss -1.9792253971099854\n",
      "Iteration 4392: loss -2.1331288814544678\n",
      "Iteration 4393: loss -2.3137738704681396\n",
      "Iteration 4394: loss -2.067507743835449\n",
      "Iteration 4395: loss -2.13736891746521\n",
      "Iteration 4396: loss -2.1577212810516357\n",
      "Iteration 4397: loss -2.295992136001587\n",
      "Iteration 4398: loss -2.18607497215271\n",
      "Iteration 4399: loss -2.2686071395874023\n",
      "Iteration 4400: loss -2.153433084487915\n",
      "Iteration 4401: loss -2.264209032058716\n",
      "Iteration 4402: loss -2.155787944793701\n",
      "Iteration 4403: loss -2.1494412422180176\n",
      "Iteration 4404: loss -2.2450037002563477\n",
      "Iteration 4405: loss -2.088665246963501\n",
      "Iteration 4406: loss -2.2380752563476562\n",
      "Iteration 4407: loss -2.205979108810425\n",
      "Iteration 4408: loss -2.0719001293182373\n",
      "Iteration 4409: loss -2.146836280822754\n",
      "Iteration 4410: loss -2.0986199378967285\n",
      "Iteration 4411: loss -2.090108633041382\n",
      "Iteration 4412: loss -2.1036758422851562\n",
      "Iteration 4413: loss -2.059842109680176\n",
      "Iteration 4414: loss -2.0182650089263916\n",
      "Iteration 4415: loss -2.3122239112854004\n",
      "Iteration 4416: loss -1.9272024631500244\n",
      "Iteration 4417: loss -2.2021782398223877\n",
      "Iteration 4418: loss -2.22873854637146\n",
      "Iteration 4419: loss -2.262620449066162\n",
      "Iteration 4420: loss -2.1513099670410156\n",
      "Iteration 4421: loss -2.181427001953125\n",
      "Iteration 4422: loss -2.0607008934020996\n",
      "Iteration 4423: loss -2.0028438568115234\n",
      "Iteration 4424: loss -2.187621831893921\n",
      "Iteration 4425: loss -2.0462236404418945\n",
      "Iteration 4426: loss -2.1447153091430664\n",
      "Iteration 4427: loss -2.175079107284546\n",
      "Iteration 4428: loss -2.1734631061553955\n",
      "Iteration 4429: loss -2.3063316345214844\n",
      "Iteration 4430: loss -2.1038522720336914\n",
      "Iteration 4431: loss -2.1520378589630127\n",
      "Iteration 4432: loss -1.9967467784881592\n",
      "Iteration 4433: loss -2.1527998447418213\n",
      "Iteration 4434: loss -2.009415864944458\n",
      "Iteration 4435: loss -2.2023723125457764\n",
      "Iteration 4436: loss -2.0992281436920166\n",
      "Iteration 4437: loss -2.120718002319336\n",
      "Iteration 4438: loss -2.0995543003082275\n",
      "Iteration 4439: loss -2.1003310680389404\n",
      "Iteration 4440: loss -1.9575985670089722\n",
      "Iteration 4441: loss -2.1205689907073975\n",
      "Iteration 4442: loss -2.1008460521698\n",
      "Iteration 4443: loss -2.210448980331421\n",
      "Iteration 4444: loss -2.180469512939453\n",
      "Iteration 4445: loss -2.1986961364746094\n",
      "Iteration 4446: loss -2.0332067012786865\n",
      "Iteration 4447: loss -2.0798449516296387\n",
      "Iteration 4448: loss -2.220085859298706\n",
      "Iteration 4449: loss -2.155839204788208\n",
      "Iteration 4450: loss -2.122222900390625\n",
      "Iteration 4451: loss -2.2846546173095703\n",
      "Iteration 4452: loss -2.078486680984497\n",
      "Iteration 4453: loss -1.959147334098816\n",
      "Iteration 4454: loss -2.1046156883239746\n",
      "Iteration 4455: loss -1.941887617111206\n",
      "Iteration 4456: loss -1.8930810689926147\n",
      "Iteration 4457: loss -2.2634921073913574\n",
      "Iteration 4458: loss -2.1657235622406006\n",
      "Iteration 4459: loss -2.145944833755493\n",
      "Iteration 4460: loss -2.148798704147339\n",
      "Iteration 4461: loss -2.169159412384033\n",
      "Iteration 4462: loss -2.182790756225586\n",
      "Iteration 4463: loss -2.295358180999756\n",
      "Iteration 4464: loss -2.162015438079834\n",
      "Iteration 4465: loss -2.028650999069214\n",
      "Iteration 4466: loss -2.219911813735962\n",
      "Iteration 4467: loss -2.15175724029541\n",
      "Iteration 4468: loss -2.2948858737945557\n",
      "Iteration 4469: loss -2.1343741416931152\n",
      "Iteration 4470: loss -2.0944857597351074\n",
      "Iteration 4471: loss -2.0400917530059814\n",
      "Iteration 4472: loss -2.236340045928955\n",
      "Iteration 4473: loss -2.3032422065734863\n",
      "Iteration 4474: loss -2.2444398403167725\n",
      "Iteration 4475: loss -2.3306243419647217\n",
      "Iteration 4476: loss -2.0051259994506836\n",
      "Iteration 4477: loss -2.140840768814087\n",
      "Iteration 4478: loss -2.0857162475585938\n",
      "Iteration 4479: loss -2.08573055267334\n",
      "Iteration 4480: loss -2.268855094909668\n",
      "Iteration 4481: loss -1.9780300855636597\n",
      "Iteration 4482: loss -2.0980026721954346\n",
      "Iteration 4483: loss -2.3096585273742676\n",
      "Iteration 4484: loss -2.085644483566284\n",
      "Iteration 4485: loss -2.1010630130767822\n",
      "Iteration 4486: loss -2.1644163131713867\n",
      "Iteration 4487: loss -2.1019952297210693\n",
      "Iteration 4488: loss -2.1072120666503906\n",
      "Iteration 4489: loss -2.2108442783355713\n",
      "Iteration 4490: loss -2.258481740951538\n",
      "Iteration 4491: loss -2.2114980220794678\n",
      "Iteration 4492: loss -2.1418607234954834\n",
      "Iteration 4493: loss -2.3469977378845215\n",
      "Iteration 4494: loss -2.0171844959259033\n",
      "Iteration 4495: loss -2.0133023262023926\n",
      "Iteration 4496: loss -2.147153615951538\n",
      "Iteration 4497: loss -2.0391979217529297\n",
      "Iteration 4498: loss -2.2205543518066406\n",
      "Iteration 4499: loss -2.045724630355835\n",
      "Iteration 4500: loss -2.2194230556488037\n",
      "Iteration 4501: loss -2.1941113471984863\n",
      "Iteration 4502: loss -2.220104217529297\n",
      "Iteration 4503: loss -2.13352370262146\n",
      "Iteration 4504: loss -1.8705618381500244\n",
      "Iteration 4505: loss -2.0651979446411133\n",
      "Iteration 4506: loss -2.0252881050109863\n",
      "Iteration 4507: loss -1.8549652099609375\n",
      "Iteration 4508: loss -2.1858012676239014\n",
      "Iteration 4509: loss -2.0925402641296387\n",
      "Iteration 4510: loss -1.9153764247894287\n",
      "Iteration 4511: loss -2.240560531616211\n",
      "Iteration 4512: loss -1.9959803819656372\n",
      "Iteration 4513: loss -2.119932174682617\n",
      "Iteration 4514: loss -2.286444664001465\n",
      "Iteration 4515: loss -2.3316547870635986\n",
      "Iteration 4516: loss -2.1844000816345215\n",
      "Iteration 4517: loss -2.0720138549804688\n",
      "Iteration 4518: loss -2.3238437175750732\n",
      "Iteration 4519: loss -2.0819387435913086\n",
      "Iteration 4520: loss -2.172386407852173\n",
      "Iteration 4521: loss -2.1956567764282227\n",
      "Iteration 4522: loss -2.1197476387023926\n",
      "Iteration 4523: loss -2.199341297149658\n",
      "Iteration 4524: loss -2.19089937210083\n",
      "Iteration 4525: loss -2.2139148712158203\n",
      "Iteration 4526: loss -2.15756893157959\n",
      "Iteration 4527: loss -2.2080421447753906\n",
      "Iteration 4528: loss -2.1470847129821777\n",
      "Iteration 4529: loss -2.076273202896118\n",
      "Iteration 4530: loss -1.99661386013031\n",
      "Iteration 4531: loss -2.2676610946655273\n",
      "Iteration 4532: loss -2.2006328105926514\n",
      "Iteration 4533: loss -2.1480607986450195\n",
      "Iteration 4534: loss -2.2296512126922607\n",
      "Iteration 4535: loss -2.088413953781128\n",
      "Iteration 4536: loss -2.1135945320129395\n",
      "Iteration 4537: loss -2.1467373371124268\n",
      "Iteration 4538: loss -1.9719210863113403\n",
      "Iteration 4539: loss -2.0957581996917725\n",
      "Iteration 4540: loss -2.092576742172241\n",
      "Iteration 4541: loss -2.1431164741516113\n",
      "Iteration 4542: loss -2.0797553062438965\n",
      "Iteration 4543: loss -2.183346748352051\n",
      "Iteration 4544: loss -2.2022531032562256\n",
      "Iteration 4545: loss -2.245422840118408\n",
      "Iteration 4546: loss -2.0311994552612305\n",
      "Iteration 4547: loss -2.10438871383667\n",
      "Iteration 4548: loss -1.9845763444900513\n",
      "Iteration 4549: loss -2.0127127170562744\n",
      "Iteration 4550: loss -2.1786763668060303\n",
      "Iteration 4551: loss -2.1981899738311768\n",
      "Iteration 4552: loss -2.1654160022735596\n",
      "Iteration 4553: loss -2.0943307876586914\n",
      "Iteration 4554: loss -2.0719122886657715\n",
      "Iteration 4555: loss -2.200908660888672\n",
      "Iteration 4556: loss -1.9955865144729614\n",
      "Iteration 4557: loss -2.1102981567382812\n",
      "Iteration 4558: loss -2.031050443649292\n",
      "Iteration 4559: loss -1.971365213394165\n",
      "Iteration 4560: loss -2.011195421218872\n",
      "Iteration 4561: loss -1.8770115375518799\n",
      "Iteration 4562: loss -2.122271776199341\n",
      "Iteration 4563: loss -2.068618059158325\n",
      "Iteration 4564: loss -1.9488863945007324\n",
      "Iteration 4565: loss -2.0568182468414307\n",
      "Iteration 4566: loss -2.125370502471924\n",
      "Iteration 4567: loss -2.1830317974090576\n",
      "Iteration 4568: loss -2.005390167236328\n",
      "Iteration 4569: loss -2.0583908557891846\n",
      "Iteration 4570: loss -2.096334457397461\n",
      "Iteration 4571: loss -2.0962839126586914\n",
      "Iteration 4572: loss -2.028690814971924\n",
      "Iteration 4573: loss -2.1374940872192383\n",
      "Iteration 4574: loss -2.1375181674957275\n",
      "Iteration 4575: loss -2.0478808879852295\n",
      "Iteration 4576: loss -2.157266139984131\n",
      "Iteration 4577: loss -2.326101064682007\n",
      "Iteration 4578: loss -2.1680219173431396\n",
      "Iteration 4579: loss -2.137122392654419\n",
      "Iteration 4580: loss -2.26700758934021\n",
      "Iteration 4581: loss -2.213627815246582\n",
      "Iteration 4582: loss -2.101097345352173\n",
      "Iteration 4583: loss -2.1193010807037354\n",
      "Iteration 4584: loss -2.222214698791504\n",
      "Iteration 4585: loss -2.1997647285461426\n",
      "Iteration 4586: loss -2.0749731063842773\n",
      "Iteration 4587: loss -2.1682708263397217\n",
      "Iteration 4588: loss -2.0660173892974854\n",
      "Iteration 4589: loss -2.153428554534912\n",
      "Iteration 4590: loss -2.267188787460327\n",
      "Iteration 4591: loss -1.9128328561782837\n",
      "Iteration 4592: loss -2.108006000518799\n",
      "Iteration 4593: loss -2.0513694286346436\n",
      "Iteration 4594: loss -2.1441125869750977\n",
      "Iteration 4595: loss -2.162626266479492\n",
      "Iteration 4596: loss -2.1038994789123535\n",
      "Iteration 4597: loss -2.1036925315856934\n",
      "Iteration 4598: loss -1.9862133264541626\n",
      "Iteration 4599: loss -2.1078250408172607\n",
      "Iteration 4600: loss -2.0351996421813965\n",
      "Iteration 4601: loss -2.238985300064087\n",
      "Iteration 4602: loss -2.176445960998535\n",
      "Iteration 4603: loss -2.0986671447753906\n",
      "Iteration 4604: loss -2.1488187313079834\n",
      "Iteration 4605: loss -2.0944080352783203\n",
      "Iteration 4606: loss -2.1855530738830566\n",
      "Iteration 4607: loss -2.1544189453125\n",
      "Iteration 4608: loss -2.1741960048675537\n",
      "Iteration 4609: loss -2.1551547050476074\n",
      "Iteration 4610: loss -2.090989112854004\n",
      "Iteration 4611: loss -2.1198806762695312\n",
      "Iteration 4612: loss -2.1631152629852295\n",
      "Iteration 4613: loss -2.208183765411377\n",
      "Iteration 4614: loss -2.1475584506988525\n",
      "Iteration 4615: loss -1.9314419031143188\n",
      "Iteration 4616: loss -2.152716636657715\n",
      "Iteration 4617: loss -2.12925124168396\n",
      "Iteration 4618: loss -2.208904981613159\n",
      "Iteration 4619: loss -2.1431448459625244\n",
      "Iteration 4620: loss -2.2011678218841553\n",
      "Iteration 4621: loss -2.2287137508392334\n",
      "Iteration 4622: loss -1.9684242010116577\n",
      "Iteration 4623: loss -2.2600395679473877\n",
      "Iteration 4624: loss -2.1724205017089844\n",
      "Iteration 4625: loss -2.1792802810668945\n",
      "Iteration 4626: loss -2.0542242527008057\n",
      "Iteration 4627: loss -2.041673183441162\n",
      "Iteration 4628: loss -2.1709675788879395\n",
      "Iteration 4629: loss -2.0080783367156982\n",
      "Iteration 4630: loss -2.0599687099456787\n",
      "Iteration 4631: loss -1.8967821598052979\n",
      "Iteration 4632: loss -2.1369550228118896\n",
      "Iteration 4633: loss -2.2232322692871094\n",
      "Iteration 4634: loss -1.9148693084716797\n",
      "Iteration 4635: loss -1.9699300527572632\n",
      "Iteration 4636: loss -2.026121139526367\n",
      "Iteration 4637: loss -2.0648627281188965\n",
      "Iteration 4638: loss -1.930578589439392\n",
      "Iteration 4639: loss -2.224140167236328\n",
      "Iteration 4640: loss -1.8946901559829712\n",
      "Iteration 4641: loss -2.1148452758789062\n",
      "Iteration 4642: loss -2.242335557937622\n",
      "Iteration 4643: loss -2.231369972229004\n",
      "Iteration 4644: loss -2.2151145935058594\n",
      "Iteration 4645: loss -1.8883249759674072\n",
      "Iteration 4646: loss -2.0396862030029297\n",
      "Iteration 4647: loss -2.180943012237549\n",
      "Iteration 4648: loss -2.022789478302002\n",
      "Iteration 4649: loss -2.111757516860962\n",
      "Iteration 4650: loss -2.2033259868621826\n",
      "Iteration 4651: loss -2.088045597076416\n",
      "Iteration 4652: loss -2.0226542949676514\n",
      "Iteration 4653: loss -2.1707098484039307\n",
      "Iteration 4654: loss -2.1253273487091064\n",
      "Iteration 4655: loss -2.0647971630096436\n",
      "Iteration 4656: loss -2.0382161140441895\n",
      "Iteration 4657: loss -2.13037371635437\n",
      "Iteration 4658: loss -2.0362789630889893\n",
      "Iteration 4659: loss -1.9658832550048828\n",
      "Iteration 4660: loss -2.218003988265991\n",
      "Iteration 4661: loss -2.139206647872925\n",
      "Iteration 4662: loss -1.9861016273498535\n",
      "Iteration 4663: loss -2.1149771213531494\n",
      "Iteration 4664: loss -2.04490065574646\n",
      "Iteration 4665: loss -2.244119167327881\n",
      "Iteration 4666: loss -1.9792150259017944\n",
      "Iteration 4667: loss -2.1238417625427246\n",
      "Iteration 4668: loss -2.12312388420105\n",
      "Iteration 4669: loss -2.1557157039642334\n",
      "Iteration 4670: loss -2.150230884552002\n",
      "Iteration 4671: loss -2.0756747722625732\n",
      "Iteration 4672: loss -2.2176265716552734\n",
      "Iteration 4673: loss -2.11851167678833\n",
      "Iteration 4674: loss -2.1710195541381836\n",
      "Iteration 4675: loss -2.142484426498413\n",
      "Iteration 4676: loss -2.0825300216674805\n",
      "Iteration 4677: loss -2.1554250717163086\n",
      "Iteration 4678: loss -2.013650417327881\n",
      "Iteration 4679: loss -2.012754201889038\n",
      "Iteration 4680: loss -2.228480339050293\n",
      "Iteration 4681: loss -1.993875503540039\n",
      "Iteration 4682: loss -1.9421027898788452\n",
      "Iteration 4683: loss -2.124753475189209\n",
      "Iteration 4684: loss -2.057716131210327\n",
      "Iteration 4685: loss -2.081939458847046\n",
      "Iteration 4686: loss -2.1018261909484863\n",
      "Iteration 4687: loss -2.070902109146118\n",
      "Iteration 4688: loss -2.193795680999756\n",
      "Iteration 4689: loss -2.2798237800598145\n",
      "Iteration 4690: loss -2.027759552001953\n",
      "Iteration 4691: loss -2.0340797901153564\n",
      "Iteration 4692: loss -1.9871546030044556\n",
      "Iteration 4693: loss -2.1822752952575684\n",
      "Iteration 4694: loss -2.160248279571533\n",
      "Iteration 4695: loss -2.2918243408203125\n",
      "Iteration 4696: loss -2.12923264503479\n",
      "Iteration 4697: loss -2.1193559169769287\n",
      "Iteration 4698: loss -2.119231700897217\n",
      "Iteration 4699: loss -2.106717109680176\n",
      "Iteration 4700: loss -2.0962517261505127\n",
      "Iteration 4701: loss -2.007050037384033\n",
      "Iteration 4702: loss -2.18939208984375\n",
      "Iteration 4703: loss -2.08133602142334\n",
      "Iteration 4704: loss -2.206915855407715\n",
      "Iteration 4705: loss -2.260585308074951\n",
      "Iteration 4706: loss -2.3190526962280273\n",
      "Iteration 4707: loss -2.2701940536499023\n",
      "Iteration 4708: loss -2.3732168674468994\n",
      "Iteration 4709: loss -2.276057481765747\n",
      "Iteration 4710: loss -2.1196160316467285\n",
      "Iteration 4711: loss -2.051072359085083\n",
      "Iteration 4712: loss -2.2446041107177734\n",
      "Iteration 4713: loss -2.209977626800537\n",
      "Iteration 4714: loss -2.0372612476348877\n",
      "Iteration 4715: loss -2.063887357711792\n",
      "Iteration 4716: loss -2.1052446365356445\n",
      "Iteration 4717: loss -2.006272792816162\n",
      "Iteration 4718: loss -2.2237143516540527\n",
      "Iteration 4719: loss -2.2074649333953857\n",
      "Iteration 4720: loss -2.2338333129882812\n",
      "Iteration 4721: loss -2.155963897705078\n",
      "Iteration 4722: loss -2.101926326751709\n",
      "Iteration 4723: loss -2.143127679824829\n",
      "Iteration 4724: loss -2.2822062969207764\n",
      "Iteration 4725: loss -2.03633451461792\n",
      "Iteration 4726: loss -2.139524221420288\n",
      "Iteration 4727: loss -2.0224616527557373\n",
      "Iteration 4728: loss -2.203829050064087\n",
      "Iteration 4729: loss -2.168698310852051\n",
      "Iteration 4730: loss -2.1826884746551514\n",
      "Iteration 4731: loss -2.016415596008301\n",
      "Iteration 4732: loss -2.129106044769287\n",
      "Iteration 4733: loss -2.161750078201294\n",
      "Iteration 4734: loss -2.0212178230285645\n",
      "Iteration 4735: loss -2.1291468143463135\n",
      "Iteration 4736: loss -2.110968589782715\n",
      "Iteration 4737: loss -2.0961108207702637\n",
      "Iteration 4738: loss -1.99639892578125\n",
      "Iteration 4739: loss -2.1494836807250977\n",
      "Iteration 4740: loss -2.053119421005249\n",
      "Iteration 4741: loss -2.2203152179718018\n",
      "Iteration 4742: loss -2.143850803375244\n",
      "Iteration 4743: loss -2.0902249813079834\n",
      "Iteration 4744: loss -2.270796537399292\n",
      "Iteration 4745: loss -2.1512413024902344\n",
      "Iteration 4746: loss -2.121739387512207\n",
      "Iteration 4747: loss -2.1587207317352295\n",
      "Iteration 4748: loss -2.03800368309021\n",
      "Iteration 4749: loss -2.124640703201294\n",
      "Iteration 4750: loss -2.0353147983551025\n",
      "Iteration 4751: loss -2.265671968460083\n",
      "Iteration 4752: loss -2.226471424102783\n",
      "Iteration 4753: loss -2.153663158416748\n",
      "Iteration 4754: loss -2.2430434226989746\n",
      "Iteration 4755: loss -2.1506385803222656\n",
      "Iteration 4756: loss -2.1947553157806396\n",
      "Iteration 4757: loss -2.368196964263916\n",
      "Iteration 4758: loss -2.0055055618286133\n",
      "Iteration 4759: loss -2.141019582748413\n",
      "Iteration 4760: loss -2.225358247756958\n",
      "Iteration 4761: loss -2.0366439819335938\n",
      "Iteration 4762: loss -2.2085089683532715\n",
      "Iteration 4763: loss -2.1576552391052246\n",
      "Iteration 4764: loss -2.0926239490509033\n",
      "Iteration 4765: loss -2.105416774749756\n",
      "Iteration 4766: loss -2.1381633281707764\n",
      "Iteration 4767: loss -2.2028801441192627\n",
      "Iteration 4768: loss -2.229752540588379\n",
      "Iteration 4769: loss -2.182241439819336\n",
      "Iteration 4770: loss -2.1429994106292725\n",
      "Iteration 4771: loss -2.1175007820129395\n",
      "Iteration 4772: loss -2.2472238540649414\n",
      "Iteration 4773: loss -2.1975772380828857\n",
      "Iteration 4774: loss -2.2243292331695557\n",
      "Iteration 4775: loss -2.089109420776367\n",
      "Iteration 4776: loss -2.0701005458831787\n",
      "Iteration 4777: loss -1.9117919206619263\n",
      "Iteration 4778: loss -2.282102584838867\n",
      "Iteration 4779: loss -2.258476734161377\n",
      "Iteration 4780: loss -2.2499799728393555\n",
      "Iteration 4781: loss -2.177496910095215\n",
      "Iteration 4782: loss -2.073582410812378\n",
      "Iteration 4783: loss -2.1547279357910156\n",
      "Iteration 4784: loss -2.241483449935913\n",
      "Iteration 4785: loss -2.1205155849456787\n",
      "Iteration 4786: loss -2.194235324859619\n",
      "Iteration 4787: loss -2.125333309173584\n",
      "Iteration 4788: loss -2.116962194442749\n",
      "Iteration 4789: loss -2.213505744934082\n",
      "Iteration 4790: loss -2.118549346923828\n",
      "Iteration 4791: loss -2.31512713432312\n",
      "Iteration 4792: loss -2.119523525238037\n",
      "Iteration 4793: loss -2.1916747093200684\n",
      "Iteration 4794: loss -2.146228313446045\n",
      "Iteration 4795: loss -2.056309700012207\n",
      "Iteration 4796: loss -2.2761266231536865\n",
      "Iteration 4797: loss -2.1498358249664307\n",
      "Iteration 4798: loss -2.053483724594116\n",
      "Iteration 4799: loss -2.102071762084961\n",
      "Iteration 4800: loss -2.2257678508758545\n",
      "Iteration 4801: loss -2.16986346244812\n",
      "Iteration 4802: loss -2.2066915035247803\n",
      "Iteration 4803: loss -2.1916732788085938\n",
      "Iteration 4804: loss -2.063837766647339\n",
      "Iteration 4805: loss -2.010787010192871\n",
      "Iteration 4806: loss -2.2888307571411133\n",
      "Iteration 4807: loss -1.9478496313095093\n",
      "Iteration 4808: loss -2.3136024475097656\n",
      "Iteration 4809: loss -2.2323145866394043\n",
      "Iteration 4810: loss -2.326080799102783\n",
      "Iteration 4811: loss -2.0023176670074463\n",
      "Iteration 4812: loss -2.2028162479400635\n",
      "Iteration 4813: loss -2.0784335136413574\n",
      "Iteration 4814: loss -2.0772948265075684\n",
      "Iteration 4815: loss -2.2030081748962402\n",
      "Iteration 4816: loss -2.087752342224121\n",
      "Iteration 4817: loss -2.1017794609069824\n",
      "Iteration 4818: loss -2.17832612991333\n",
      "Iteration 4819: loss -1.9865859746932983\n",
      "Iteration 4820: loss -2.1784753799438477\n",
      "Iteration 4821: loss -2.1439757347106934\n",
      "Iteration 4822: loss -2.063580274581909\n",
      "Iteration 4823: loss -2.1290547847747803\n",
      "Iteration 4824: loss -2.1027512550354004\n",
      "Iteration 4825: loss -2.090646982192993\n",
      "Iteration 4826: loss -2.07694935798645\n",
      "Iteration 4827: loss -2.2863998413085938\n",
      "Iteration 4828: loss -2.10616397857666\n",
      "Iteration 4829: loss -2.067535877227783\n",
      "Iteration 4830: loss -2.1491713523864746\n",
      "Iteration 4831: loss -2.106428623199463\n",
      "Iteration 4832: loss -2.1643762588500977\n",
      "Iteration 4833: loss -2.115964412689209\n",
      "Iteration 4834: loss -2.199457883834839\n",
      "Iteration 4835: loss -1.9948376417160034\n",
      "Iteration 4836: loss -2.1668975353240967\n",
      "Iteration 4837: loss -2.186617374420166\n",
      "Iteration 4838: loss -2.2395401000976562\n",
      "Iteration 4839: loss -2.120534896850586\n",
      "Iteration 4840: loss -2.261277198791504\n",
      "Iteration 4841: loss -2.1408653259277344\n",
      "Iteration 4842: loss -2.3366482257843018\n",
      "Iteration 4843: loss -2.254640817642212\n",
      "Iteration 4844: loss -2.117197275161743\n",
      "Iteration 4845: loss -2.1521146297454834\n",
      "Iteration 4846: loss -2.082387924194336\n",
      "Iteration 4847: loss -2.229862689971924\n",
      "Iteration 4848: loss -2.0907466411590576\n",
      "Iteration 4849: loss -2.1454360485076904\n",
      "Iteration 4850: loss -2.120924711227417\n",
      "Iteration 4851: loss -2.164410352706909\n",
      "Iteration 4852: loss -2.2086944580078125\n",
      "Iteration 4853: loss -2.124222755432129\n",
      "Iteration 4854: loss -2.1888296604156494\n",
      "Iteration 4855: loss -2.1162400245666504\n",
      "Iteration 4856: loss -2.1891462802886963\n",
      "Iteration 4857: loss -2.047647476196289\n",
      "Iteration 4858: loss -2.1694581508636475\n",
      "Iteration 4859: loss -2.104691982269287\n",
      "Iteration 4860: loss -2.0833606719970703\n",
      "Iteration 4861: loss -2.103271245956421\n",
      "Iteration 4862: loss -2.1347060203552246\n",
      "Iteration 4863: loss -2.073241949081421\n",
      "Iteration 4864: loss -2.136270046234131\n",
      "Iteration 4865: loss -2.31921648979187\n",
      "Iteration 4866: loss -2.112668752670288\n",
      "Iteration 4867: loss -2.255314826965332\n",
      "Iteration 4868: loss -2.109971761703491\n",
      "Iteration 4869: loss -2.1643593311309814\n",
      "Iteration 4870: loss -2.1222167015075684\n",
      "Iteration 4871: loss -2.225527286529541\n",
      "Iteration 4872: loss -2.111733913421631\n",
      "Iteration 4873: loss -2.1829724311828613\n",
      "Iteration 4874: loss -2.1224727630615234\n",
      "Iteration 4875: loss -2.1683990955352783\n",
      "Iteration 4876: loss -2.1472110748291016\n",
      "Iteration 4877: loss -2.171832323074341\n",
      "Iteration 4878: loss -2.019751787185669\n",
      "Iteration 4879: loss -2.1002893447875977\n",
      "Iteration 4880: loss -2.1124751567840576\n",
      "Iteration 4881: loss -2.155428171157837\n",
      "Iteration 4882: loss -2.1338930130004883\n",
      "Iteration 4883: loss -2.2354931831359863\n",
      "Iteration 4884: loss -2.156970977783203\n",
      "Iteration 4885: loss -2.0724692344665527\n",
      "Iteration 4886: loss -2.127368927001953\n",
      "Iteration 4887: loss -2.0783190727233887\n",
      "Iteration 4888: loss -1.9915544986724854\n",
      "Iteration 4889: loss -2.2502965927124023\n",
      "Iteration 4890: loss -2.1464765071868896\n",
      "Iteration 4891: loss -2.138234853744507\n",
      "Iteration 4892: loss -2.166552782058716\n",
      "Iteration 4893: loss -2.173896551132202\n",
      "Iteration 4894: loss -2.1480307579040527\n",
      "Iteration 4895: loss -2.0166614055633545\n",
      "Iteration 4896: loss -2.0229218006134033\n",
      "Iteration 4897: loss -2.174710273742676\n",
      "Iteration 4898: loss -2.203300714492798\n",
      "Iteration 4899: loss -2.1101431846618652\n",
      "Iteration 4900: loss -2.0720012187957764\n",
      "Iteration 4901: loss -2.07684326171875\n",
      "Iteration 4902: loss -2.307196855545044\n",
      "Iteration 4903: loss -2.2622461318969727\n",
      "Iteration 4904: loss -2.2590761184692383\n",
      "Iteration 4905: loss -2.192747116088867\n",
      "Iteration 4906: loss -2.104396104812622\n",
      "Iteration 4907: loss -2.2590596675872803\n",
      "Iteration 4908: loss -2.17802357673645\n",
      "Iteration 4909: loss -2.2322702407836914\n",
      "Iteration 4910: loss -2.236173152923584\n",
      "Iteration 4911: loss -2.2223808765411377\n",
      "Iteration 4912: loss -2.325441837310791\n",
      "Iteration 4913: loss -2.2154481410980225\n",
      "Iteration 4914: loss -2.197183609008789\n",
      "Iteration 4915: loss -2.0637450218200684\n",
      "Iteration 4916: loss -2.090531587600708\n",
      "Iteration 4917: loss -2.233956813812256\n",
      "Iteration 4918: loss -2.1955974102020264\n",
      "Iteration 4919: loss -1.9451382160186768\n",
      "Iteration 4920: loss -1.885274887084961\n",
      "Iteration 4921: loss -1.9292702674865723\n",
      "Iteration 4922: loss -2.1175448894500732\n",
      "Iteration 4923: loss -1.7595460414886475\n",
      "Iteration 4924: loss -1.6506489515304565\n",
      "Iteration 4925: loss -2.0184521675109863\n",
      "Iteration 4926: loss -2.148113250732422\n",
      "Iteration 4927: loss -1.872139811515808\n",
      "Iteration 4928: loss -1.6482452154159546\n",
      "Iteration 4929: loss -2.1824228763580322\n",
      "Iteration 4930: loss -1.8566488027572632\n",
      "Iteration 4931: loss -1.5345197916030884\n",
      "Iteration 4932: loss -1.923830509185791\n",
      "Iteration 4933: loss -1.94698166847229\n",
      "Iteration 4934: loss -2.0286221504211426\n",
      "Iteration 4935: loss -1.753393530845642\n",
      "Iteration 4936: loss -1.7898555994033813\n",
      "Iteration 4937: loss -2.0921995639801025\n",
      "Iteration 4938: loss -2.095745325088501\n",
      "Iteration 4939: loss -1.908143162727356\n",
      "Iteration 4940: loss -1.9629186391830444\n",
      "Iteration 4941: loss -1.9642126560211182\n",
      "Iteration 4942: loss -1.9978790283203125\n",
      "Iteration 4943: loss -2.0694642066955566\n",
      "Iteration 4944: loss -1.9429259300231934\n",
      "Iteration 4945: loss -2.0594258308410645\n",
      "Iteration 4946: loss -2.1848297119140625\n",
      "Iteration 4947: loss -2.1864891052246094\n",
      "Iteration 4948: loss -2.156169891357422\n",
      "Iteration 4949: loss -1.9945464134216309\n",
      "Iteration 4950: loss -1.9686228036880493\n",
      "Iteration 4951: loss -2.0793206691741943\n",
      "Iteration 4952: loss -2.051835536956787\n",
      "Iteration 4953: loss -2.2697722911834717\n",
      "Iteration 4954: loss -2.181018352508545\n",
      "Iteration 4955: loss -2.1446828842163086\n",
      "Iteration 4956: loss -2.1600091457366943\n",
      "Iteration 4957: loss -2.223160982131958\n",
      "Iteration 4958: loss -2.277096748352051\n",
      "Iteration 4959: loss -2.17543888092041\n",
      "Iteration 4960: loss -2.1051385402679443\n",
      "Iteration 4961: loss -2.1162500381469727\n",
      "Iteration 4962: loss -2.033287525177002\n",
      "Iteration 4963: loss -2.21736741065979\n",
      "Iteration 4964: loss -2.274620532989502\n",
      "Iteration 4965: loss -2.233661651611328\n",
      "Iteration 4966: loss -1.9951809644699097\n",
      "Iteration 4967: loss -2.1042439937591553\n",
      "Iteration 4968: loss -2.165585517883301\n",
      "Iteration 4969: loss -2.2671072483062744\n",
      "Iteration 4970: loss -2.3399345874786377\n",
      "Iteration 4971: loss -2.2440755367279053\n",
      "Iteration 4972: loss -2.2116899490356445\n",
      "Iteration 4973: loss -2.088616371154785\n",
      "Iteration 4974: loss -2.268604040145874\n",
      "Iteration 4975: loss -1.9790982007980347\n",
      "Iteration 4976: loss -2.0040197372436523\n",
      "Iteration 4977: loss -2.2387290000915527\n",
      "Iteration 4978: loss -2.0084850788116455\n",
      "Iteration 4979: loss -2.227915048599243\n",
      "Iteration 4980: loss -2.0647385120391846\n",
      "Iteration 4981: loss -2.2995259761810303\n",
      "Iteration 4982: loss -2.0668540000915527\n",
      "Iteration 4983: loss -2.2914464473724365\n",
      "Iteration 4984: loss -2.313511848449707\n",
      "Iteration 4985: loss -2.1925790309906006\n",
      "Iteration 4986: loss -2.1573519706726074\n",
      "Iteration 4987: loss -1.8913906812667847\n",
      "Iteration 4988: loss -2.206232786178589\n",
      "Iteration 4989: loss -2.0093026161193848\n",
      "Iteration 4990: loss -2.1960110664367676\n",
      "Iteration 4991: loss -2.226184606552124\n",
      "Iteration 4992: loss -2.2193257808685303\n",
      "Iteration 4993: loss -2.1253437995910645\n",
      "Iteration 4994: loss -2.10056734085083\n",
      "Iteration 4995: loss -2.0291218757629395\n",
      "Iteration 4996: loss -2.0656073093414307\n",
      "Iteration 4997: loss -2.1364123821258545\n",
      "Iteration 4998: loss -2.070905923843384\n",
      "Iteration 4999: loss -1.9851053953170776\n"
     ]
    }
   ],
   "source": [
    "for j in range(5000):\n",
    "    theta, x = generate_data(mb_size, return_theta=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -1*encoder.log_prob(theta.to(device), x.to(device)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Iteration {}: loss {}'.format(j, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e767721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post(x, theta, **kwargs):\n",
    "    '''vectorized version of the above in theta for fixed x'''\n",
    "    assert theta.shape[1] == 1, \"not yet implemented for evaluation on multiple points at once\"\n",
    "    r_dist = kwargs['r_dist']\n",
    "    new1 = -1*torch.sum(theta, 1).abs()/math.sqrt(2)\n",
    "    new2 = (-1*theta[:,0])/math.sqrt(2)\n",
    "    new = torch.stack([new1, new2]).T\n",
    "    p = x-new\n",
    "    print(p)\n",
    "    u = p[:,0]-.25\n",
    "    v = p[:,1]\n",
    "    r = torch.sqrt(u ** 2 + v ** 2)  # note the angle distribution is uniform\n",
    "    to_adjust = r_dist.log_prob(r)\n",
    "    adjusted = torch.where(u < 0.0, -torch.inf, to_adjust.double())\n",
    "    return adjusted\n",
    "\n",
    "# Code to plot the true posterior density\n",
    "def plot(j, x, encoder, **kwargs):\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    # Plot exact density\n",
    "    eval_pts = torch.arange(-1., 1., .01).unsqueeze(1)\n",
    "    lps = log_post(x[j], eval_pts, **kwargs)\n",
    "    X = eval_pts\n",
    "    Z = lps.view(X.shape)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(24,8))\n",
    "    ax[0].plot(X.numpy(), Z.exp().numpy())\n",
    "    ax[0].set_title('Exact')\n",
    "    \n",
    "\n",
    "    # eval_pts = torch.arange(-1., 1., .01).unsqueeze(1)\n",
    "    lps = encoder.log_prob(eval_pts.to(device), x[j].view(1,-1).repeat(eval_pts.shape[0],1).to(device)).detach()\n",
    "    # X, Y = torch.meshgrid(vals, vals)\n",
    "    Z = lps.view(X.shape)\n",
    "    ax[1].plot(X.cpu().numpy(), Z.cpu().exp().numpy())\n",
    "    ax[1].set_title('Approximate Posterior Flow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955455ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.9996e-01, -4.0212e-01],\n",
      "        [ 6.9289e-01, -3.9505e-01],\n",
      "        [ 6.8582e-01, -3.8798e-01],\n",
      "        [ 6.7875e-01, -3.8091e-01],\n",
      "        [ 6.7168e-01, -3.7384e-01],\n",
      "        [ 6.6461e-01, -3.6677e-01],\n",
      "        [ 6.5754e-01, -3.5970e-01],\n",
      "        [ 6.5047e-01, -3.5262e-01],\n",
      "        [ 6.4340e-01, -3.4555e-01],\n",
      "        [ 6.3632e-01, -3.3848e-01],\n",
      "        [ 6.2925e-01, -3.3141e-01],\n",
      "        [ 6.2218e-01, -3.2434e-01],\n",
      "        [ 6.1511e-01, -3.1727e-01],\n",
      "        [ 6.0804e-01, -3.1020e-01],\n",
      "        [ 6.0097e-01, -3.0313e-01],\n",
      "        [ 5.9390e-01, -2.9606e-01],\n",
      "        [ 5.8683e-01, -2.8898e-01],\n",
      "        [ 5.7976e-01, -2.8191e-01],\n",
      "        [ 5.7269e-01, -2.7484e-01],\n",
      "        [ 5.6561e-01, -2.6777e-01],\n",
      "        [ 5.5854e-01, -2.6070e-01],\n",
      "        [ 5.5147e-01, -2.5363e-01],\n",
      "        [ 5.4440e-01, -2.4656e-01],\n",
      "        [ 5.3733e-01, -2.3949e-01],\n",
      "        [ 5.3026e-01, -2.3242e-01],\n",
      "        [ 5.2319e-01, -2.2535e-01],\n",
      "        [ 5.1612e-01, -2.1827e-01],\n",
      "        [ 5.0905e-01, -2.1120e-01],\n",
      "        [ 5.0197e-01, -2.0413e-01],\n",
      "        [ 4.9490e-01, -1.9706e-01],\n",
      "        [ 4.8783e-01, -1.8999e-01],\n",
      "        [ 4.8076e-01, -1.8292e-01],\n",
      "        [ 4.7369e-01, -1.7585e-01],\n",
      "        [ 4.6662e-01, -1.6878e-01],\n",
      "        [ 4.5955e-01, -1.6171e-01],\n",
      "        [ 4.5248e-01, -1.5463e-01],\n",
      "        [ 4.4541e-01, -1.4756e-01],\n",
      "        [ 4.3833e-01, -1.4049e-01],\n",
      "        [ 4.3126e-01, -1.3342e-01],\n",
      "        [ 4.2419e-01, -1.2635e-01],\n",
      "        [ 4.1712e-01, -1.1928e-01],\n",
      "        [ 4.1005e-01, -1.1221e-01],\n",
      "        [ 4.0298e-01, -1.0514e-01],\n",
      "        [ 3.9591e-01, -9.8066e-02],\n",
      "        [ 3.8884e-01, -9.0995e-02],\n",
      "        [ 3.8177e-01, -8.3924e-02],\n",
      "        [ 3.7470e-01, -7.6853e-02],\n",
      "        [ 3.6762e-01, -6.9782e-02],\n",
      "        [ 3.6055e-01, -6.2711e-02],\n",
      "        [ 3.5348e-01, -5.5640e-02],\n",
      "        [ 3.4641e-01, -4.8568e-02],\n",
      "        [ 3.3934e-01, -4.1497e-02],\n",
      "        [ 3.3227e-01, -3.4426e-02],\n",
      "        [ 3.2520e-01, -2.7355e-02],\n",
      "        [ 3.1813e-01, -2.0284e-02],\n",
      "        [ 3.1106e-01, -1.3213e-02],\n",
      "        [ 3.0398e-01, -6.1420e-03],\n",
      "        [ 2.9691e-01,  9.2903e-04],\n",
      "        [ 2.8984e-01,  8.0001e-03],\n",
      "        [ 2.8277e-01,  1.5071e-02],\n",
      "        [ 2.7570e-01,  2.2142e-02],\n",
      "        [ 2.6863e-01,  2.9213e-02],\n",
      "        [ 2.6156e-01,  3.6284e-02],\n",
      "        [ 2.5449e-01,  4.3355e-02],\n",
      "        [ 2.4742e-01,  5.0427e-02],\n",
      "        [ 2.4035e-01,  5.7498e-02],\n",
      "        [ 2.3327e-01,  6.4569e-02],\n",
      "        [ 2.2620e-01,  7.1640e-02],\n",
      "        [ 2.1913e-01,  7.8711e-02],\n",
      "        [ 2.1206e-01,  8.5782e-02],\n",
      "        [ 2.0499e-01,  9.2853e-02],\n",
      "        [ 1.9792e-01,  9.9924e-02],\n",
      "        [ 1.9085e-01,  1.0700e-01],\n",
      "        [ 1.8378e-01,  1.1407e-01],\n",
      "        [ 1.7671e-01,  1.2114e-01],\n",
      "        [ 1.6963e-01,  1.2821e-01],\n",
      "        [ 1.6256e-01,  1.3528e-01],\n",
      "        [ 1.5549e-01,  1.4235e-01],\n",
      "        [ 1.4842e-01,  1.4942e-01],\n",
      "        [ 1.4135e-01,  1.5649e-01],\n",
      "        [ 1.3428e-01,  1.6356e-01],\n",
      "        [ 1.2721e-01,  1.7063e-01],\n",
      "        [ 1.2014e-01,  1.7771e-01],\n",
      "        [ 1.1307e-01,  1.8478e-01],\n",
      "        [ 1.0599e-01,  1.9185e-01],\n",
      "        [ 9.8924e-02,  1.9892e-01],\n",
      "        [ 9.1853e-02,  2.0599e-01],\n",
      "        [ 8.4781e-02,  2.1306e-01],\n",
      "        [ 7.7710e-02,  2.2013e-01],\n",
      "        [ 7.0639e-02,  2.2720e-01],\n",
      "        [ 6.3568e-02,  2.3427e-01],\n",
      "        [ 5.6497e-02,  2.4135e-01],\n",
      "        [ 4.9426e-02,  2.4842e-01],\n",
      "        [ 4.2355e-02,  2.5549e-01],\n",
      "        [ 3.5284e-02,  2.6256e-01],\n",
      "        [ 2.8213e-02,  2.6963e-01],\n",
      "        [ 2.1142e-02,  2.7670e-01],\n",
      "        [ 1.4071e-02,  2.8377e-01],\n",
      "        [ 6.9997e-03,  2.9084e-01],\n",
      "        [-7.1328e-05,  2.9791e-01],\n",
      "        [-7.1424e-03,  3.0498e-01],\n",
      "        [-7.1326e-05,  3.1206e-01],\n",
      "        [ 6.9997e-03,  3.1913e-01],\n",
      "        [ 1.4071e-02,  3.2620e-01],\n",
      "        [ 2.1142e-02,  3.3327e-01],\n",
      "        [ 2.8213e-02,  3.4034e-01],\n",
      "        [ 3.5284e-02,  3.4741e-01],\n",
      "        [ 4.2355e-02,  3.5448e-01],\n",
      "        [ 4.9426e-02,  3.6155e-01],\n",
      "        [ 5.6497e-02,  3.6862e-01],\n",
      "        [ 6.3568e-02,  3.7570e-01],\n",
      "        [ 7.0639e-02,  3.8277e-01],\n",
      "        [ 7.7710e-02,  3.8984e-01],\n",
      "        [ 8.4781e-02,  3.9691e-01],\n",
      "        [ 9.1853e-02,  4.0398e-01],\n",
      "        [ 9.8924e-02,  4.1105e-01],\n",
      "        [ 1.0599e-01,  4.1812e-01],\n",
      "        [ 1.1307e-01,  4.2519e-01],\n",
      "        [ 1.2014e-01,  4.3226e-01],\n",
      "        [ 1.2721e-01,  4.3934e-01],\n",
      "        [ 1.3428e-01,  4.4641e-01],\n",
      "        [ 1.4135e-01,  4.5348e-01],\n",
      "        [ 1.4842e-01,  4.6055e-01],\n",
      "        [ 1.5549e-01,  4.6762e-01],\n",
      "        [ 1.6256e-01,  4.7469e-01],\n",
      "        [ 1.6963e-01,  4.8176e-01],\n",
      "        [ 1.7671e-01,  4.8883e-01],\n",
      "        [ 1.8378e-01,  4.9590e-01],\n",
      "        [ 1.9085e-01,  5.0297e-01],\n",
      "        [ 1.9792e-01,  5.1005e-01],\n",
      "        [ 2.0499e-01,  5.1712e-01],\n",
      "        [ 2.1206e-01,  5.2419e-01],\n",
      "        [ 2.1913e-01,  5.3126e-01],\n",
      "        [ 2.2620e-01,  5.3833e-01],\n",
      "        [ 2.3327e-01,  5.4540e-01],\n",
      "        [ 2.4034e-01,  5.5247e-01],\n",
      "        [ 2.4742e-01,  5.5954e-01],\n",
      "        [ 2.5449e-01,  5.6661e-01],\n",
      "        [ 2.6156e-01,  5.7369e-01],\n",
      "        [ 2.6863e-01,  5.8076e-01],\n",
      "        [ 2.7570e-01,  5.8783e-01],\n",
      "        [ 2.8277e-01,  5.9490e-01],\n",
      "        [ 2.8984e-01,  6.0197e-01],\n",
      "        [ 2.9691e-01,  6.0904e-01],\n",
      "        [ 3.0398e-01,  6.1611e-01],\n",
      "        [ 3.1106e-01,  6.2318e-01],\n",
      "        [ 3.1813e-01,  6.3025e-01],\n",
      "        [ 3.2520e-01,  6.3733e-01],\n",
      "        [ 3.3227e-01,  6.4440e-01],\n",
      "        [ 3.3934e-01,  6.5147e-01],\n",
      "        [ 3.4641e-01,  6.5854e-01],\n",
      "        [ 3.5348e-01,  6.6561e-01],\n",
      "        [ 3.6055e-01,  6.7268e-01],\n",
      "        [ 3.6762e-01,  6.7975e-01],\n",
      "        [ 3.7470e-01,  6.8682e-01],\n",
      "        [ 3.8177e-01,  6.9389e-01],\n",
      "        [ 3.8884e-01,  7.0096e-01],\n",
      "        [ 3.9591e-01,  7.0804e-01],\n",
      "        [ 4.0298e-01,  7.1511e-01],\n",
      "        [ 4.1005e-01,  7.2218e-01],\n",
      "        [ 4.1712e-01,  7.2925e-01],\n",
      "        [ 4.2419e-01,  7.3632e-01],\n",
      "        [ 4.3126e-01,  7.4339e-01],\n",
      "        [ 4.3833e-01,  7.5046e-01],\n",
      "        [ 4.4541e-01,  7.5753e-01],\n",
      "        [ 4.5248e-01,  7.6460e-01],\n",
      "        [ 4.5955e-01,  7.7168e-01],\n",
      "        [ 4.6662e-01,  7.7875e-01],\n",
      "        [ 4.7369e-01,  7.8582e-01],\n",
      "        [ 4.8076e-01,  7.9289e-01],\n",
      "        [ 4.8783e-01,  7.9996e-01],\n",
      "        [ 4.9490e-01,  8.0703e-01],\n",
      "        [ 5.0197e-01,  8.1410e-01],\n",
      "        [ 5.0905e-01,  8.2117e-01],\n",
      "        [ 5.1612e-01,  8.2824e-01],\n",
      "        [ 5.2319e-01,  8.3532e-01],\n",
      "        [ 5.3026e-01,  8.4239e-01],\n",
      "        [ 5.3733e-01,  8.4946e-01],\n",
      "        [ 5.4440e-01,  8.5653e-01],\n",
      "        [ 5.5147e-01,  8.6360e-01],\n",
      "        [ 5.5854e-01,  8.7067e-01],\n",
      "        [ 5.6561e-01,  8.7774e-01],\n",
      "        [ 5.7269e-01,  8.8481e-01],\n",
      "        [ 5.7976e-01,  8.9188e-01],\n",
      "        [ 5.8683e-01,  8.9895e-01],\n",
      "        [ 5.9390e-01,  9.0603e-01],\n",
      "        [ 6.0097e-01,  9.1310e-01],\n",
      "        [ 6.0804e-01,  9.2017e-01],\n",
      "        [ 6.1511e-01,  9.2724e-01],\n",
      "        [ 6.2218e-01,  9.3431e-01],\n",
      "        [ 6.2925e-01,  9.4138e-01],\n",
      "        [ 6.3632e-01,  9.4845e-01],\n",
      "        [ 6.4340e-01,  9.5552e-01],\n",
      "        [ 6.5047e-01,  9.6259e-01],\n",
      "        [ 6.5754e-01,  9.6967e-01],\n",
      "        [ 6.6461e-01,  9.7674e-01],\n",
      "        [ 6.7168e-01,  9.8381e-01],\n",
      "        [ 6.7875e-01,  9.9088e-01],\n",
      "        [ 6.8582e-01,  9.9795e-01],\n",
      "        [ 6.9289e-01,  1.0050e+00]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(j=5, x=x, encoder=encoder, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf4277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "025fdf49e09ee838b0c05e971129fbc14df70fae1b22b06a04398c8d66c2f675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
