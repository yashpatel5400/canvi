{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4188c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4212fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mg1\"\n",
    "\n",
    "train_inp = f\"data/{dataset}/train/inputs.npy\"\n",
    "train_oup = f\"data/{dataset}/train/outputs.npy\"\n",
    "\n",
    "test_inp = f\"data/{dataset}/test/inputs.npy\"\n",
    "test_oup = f\"data/{dataset}/test/outputs.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec07cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_theta = torch.Tensor(np.load(train_inp))\n",
    "train_x = torch.Tensor(np.load(train_oup))\n",
    "\n",
    "calibration_size = 10_000\n",
    "test_theta_presplit = np.load(test_inp)\n",
    "test_x_presplit = np.load(test_oup)\n",
    "\n",
    "test_theta = torch.Tensor(test_theta_presplit[:-calibration_size])\n",
    "test_x = torch.Tensor(test_x_presplit[:-calibration_size])\n",
    "\n",
    "calibration_theta = torch.Tensor(test_theta_presplit[-calibration_size:])\n",
    "calibration_x = torch.Tensor(test_x_presplit[-calibration_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2712436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gen_idx = 0\n",
    "\n",
    "def generate_data(n_pts, return_theta=False):\n",
    "    global gen_idx\n",
    "    theta, x = train_theta[gen_idx:gen_idx+n_pts], train_x[gen_idx:gen_idx+n_pts]\n",
    "    gen_idx += n_pts\n",
    "\n",
    "    if return_theta: \n",
    "        return theta, x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca0e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknos.nflows import flows, transforms\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "from warnings import warn\n",
    "\n",
    "from pyknos.nflows import distributions as distributions_\n",
    "from pyknos.nflows import flows, transforms\n",
    "from pyknos.nflows.nn import nets\n",
    "from pyknos.nflows.transforms.splines import rational_quadratic\n",
    "from torch import Tensor, nn, relu, tanh, tensor, uint8\n",
    "\n",
    "from sbi.utils.sbiutils import (\n",
    "    standardizing_net,\n",
    "    standardizing_transform,\n",
    "    z_score_parser,\n",
    ")\n",
    "from sbi.utils.torchutils import create_alternating_binary_mask\n",
    "from sbi.utils.user_input_checks import check_data_device, check_embedding_net_device\n",
    "\n",
    "class ContextSplineMap(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network from `context` to the spline parameters.\n",
    "    We cannot use the resnet as conditioner to learn each dimension conditioned\n",
    "    on the other dimensions (because there is only one). Instead, we learn the\n",
    "    spline parameters directly. In the case of conditinal density estimation,\n",
    "    we make the spline parameters conditional on the context. This is\n",
    "    implemented in this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_features: int,\n",
    "        context_features: int,\n",
    "        hidden_layers: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize neural network that learns to predict spline parameters.\n",
    "        Args:\n",
    "            in_features: Unused since there is no `conditioner` in 1D.\n",
    "            out_features: Number of spline parameters.\n",
    "            hidden_features: Number of hidden units.\n",
    "            context_features: Number of context features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # `self.hidden_features` is only defined such that nflows can infer\n",
    "        # a scaling factor for initializations.\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        # Use a non-linearity because otherwise, there will be a linear\n",
    "        # mapping from context features onto distribution parameters.\n",
    "\n",
    "        # Initialize with input layer.\n",
    "        layer_list = [nn.Linear(context_features, hidden_features), nn.ReLU()]\n",
    "        # Add hidden layers.\n",
    "        layer_list += [\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "        ] * hidden_layers\n",
    "        # Add output layer.\n",
    "        layer_list += [nn.Linear(hidden_features, out_features)]\n",
    "        self.spline_predictor = nn.Sequential(*layer_list)\n",
    "\n",
    "    def __call__(self, inputs: Tensor, context: Tensor, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Return parameters of the spline given the context.\n",
    "        Args:\n",
    "            inputs: Unused. It would usually be the other dimensions, but in\n",
    "                1D, there are no other dimensions.\n",
    "            context: Context features.\n",
    "        Returns:\n",
    "            Spline parameters.\n",
    "        \"\"\"\n",
    "        return self.spline_predictor(context)\n",
    "\n",
    "# Declan: this code from SBI library\n",
    "def build_nsf(\n",
    "    batch_x: Tensor,\n",
    "    batch_y: Tensor,\n",
    "    z_score_x: Optional[str] = \"independent\",\n",
    "    z_score_y: Optional[str] = \"independent\",\n",
    "    hidden_features: int = 50,\n",
    "    num_transforms: int = 5,\n",
    "    num_bins: int = 10,\n",
    "    embedding_net: nn.Module = nn.Identity(),\n",
    "    tail_bound: float = 3.0,\n",
    "    hidden_layers_spline_context: int = 1,\n",
    "    num_blocks: int = 2,\n",
    "    dropout_probability: float = 0.0,\n",
    "    use_batch_norm: bool = False,\n",
    "    **kwargs,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Builds NSF p(x|y).\n",
    "    Args:\n",
    "        batch_x: Batch of xs, used to infer dimensionality and (optional) z-scoring.\n",
    "        batch_y: Batch of ys, used to infer dimensionality and (optional) z-scoring.\n",
    "        z_score_x: Whether to z-score xs passing into the network, can be one of:\n",
    "            - `none`, or None: do not z-score.\n",
    "            - `independent`: z-score each dimension independently.\n",
    "            - `structured`: treat dimensions as related, therefore compute mean and std\n",
    "            over the entire batch, instead of per-dimension. Should be used when each\n",
    "            sample is, for example, a time series or an image.\n",
    "        z_score_y: Whether to z-score ys passing into the network, same options as\n",
    "            z_score_x.\n",
    "        hidden_features: Number of hidden features.\n",
    "        num_transforms: Number of transforms.\n",
    "        num_bins: Number of bins used for the splines.\n",
    "        embedding_net: Optional embedding network for y.\n",
    "        tail_bound: tail bound for each spline.\n",
    "        hidden_layers_spline_context: number of hidden layers of the spline context net\n",
    "            for one-dimensional x.\n",
    "        num_blocks: number of blocks used for residual net for context embedding.\n",
    "        dropout_probability: dropout probability for regularization in residual net.\n",
    "        use_batch_norm: whether to use batch norm in residual net.\n",
    "        kwargs: Additional arguments that are passed by the build function but are not\n",
    "            relevant for maf and are therefore ignored.\n",
    "    Returns:\n",
    "        Neural network.\n",
    "    \"\"\"\n",
    "    x_numel = batch_x[0].numel()\n",
    "    # Infer the output dimensionality of the embedding_net by making a forward pass.\n",
    "    check_data_device(batch_x, batch_y)\n",
    "    check_embedding_net_device(embedding_net=embedding_net, datum=batch_y)\n",
    "    y_numel = embedding_net(batch_y[:1]).numel()\n",
    "\n",
    "    # Define mask function to alternate between predicted x-dimensions.\n",
    "    def mask_in_layer(i):\n",
    "        return create_alternating_binary_mask(features=x_numel, even=(i % 2 == 0))\n",
    "\n",
    "    # If x is just a scalar then use a dummy mask and learn spline parameters using the\n",
    "    # conditioning variables only.\n",
    "    if x_numel == 1:\n",
    "        # Conditioner ignores the data and uses the conditioning variables only.\n",
    "        conditioner = partial(\n",
    "            ContextSplineMap,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=y_numel,\n",
    "            hidden_layers=hidden_layers_spline_context,\n",
    "        )\n",
    "    else:\n",
    "        # Use conditional resnet as spline conditioner.\n",
    "        conditioner = partial(\n",
    "            nets.ResidualNet,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=y_numel,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "        )\n",
    "\n",
    "    # Stack spline transforms.\n",
    "    transform_list = []\n",
    "    for i in range(num_transforms):\n",
    "        block = [\n",
    "            transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "                mask=mask_in_layer(i) if x_numel > 1 else tensor([1], dtype=uint8),\n",
    "                transform_net_create_fn=conditioner,\n",
    "                num_bins=num_bins,\n",
    "                tails=\"linear\",\n",
    "                tail_bound=tail_bound,\n",
    "                apply_unconditional_transform=False,\n",
    "            )\n",
    "        ]\n",
    "        # Add LU transform only for high D x. Permutation makes sense only for more than\n",
    "        # one feature.\n",
    "        if x_numel > 1:\n",
    "            block.append(\n",
    "                transforms.LULinear(x_numel, identity_init=True),\n",
    "            )\n",
    "        transform_list += block\n",
    "\n",
    "    z_score_x_bool, structured_x = z_score_parser(z_score_x)\n",
    "    if z_score_x_bool:\n",
    "        # Prepend standardizing transform to nsf transforms.\n",
    "        transform_list = [\n",
    "            standardizing_transform(batch_x, structured_x)\n",
    "        ] + transform_list\n",
    "\n",
    "    z_score_y_bool, structured_y = z_score_parser(z_score_y)\n",
    "    if z_score_y_bool:\n",
    "        # Prepend standardizing transform to y-embedding.\n",
    "        embedding_net = nn.Sequential(\n",
    "            standardizing_net(batch_y, structured_y), embedding_net\n",
    "        )\n",
    "\n",
    "    distribution = distributions_.StandardNormal((x_numel,))\n",
    "\n",
    "    # Combine transforms.\n",
    "    transform = transforms.CompositeTransform(transform_list)\n",
    "    neural_net = flows.Flow(transform, distribution, embedding_net)\n",
    "\n",
    "    return neural_net\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.context_dim = dim\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Assumes context x is of shape (batch_size, self.context_dim)\n",
    "        '''\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a406aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size=100\n",
    "device='cuda:0'\n",
    "\n",
    "# EXAMPLE BATCH FOR SHAPES\n",
    "z_dim = train_theta.shape[-1]\n",
    "x_dim = train_x.shape[-1]\n",
    "num_obs_flow = mb_size\n",
    "fake_zs = torch.randn((mb_size, z_dim))\n",
    "fake_xs = torch.randn((mb_size, x_dim))\n",
    "encoder = build_nsf(fake_zs, fake_xs, z_score_x='none', z_score_y='none')\n",
    "\n",
    "encoder.to(device)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87b099ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 46.24494934082031\n",
      "Iteration 1: loss 46.41064453125\n",
      "Iteration 2: loss 42.17922592163086\n",
      "Iteration 3: loss 41.211387634277344\n",
      "Iteration 4: loss 42.21302795410156\n",
      "Iteration 5: loss 42.915340423583984\n",
      "Iteration 6: loss 37.574100494384766\n",
      "Iteration 7: loss 41.07575225830078\n",
      "Iteration 8: loss 38.61660385131836\n",
      "Iteration 9: loss 35.34571838378906\n",
      "Iteration 10: loss 38.589576721191406\n",
      "Iteration 11: loss 36.24908447265625\n",
      "Iteration 12: loss 33.62785720825195\n",
      "Iteration 13: loss 35.46210479736328\n",
      "Iteration 14: loss 36.16910171508789\n",
      "Iteration 15: loss 36.055145263671875\n",
      "Iteration 16: loss 32.9692497253418\n",
      "Iteration 17: loss 33.60988998413086\n",
      "Iteration 18: loss 31.144018173217773\n",
      "Iteration 19: loss 29.093114852905273\n",
      "Iteration 20: loss 29.837177276611328\n",
      "Iteration 21: loss 30.941017150878906\n",
      "Iteration 22: loss 29.712608337402344\n",
      "Iteration 23: loss 29.942617416381836\n",
      "Iteration 24: loss 31.840702056884766\n",
      "Iteration 25: loss 30.446693420410156\n",
      "Iteration 26: loss 31.440284729003906\n",
      "Iteration 27: loss 29.376449584960938\n",
      "Iteration 28: loss 31.56229019165039\n",
      "Iteration 29: loss 28.317943572998047\n",
      "Iteration 30: loss 23.42268180847168\n",
      "Iteration 31: loss 26.869482040405273\n",
      "Iteration 32: loss 28.055953979492188\n",
      "Iteration 33: loss 27.162973403930664\n",
      "Iteration 34: loss 25.339447021484375\n",
      "Iteration 35: loss 24.30677032470703\n",
      "Iteration 36: loss 26.325702667236328\n",
      "Iteration 37: loss 27.396024703979492\n",
      "Iteration 38: loss 23.382965087890625\n",
      "Iteration 39: loss 27.853736877441406\n",
      "Iteration 40: loss 25.99853515625\n",
      "Iteration 41: loss 24.30939483642578\n",
      "Iteration 42: loss 22.3770694732666\n",
      "Iteration 43: loss 23.39076042175293\n",
      "Iteration 44: loss 23.921249389648438\n",
      "Iteration 45: loss 22.553091049194336\n",
      "Iteration 46: loss 22.94109535217285\n",
      "Iteration 47: loss 22.729145050048828\n",
      "Iteration 48: loss 22.930343627929688\n",
      "Iteration 49: loss 21.385395050048828\n",
      "Iteration 50: loss 22.718000411987305\n",
      "Iteration 51: loss 20.623319625854492\n",
      "Iteration 52: loss 21.886146545410156\n",
      "Iteration 53: loss 21.666757583618164\n",
      "Iteration 54: loss 23.93952178955078\n",
      "Iteration 55: loss 21.231319427490234\n",
      "Iteration 56: loss 21.05693817138672\n",
      "Iteration 57: loss 21.208797454833984\n",
      "Iteration 58: loss 20.68250274658203\n",
      "Iteration 59: loss 20.874662399291992\n",
      "Iteration 60: loss 20.837804794311523\n",
      "Iteration 61: loss 20.090246200561523\n",
      "Iteration 62: loss 19.27015495300293\n",
      "Iteration 63: loss 18.590091705322266\n",
      "Iteration 64: loss 18.945486068725586\n",
      "Iteration 65: loss 19.56052017211914\n",
      "Iteration 66: loss 17.38623809814453\n",
      "Iteration 67: loss 18.78013038635254\n",
      "Iteration 68: loss 19.447500228881836\n",
      "Iteration 69: loss 18.633609771728516\n",
      "Iteration 70: loss 17.182598114013672\n",
      "Iteration 71: loss 19.822467803955078\n",
      "Iteration 72: loss 19.610015869140625\n",
      "Iteration 73: loss 18.997514724731445\n",
      "Iteration 74: loss 18.367094039916992\n",
      "Iteration 75: loss 17.918296813964844\n",
      "Iteration 76: loss 17.60188102722168\n",
      "Iteration 77: loss 17.23346519470215\n",
      "Iteration 78: loss 17.310760498046875\n",
      "Iteration 79: loss 16.517107009887695\n",
      "Iteration 80: loss 18.334903717041016\n",
      "Iteration 81: loss 17.847211837768555\n",
      "Iteration 82: loss 16.76325798034668\n",
      "Iteration 83: loss 16.402271270751953\n",
      "Iteration 84: loss 17.38804054260254\n",
      "Iteration 85: loss 17.96443748474121\n",
      "Iteration 86: loss 17.3793888092041\n",
      "Iteration 87: loss 18.12293243408203\n",
      "Iteration 88: loss 17.196147918701172\n",
      "Iteration 89: loss 16.75211524963379\n",
      "Iteration 90: loss 16.41994285583496\n",
      "Iteration 91: loss 17.24298858642578\n",
      "Iteration 92: loss 17.808269500732422\n",
      "Iteration 93: loss 16.627490997314453\n",
      "Iteration 94: loss 16.31839370727539\n",
      "Iteration 95: loss 17.076284408569336\n",
      "Iteration 96: loss 16.763120651245117\n",
      "Iteration 97: loss 16.954343795776367\n",
      "Iteration 98: loss 15.99954891204834\n",
      "Iteration 99: loss 15.723495483398438\n",
      "Iteration 100: loss 14.968328475952148\n",
      "Iteration 101: loss 16.780935287475586\n",
      "Iteration 102: loss 16.407442092895508\n",
      "Iteration 103: loss 16.304359436035156\n",
      "Iteration 104: loss 14.28511905670166\n",
      "Iteration 105: loss 16.392728805541992\n",
      "Iteration 106: loss 18.477149963378906\n",
      "Iteration 107: loss 16.70213508605957\n",
      "Iteration 108: loss 19.179452896118164\n",
      "Iteration 109: loss 17.552186965942383\n",
      "Iteration 110: loss 17.5128116607666\n",
      "Iteration 111: loss 16.760286331176758\n",
      "Iteration 112: loss 17.970827102661133\n",
      "Iteration 113: loss 16.956464767456055\n",
      "Iteration 114: loss 15.600520133972168\n",
      "Iteration 115: loss 15.655537605285645\n",
      "Iteration 116: loss 16.30930519104004\n",
      "Iteration 117: loss 17.189594268798828\n",
      "Iteration 118: loss 14.108234405517578\n",
      "Iteration 119: loss 14.160253524780273\n",
      "Iteration 120: loss 17.26831817626953\n",
      "Iteration 121: loss 15.962228775024414\n",
      "Iteration 122: loss 16.646928787231445\n",
      "Iteration 123: loss 14.642338752746582\n",
      "Iteration 124: loss 15.663166999816895\n",
      "Iteration 125: loss 16.189416885375977\n",
      "Iteration 126: loss 15.436951637268066\n",
      "Iteration 127: loss 14.501687049865723\n",
      "Iteration 128: loss 15.362071990966797\n",
      "Iteration 129: loss 15.137958526611328\n",
      "Iteration 130: loss 12.929290771484375\n",
      "Iteration 131: loss 13.996180534362793\n",
      "Iteration 132: loss 14.603007316589355\n",
      "Iteration 133: loss 14.524581909179688\n",
      "Iteration 134: loss 14.61564826965332\n",
      "Iteration 135: loss 13.910900115966797\n",
      "Iteration 136: loss 16.39842414855957\n",
      "Iteration 137: loss 14.468422889709473\n",
      "Iteration 138: loss 13.027070045471191\n",
      "Iteration 139: loss 13.434426307678223\n",
      "Iteration 140: loss 15.10191822052002\n",
      "Iteration 141: loss 13.152178764343262\n",
      "Iteration 142: loss 13.776493072509766\n",
      "Iteration 143: loss 13.417878150939941\n",
      "Iteration 144: loss 13.470630645751953\n",
      "Iteration 145: loss 12.919795036315918\n",
      "Iteration 146: loss 13.161032676696777\n",
      "Iteration 147: loss 12.706684112548828\n",
      "Iteration 148: loss 13.456629753112793\n",
      "Iteration 149: loss 12.910273551940918\n",
      "Iteration 150: loss 12.338725090026855\n",
      "Iteration 151: loss 12.820011138916016\n",
      "Iteration 152: loss 13.122509002685547\n",
      "Iteration 153: loss 13.748476028442383\n",
      "Iteration 154: loss 12.283082008361816\n",
      "Iteration 155: loss 12.701025009155273\n",
      "Iteration 156: loss 12.313940048217773\n",
      "Iteration 157: loss 11.939727783203125\n",
      "Iteration 158: loss 12.927956581115723\n",
      "Iteration 159: loss 12.539386749267578\n",
      "Iteration 160: loss 11.579829216003418\n",
      "Iteration 161: loss 11.848405838012695\n",
      "Iteration 162: loss 12.326483726501465\n",
      "Iteration 163: loss 11.357171058654785\n",
      "Iteration 164: loss 11.771567344665527\n",
      "Iteration 165: loss 11.992525100708008\n",
      "Iteration 166: loss 11.212983131408691\n",
      "Iteration 167: loss 11.18186092376709\n",
      "Iteration 168: loss 11.166370391845703\n",
      "Iteration 169: loss 11.985849380493164\n",
      "Iteration 170: loss 11.65030288696289\n",
      "Iteration 171: loss 10.036443710327148\n",
      "Iteration 172: loss 12.065033912658691\n",
      "Iteration 173: loss 10.479970932006836\n",
      "Iteration 174: loss 11.627946853637695\n",
      "Iteration 175: loss 11.463808059692383\n",
      "Iteration 176: loss 10.871016502380371\n",
      "Iteration 177: loss 10.874735832214355\n",
      "Iteration 178: loss 9.922621726989746\n",
      "Iteration 179: loss 12.109392166137695\n",
      "Iteration 180: loss 9.830365180969238\n",
      "Iteration 181: loss 11.28769302368164\n",
      "Iteration 182: loss 9.503338813781738\n",
      "Iteration 183: loss 12.125185012817383\n",
      "Iteration 184: loss 10.429319381713867\n",
      "Iteration 185: loss 10.34646987915039\n",
      "Iteration 186: loss 10.68181037902832\n",
      "Iteration 187: loss 9.901739120483398\n",
      "Iteration 188: loss 10.043588638305664\n",
      "Iteration 189: loss 9.303560256958008\n",
      "Iteration 190: loss 10.968655586242676\n",
      "Iteration 191: loss 9.819302558898926\n",
      "Iteration 192: loss 9.326973915100098\n",
      "Iteration 193: loss 9.123418807983398\n",
      "Iteration 194: loss 8.511516571044922\n",
      "Iteration 195: loss 9.545592308044434\n",
      "Iteration 196: loss 9.995423316955566\n",
      "Iteration 197: loss 10.237947463989258\n",
      "Iteration 198: loss 10.107004165649414\n",
      "Iteration 199: loss 9.533557891845703\n",
      "Iteration 200: loss 9.541311264038086\n",
      "Iteration 201: loss 10.230609893798828\n",
      "Iteration 202: loss 9.143421173095703\n",
      "Iteration 203: loss 9.574210166931152\n",
      "Iteration 204: loss 9.723315238952637\n",
      "Iteration 205: loss 8.748175621032715\n",
      "Iteration 206: loss 9.551700592041016\n",
      "Iteration 207: loss 9.850014686584473\n",
      "Iteration 208: loss 8.977739334106445\n",
      "Iteration 209: loss 9.406394958496094\n",
      "Iteration 210: loss 10.372937202453613\n",
      "Iteration 211: loss 9.616201400756836\n",
      "Iteration 212: loss 8.891242980957031\n",
      "Iteration 213: loss 9.399497032165527\n",
      "Iteration 214: loss 9.122794151306152\n",
      "Iteration 215: loss 9.090226173400879\n",
      "Iteration 216: loss 10.281014442443848\n",
      "Iteration 217: loss 9.075297355651855\n",
      "Iteration 218: loss 10.114768981933594\n",
      "Iteration 219: loss 8.701566696166992\n",
      "Iteration 220: loss 9.109370231628418\n",
      "Iteration 221: loss 8.426773071289062\n",
      "Iteration 222: loss 9.42686653137207\n",
      "Iteration 223: loss 9.183798789978027\n",
      "Iteration 224: loss 9.187711715698242\n",
      "Iteration 225: loss 9.077812194824219\n",
      "Iteration 226: loss 8.70719051361084\n",
      "Iteration 227: loss 8.512924194335938\n",
      "Iteration 228: loss 8.427380561828613\n",
      "Iteration 229: loss 8.624011993408203\n",
      "Iteration 230: loss 8.01779556274414\n",
      "Iteration 231: loss 8.646784782409668\n",
      "Iteration 232: loss 9.49144172668457\n",
      "Iteration 233: loss 8.957793235778809\n",
      "Iteration 234: loss 7.504634380340576\n",
      "Iteration 235: loss 8.64136791229248\n",
      "Iteration 236: loss 8.469980239868164\n",
      "Iteration 237: loss 8.52934455871582\n",
      "Iteration 238: loss 8.620548248291016\n",
      "Iteration 239: loss 7.41131591796875\n",
      "Iteration 240: loss 8.019657135009766\n",
      "Iteration 241: loss 8.110806465148926\n",
      "Iteration 242: loss 7.83048677444458\n",
      "Iteration 243: loss 7.876275539398193\n",
      "Iteration 244: loss 8.395193099975586\n",
      "Iteration 245: loss 8.175690650939941\n",
      "Iteration 246: loss 7.786346435546875\n",
      "Iteration 247: loss 7.625735282897949\n",
      "Iteration 248: loss 7.970755577087402\n",
      "Iteration 249: loss 7.935556411743164\n",
      "Iteration 250: loss 7.9381561279296875\n",
      "Iteration 251: loss 7.482692718505859\n",
      "Iteration 252: loss 7.937417507171631\n",
      "Iteration 253: loss 7.58291482925415\n",
      "Iteration 254: loss 7.245100975036621\n",
      "Iteration 255: loss 7.003649711608887\n",
      "Iteration 256: loss 8.01672077178955\n",
      "Iteration 257: loss 7.295225143432617\n",
      "Iteration 258: loss 7.228945732116699\n",
      "Iteration 259: loss 6.973241806030273\n",
      "Iteration 260: loss 7.456871032714844\n",
      "Iteration 261: loss 7.654372215270996\n",
      "Iteration 262: loss 7.489602088928223\n",
      "Iteration 263: loss 7.188324928283691\n",
      "Iteration 264: loss 6.791972637176514\n",
      "Iteration 265: loss 7.171606063842773\n",
      "Iteration 266: loss 7.128096580505371\n",
      "Iteration 267: loss 7.175395488739014\n",
      "Iteration 268: loss 6.610358238220215\n",
      "Iteration 269: loss 6.870691776275635\n",
      "Iteration 270: loss 6.923580169677734\n",
      "Iteration 271: loss 6.974082946777344\n",
      "Iteration 272: loss 7.269461631774902\n",
      "Iteration 273: loss 7.023778915405273\n",
      "Iteration 274: loss 6.021964073181152\n",
      "Iteration 275: loss 6.836435317993164\n",
      "Iteration 276: loss 6.738797664642334\n",
      "Iteration 277: loss 6.77392053604126\n",
      "Iteration 278: loss 6.63206672668457\n",
      "Iteration 279: loss 7.199254035949707\n",
      "Iteration 280: loss 6.748069763183594\n",
      "Iteration 281: loss 5.977182865142822\n",
      "Iteration 282: loss 6.717007160186768\n",
      "Iteration 283: loss 6.831932067871094\n",
      "Iteration 284: loss 6.96082878112793\n",
      "Iteration 285: loss 6.030034065246582\n",
      "Iteration 286: loss 5.938764572143555\n",
      "Iteration 287: loss 6.112161636352539\n",
      "Iteration 288: loss 6.2535271644592285\n",
      "Iteration 289: loss 6.0984320640563965\n",
      "Iteration 290: loss 5.809932231903076\n",
      "Iteration 291: loss 5.9501824378967285\n",
      "Iteration 292: loss 6.544815540313721\n",
      "Iteration 293: loss 6.373490333557129\n",
      "Iteration 294: loss 6.665846347808838\n",
      "Iteration 295: loss 5.901825904846191\n",
      "Iteration 296: loss 6.326244354248047\n",
      "Iteration 297: loss 7.312716960906982\n",
      "Iteration 298: loss 6.653697490692139\n",
      "Iteration 299: loss 5.690046310424805\n",
      "Iteration 300: loss 7.069983959197998\n",
      "Iteration 301: loss 6.562361717224121\n",
      "Iteration 302: loss 6.892436981201172\n",
      "Iteration 303: loss 5.401432991027832\n",
      "Iteration 304: loss 6.217705726623535\n",
      "Iteration 305: loss 5.302620887756348\n",
      "Iteration 306: loss 6.297294616699219\n",
      "Iteration 307: loss 6.7079057693481445\n",
      "Iteration 308: loss 5.740644454956055\n",
      "Iteration 309: loss 5.553587436676025\n",
      "Iteration 310: loss 6.0410990715026855\n",
      "Iteration 311: loss 6.253337383270264\n",
      "Iteration 312: loss 7.258455753326416\n",
      "Iteration 313: loss 8.195205688476562\n",
      "Iteration 314: loss 7.611627101898193\n",
      "Iteration 315: loss 8.107185363769531\n",
      "Iteration 316: loss 8.49465274810791\n",
      "Iteration 317: loss 9.466976165771484\n",
      "Iteration 318: loss 8.701783180236816\n",
      "Iteration 319: loss 8.382057189941406\n",
      "Iteration 320: loss 9.689090728759766\n",
      "Iteration 321: loss 9.128005981445312\n",
      "Iteration 322: loss 8.541680335998535\n",
      "Iteration 323: loss 8.500772476196289\n",
      "Iteration 324: loss 7.7736639976501465\n",
      "Iteration 325: loss 8.537507057189941\n",
      "Iteration 326: loss 7.282547473907471\n",
      "Iteration 327: loss 7.389925479888916\n",
      "Iteration 328: loss 7.595357418060303\n",
      "Iteration 329: loss 7.297036647796631\n",
      "Iteration 330: loss 8.248140335083008\n",
      "Iteration 331: loss 7.083784580230713\n",
      "Iteration 332: loss 6.9319329261779785\n",
      "Iteration 333: loss 6.946761131286621\n",
      "Iteration 334: loss 7.071346282958984\n",
      "Iteration 335: loss 6.258315086364746\n",
      "Iteration 336: loss 6.624092102050781\n",
      "Iteration 337: loss 7.023622989654541\n",
      "Iteration 338: loss 6.334721565246582\n",
      "Iteration 339: loss 7.68569278717041\n",
      "Iteration 340: loss 6.4903106689453125\n",
      "Iteration 341: loss 6.257527828216553\n",
      "Iteration 342: loss 6.30518913269043\n",
      "Iteration 343: loss 6.405095100402832\n",
      "Iteration 344: loss 6.793503284454346\n",
      "Iteration 345: loss 6.203880310058594\n",
      "Iteration 346: loss 6.51480770111084\n",
      "Iteration 347: loss 6.37455940246582\n",
      "Iteration 348: loss 7.165955543518066\n",
      "Iteration 349: loss 6.466490268707275\n",
      "Iteration 350: loss 5.868570327758789\n",
      "Iteration 351: loss 5.977016448974609\n",
      "Iteration 352: loss 5.739228248596191\n",
      "Iteration 353: loss 6.457977294921875\n",
      "Iteration 354: loss 6.602985858917236\n",
      "Iteration 355: loss 6.346045017242432\n",
      "Iteration 356: loss 7.065304279327393\n",
      "Iteration 357: loss 6.117440223693848\n",
      "Iteration 358: loss 6.108263969421387\n",
      "Iteration 359: loss 5.673059463500977\n",
      "Iteration 360: loss 6.206357955932617\n",
      "Iteration 361: loss 5.999267578125\n",
      "Iteration 362: loss 6.0982346534729\n",
      "Iteration 363: loss 5.843052864074707\n",
      "Iteration 364: loss 5.80311393737793\n",
      "Iteration 365: loss 6.31823205947876\n",
      "Iteration 366: loss 5.812182426452637\n",
      "Iteration 367: loss 6.371408462524414\n",
      "Iteration 368: loss 6.580540657043457\n",
      "Iteration 369: loss 6.603785037994385\n",
      "Iteration 370: loss 6.7592453956604\n",
      "Iteration 371: loss 6.092329978942871\n",
      "Iteration 372: loss 6.710859298706055\n",
      "Iteration 373: loss 6.2801513671875\n",
      "Iteration 374: loss 6.323063850402832\n",
      "Iteration 375: loss 6.411624431610107\n",
      "Iteration 376: loss 6.393648147583008\n",
      "Iteration 377: loss 6.330399036407471\n",
      "Iteration 378: loss 7.020551681518555\n",
      "Iteration 379: loss 6.392934322357178\n",
      "Iteration 380: loss 6.55692720413208\n",
      "Iteration 381: loss 5.6060638427734375\n",
      "Iteration 382: loss 6.232882499694824\n",
      "Iteration 383: loss 5.528545379638672\n",
      "Iteration 384: loss 5.858935832977295\n",
      "Iteration 385: loss 5.622885227203369\n",
      "Iteration 386: loss 5.712738990783691\n",
      "Iteration 387: loss 6.0543742179870605\n",
      "Iteration 388: loss 5.742887020111084\n",
      "Iteration 389: loss 5.264428615570068\n",
      "Iteration 390: loss 6.034511566162109\n",
      "Iteration 391: loss 4.821643829345703\n",
      "Iteration 392: loss 5.407748699188232\n",
      "Iteration 393: loss 5.5088090896606445\n",
      "Iteration 394: loss 6.308718681335449\n",
      "Iteration 395: loss 5.368905544281006\n",
      "Iteration 396: loss 5.202005386352539\n",
      "Iteration 397: loss 5.605993747711182\n",
      "Iteration 398: loss 5.987991809844971\n",
      "Iteration 399: loss 5.780906677246094\n",
      "Iteration 400: loss 5.357841491699219\n",
      "Iteration 401: loss 5.330333232879639\n",
      "Iteration 402: loss 5.671601295471191\n",
      "Iteration 403: loss 5.391603946685791\n",
      "Iteration 404: loss 5.6958465576171875\n",
      "Iteration 405: loss 5.413066387176514\n",
      "Iteration 406: loss 5.345033645629883\n",
      "Iteration 407: loss 5.644220352172852\n",
      "Iteration 408: loss 4.898634433746338\n",
      "Iteration 409: loss 5.294260025024414\n",
      "Iteration 410: loss 5.852420330047607\n",
      "Iteration 411: loss 5.6505126953125\n",
      "Iteration 412: loss 5.540565013885498\n",
      "Iteration 413: loss 6.404338359832764\n",
      "Iteration 414: loss 5.216212272644043\n",
      "Iteration 415: loss 6.060978889465332\n",
      "Iteration 416: loss 4.51091194152832\n",
      "Iteration 417: loss 5.393621921539307\n",
      "Iteration 418: loss 5.487998008728027\n",
      "Iteration 419: loss 5.491905212402344\n",
      "Iteration 420: loss 4.321103096008301\n",
      "Iteration 421: loss 4.775035381317139\n",
      "Iteration 422: loss 4.613140106201172\n",
      "Iteration 423: loss 4.811593055725098\n",
      "Iteration 424: loss 4.683684825897217\n",
      "Iteration 425: loss 5.4396514892578125\n",
      "Iteration 426: loss 5.394842624664307\n",
      "Iteration 427: loss 5.192621231079102\n",
      "Iteration 428: loss 4.802130222320557\n",
      "Iteration 429: loss 4.238717555999756\n",
      "Iteration 430: loss 5.466511249542236\n",
      "Iteration 431: loss 4.879852294921875\n",
      "Iteration 432: loss 4.80617618560791\n",
      "Iteration 433: loss 5.155638217926025\n",
      "Iteration 434: loss 4.533901214599609\n",
      "Iteration 435: loss 4.376486301422119\n",
      "Iteration 436: loss 4.23447322845459\n",
      "Iteration 437: loss 5.150589466094971\n",
      "Iteration 438: loss 4.393603324890137\n",
      "Iteration 439: loss 4.005650997161865\n",
      "Iteration 440: loss 4.3207502365112305\n",
      "Iteration 441: loss 4.90975284576416\n",
      "Iteration 442: loss 5.425676345825195\n",
      "Iteration 443: loss 4.657848358154297\n",
      "Iteration 444: loss 5.239243030548096\n",
      "Iteration 445: loss 4.804707050323486\n",
      "Iteration 446: loss 5.231010437011719\n",
      "Iteration 447: loss 4.767240047454834\n",
      "Iteration 448: loss 4.834340572357178\n",
      "Iteration 449: loss 5.241275310516357\n",
      "Iteration 450: loss 4.800909996032715\n",
      "Iteration 451: loss 4.693573951721191\n",
      "Iteration 452: loss 5.102354049682617\n",
      "Iteration 453: loss 4.960190296173096\n",
      "Iteration 454: loss 4.488919734954834\n",
      "Iteration 455: loss 5.106593608856201\n",
      "Iteration 456: loss 5.1627936363220215\n",
      "Iteration 457: loss 4.526422023773193\n",
      "Iteration 458: loss 4.822882175445557\n",
      "Iteration 459: loss 4.689172267913818\n",
      "Iteration 460: loss 5.165754795074463\n",
      "Iteration 461: loss 4.336304664611816\n",
      "Iteration 462: loss 5.312536716461182\n",
      "Iteration 463: loss 4.766285419464111\n",
      "Iteration 464: loss 4.803860187530518\n",
      "Iteration 465: loss 4.243807792663574\n",
      "Iteration 466: loss 4.9533610343933105\n",
      "Iteration 467: loss 4.698992729187012\n",
      "Iteration 468: loss 4.764909744262695\n",
      "Iteration 469: loss 4.9808807373046875\n",
      "Iteration 470: loss 4.097047805786133\n",
      "Iteration 471: loss 5.941385269165039\n",
      "Iteration 472: loss 5.280055046081543\n",
      "Iteration 473: loss 4.581521987915039\n",
      "Iteration 474: loss 3.707811117172241\n",
      "Iteration 475: loss 4.31663703918457\n",
      "Iteration 476: loss 4.86610746383667\n",
      "Iteration 477: loss 4.552078723907471\n",
      "Iteration 478: loss 3.794779062271118\n",
      "Iteration 479: loss 4.117520809173584\n",
      "Iteration 480: loss 4.927925109863281\n",
      "Iteration 481: loss 4.246131896972656\n",
      "Iteration 482: loss 4.658313274383545\n",
      "Iteration 483: loss 4.403291702270508\n",
      "Iteration 484: loss 5.38861083984375\n",
      "Iteration 485: loss 4.356151103973389\n",
      "Iteration 486: loss 5.081799030303955\n",
      "Iteration 487: loss 4.89744758605957\n",
      "Iteration 488: loss 4.300693511962891\n",
      "Iteration 489: loss 4.865666389465332\n",
      "Iteration 490: loss 4.618462085723877\n",
      "Iteration 491: loss 5.152023792266846\n",
      "Iteration 492: loss 4.523878574371338\n",
      "Iteration 493: loss 4.36039400100708\n",
      "Iteration 494: loss 5.251399040222168\n",
      "Iteration 495: loss 5.022286891937256\n",
      "Iteration 496: loss 4.857812404632568\n",
      "Iteration 497: loss 4.804007530212402\n",
      "Iteration 498: loss 5.305970191955566\n",
      "Iteration 499: loss 5.310194969177246\n",
      "Iteration 500: loss 4.489145278930664\n",
      "Iteration 501: loss 4.769245147705078\n",
      "Iteration 502: loss 4.711609840393066\n",
      "Iteration 503: loss 4.4158148765563965\n",
      "Iteration 504: loss 4.614325523376465\n",
      "Iteration 505: loss 5.130906105041504\n",
      "Iteration 506: loss 4.658082008361816\n",
      "Iteration 507: loss 3.586954355239868\n",
      "Iteration 508: loss 4.879363536834717\n",
      "Iteration 509: loss 4.5803961753845215\n",
      "Iteration 510: loss 5.184875965118408\n",
      "Iteration 511: loss 4.264129161834717\n",
      "Iteration 512: loss 4.927943706512451\n",
      "Iteration 513: loss 4.668310165405273\n",
      "Iteration 514: loss 5.086073875427246\n",
      "Iteration 515: loss 4.761084079742432\n",
      "Iteration 516: loss 4.554469108581543\n",
      "Iteration 517: loss 3.9185991287231445\n",
      "Iteration 518: loss 3.5935144424438477\n",
      "Iteration 519: loss 4.4942731857299805\n",
      "Iteration 520: loss 5.180903434753418\n",
      "Iteration 521: loss 4.461822032928467\n",
      "Iteration 522: loss 4.4005022048950195\n",
      "Iteration 523: loss 4.504490375518799\n",
      "Iteration 524: loss 4.5743408203125\n",
      "Iteration 525: loss 3.81890869140625\n",
      "Iteration 526: loss 4.284031867980957\n",
      "Iteration 527: loss 4.359220027923584\n",
      "Iteration 528: loss 3.7110915184020996\n",
      "Iteration 529: loss 4.0588555335998535\n",
      "Iteration 530: loss 4.512723445892334\n",
      "Iteration 531: loss 4.249176025390625\n",
      "Iteration 532: loss 3.7135119438171387\n",
      "Iteration 533: loss 4.453174114227295\n",
      "Iteration 534: loss 3.800947904586792\n",
      "Iteration 535: loss 5.764825820922852\n",
      "Iteration 536: loss 4.798083782196045\n",
      "Iteration 537: loss 5.492264270782471\n",
      "Iteration 538: loss 5.595279216766357\n",
      "Iteration 539: loss 5.78496789932251\n",
      "Iteration 540: loss 6.080175876617432\n",
      "Iteration 541: loss 4.986477375030518\n",
      "Iteration 542: loss 4.997248649597168\n",
      "Iteration 543: loss 6.436803817749023\n",
      "Iteration 544: loss 4.927180290222168\n",
      "Iteration 545: loss 5.872995376586914\n",
      "Iteration 546: loss 5.5265302658081055\n",
      "Iteration 547: loss 4.212040424346924\n",
      "Iteration 548: loss 4.650104999542236\n",
      "Iteration 549: loss 5.614750385284424\n",
      "Iteration 550: loss 6.175878047943115\n",
      "Iteration 551: loss 5.177750110626221\n",
      "Iteration 552: loss 5.337039470672607\n",
      "Iteration 553: loss 6.305559158325195\n",
      "Iteration 554: loss 7.116710186004639\n",
      "Iteration 555: loss 5.616570472717285\n",
      "Iteration 556: loss 4.957021236419678\n",
      "Iteration 557: loss 6.782627582550049\n",
      "Iteration 558: loss 6.101811408996582\n",
      "Iteration 559: loss 7.478124618530273\n",
      "Iteration 560: loss 7.547237396240234\n",
      "Iteration 561: loss 7.038954734802246\n",
      "Iteration 562: loss 7.258203983306885\n",
      "Iteration 563: loss 7.83201265335083\n",
      "Iteration 564: loss 8.97668170928955\n",
      "Iteration 565: loss 10.154963493347168\n",
      "Iteration 566: loss 8.343145370483398\n",
      "Iteration 567: loss 9.457357406616211\n",
      "Iteration 568: loss 8.429901123046875\n",
      "Iteration 569: loss 7.7154459953308105\n",
      "Iteration 570: loss 9.262173652648926\n",
      "Iteration 571: loss 6.402419090270996\n",
      "Iteration 572: loss 9.30758285522461\n",
      "Iteration 573: loss 8.272172927856445\n",
      "Iteration 574: loss 6.7596049308776855\n",
      "Iteration 575: loss 7.831451416015625\n",
      "Iteration 576: loss 8.710948944091797\n",
      "Iteration 577: loss 6.159282684326172\n",
      "Iteration 578: loss 6.408227920532227\n",
      "Iteration 579: loss 6.051218032836914\n",
      "Iteration 580: loss 6.223864555358887\n",
      "Iteration 581: loss 5.309741020202637\n",
      "Iteration 582: loss 5.660520076751709\n",
      "Iteration 583: loss 6.770035266876221\n",
      "Iteration 584: loss 7.27180290222168\n",
      "Iteration 585: loss 7.201303482055664\n",
      "Iteration 586: loss 6.975678443908691\n",
      "Iteration 587: loss 6.669670104980469\n",
      "Iteration 588: loss 7.0340256690979\n",
      "Iteration 589: loss 6.266201019287109\n",
      "Iteration 590: loss 5.9432806968688965\n",
      "Iteration 591: loss 5.627988815307617\n",
      "Iteration 592: loss 5.029951095581055\n",
      "Iteration 593: loss 5.326396465301514\n",
      "Iteration 594: loss 5.61820650100708\n",
      "Iteration 595: loss 6.577364444732666\n",
      "Iteration 596: loss 6.209792137145996\n",
      "Iteration 597: loss 5.921728610992432\n",
      "Iteration 598: loss 6.034224510192871\n",
      "Iteration 599: loss 5.2813334465026855\n",
      "Iteration 600: loss 6.524754524230957\n",
      "Iteration 601: loss 5.81399393081665\n",
      "Iteration 602: loss 5.943131446838379\n",
      "Iteration 603: loss 5.528163909912109\n",
      "Iteration 604: loss 5.851994514465332\n",
      "Iteration 605: loss 5.66761589050293\n",
      "Iteration 606: loss 5.952677249908447\n",
      "Iteration 607: loss 5.112010955810547\n",
      "Iteration 608: loss 4.772543430328369\n",
      "Iteration 609: loss 5.21614408493042\n",
      "Iteration 610: loss 5.092279434204102\n",
      "Iteration 611: loss 5.613833904266357\n",
      "Iteration 612: loss 5.737277507781982\n",
      "Iteration 613: loss 5.040513515472412\n",
      "Iteration 614: loss 5.791467189788818\n",
      "Iteration 615: loss 5.964688777923584\n",
      "Iteration 616: loss 4.664534091949463\n",
      "Iteration 617: loss 4.992955684661865\n",
      "Iteration 618: loss 4.876231670379639\n",
      "Iteration 619: loss 6.1648688316345215\n",
      "Iteration 620: loss 5.026240825653076\n",
      "Iteration 621: loss 5.980734348297119\n",
      "Iteration 622: loss 5.886902809143066\n",
      "Iteration 623: loss 5.510315418243408\n",
      "Iteration 624: loss 6.324878215789795\n",
      "Iteration 625: loss 4.95292329788208\n",
      "Iteration 626: loss 4.865935325622559\n",
      "Iteration 627: loss 5.238581657409668\n",
      "Iteration 628: loss 4.3501105308532715\n",
      "Iteration 629: loss 4.877754211425781\n",
      "Iteration 630: loss 5.038424968719482\n",
      "Iteration 631: loss 4.12669038772583\n",
      "Iteration 632: loss 5.369136333465576\n",
      "Iteration 633: loss 5.719008445739746\n",
      "Iteration 634: loss 4.512676239013672\n",
      "Iteration 635: loss 6.636985778808594\n",
      "Iteration 636: loss 4.743268966674805\n",
      "Iteration 637: loss 5.2110795974731445\n",
      "Iteration 638: loss 4.5958709716796875\n",
      "Iteration 639: loss 5.197635173797607\n",
      "Iteration 640: loss 4.842530250549316\n",
      "Iteration 641: loss 4.700367450714111\n",
      "Iteration 642: loss 4.298370838165283\n",
      "Iteration 643: loss 4.910184860229492\n",
      "Iteration 644: loss 4.153495788574219\n",
      "Iteration 645: loss 4.6695237159729\n",
      "Iteration 646: loss 4.42777156829834\n",
      "Iteration 647: loss 5.006838321685791\n",
      "Iteration 648: loss 4.961930274963379\n",
      "Iteration 649: loss 4.786158561706543\n",
      "Iteration 650: loss 4.773126125335693\n",
      "Iteration 651: loss 4.975677490234375\n",
      "Iteration 652: loss 4.752331256866455\n",
      "Iteration 653: loss 4.461548328399658\n",
      "Iteration 654: loss 4.603111267089844\n",
      "Iteration 655: loss 5.214751243591309\n",
      "Iteration 656: loss 5.984645843505859\n",
      "Iteration 657: loss 5.146378040313721\n",
      "Iteration 658: loss 4.664821624755859\n",
      "Iteration 659: loss 5.177446365356445\n",
      "Iteration 660: loss 5.022430419921875\n",
      "Iteration 661: loss 5.288868427276611\n",
      "Iteration 662: loss 5.125009536743164\n",
      "Iteration 663: loss 4.286006450653076\n",
      "Iteration 664: loss 4.206419467926025\n",
      "Iteration 665: loss 4.8770856857299805\n",
      "Iteration 666: loss 4.278448581695557\n",
      "Iteration 667: loss 4.8464484214782715\n",
      "Iteration 668: loss 3.944629430770874\n",
      "Iteration 669: loss 4.408375263214111\n",
      "Iteration 670: loss 3.9635348320007324\n",
      "Iteration 671: loss 4.198806285858154\n",
      "Iteration 672: loss 4.220866680145264\n",
      "Iteration 673: loss 4.065932750701904\n",
      "Iteration 674: loss 4.642183780670166\n",
      "Iteration 675: loss 4.1155829429626465\n",
      "Iteration 676: loss 3.9577791690826416\n",
      "Iteration 677: loss 3.738980293273926\n",
      "Iteration 678: loss 3.7166333198547363\n",
      "Iteration 679: loss 3.923275947570801\n",
      "Iteration 680: loss 3.6781535148620605\n",
      "Iteration 681: loss 4.001931190490723\n",
      "Iteration 682: loss 3.698044538497925\n",
      "Iteration 683: loss 3.4797985553741455\n",
      "Iteration 684: loss 3.5338478088378906\n",
      "Iteration 685: loss 3.376332998275757\n",
      "Iteration 686: loss 3.4881200790405273\n",
      "Iteration 687: loss 4.094590187072754\n",
      "Iteration 688: loss 4.037764549255371\n",
      "Iteration 689: loss 3.714695453643799\n",
      "Iteration 690: loss 4.0136399269104\n",
      "Iteration 691: loss 3.5295190811157227\n",
      "Iteration 692: loss 4.063196182250977\n",
      "Iteration 693: loss 4.24967098236084\n",
      "Iteration 694: loss 5.749223709106445\n",
      "Iteration 695: loss 5.548473834991455\n",
      "Iteration 696: loss 6.449411392211914\n",
      "Iteration 697: loss 7.6972270011901855\n",
      "Iteration 698: loss 6.952831745147705\n",
      "Iteration 699: loss 6.768290042877197\n",
      "Iteration 700: loss 6.97278356552124\n",
      "Iteration 701: loss 6.454456806182861\n",
      "Iteration 702: loss 4.8895697593688965\n",
      "Iteration 703: loss 4.943460464477539\n",
      "Iteration 704: loss 5.097480297088623\n",
      "Iteration 705: loss 4.520787715911865\n",
      "Iteration 706: loss 4.703052520751953\n",
      "Iteration 707: loss 5.556925296783447\n",
      "Iteration 708: loss 5.550285339355469\n",
      "Iteration 709: loss 5.786013126373291\n",
      "Iteration 710: loss 4.825560092926025\n",
      "Iteration 711: loss 5.064200401306152\n",
      "Iteration 712: loss 4.894858360290527\n",
      "Iteration 713: loss 5.031634330749512\n",
      "Iteration 714: loss 5.030550956726074\n",
      "Iteration 715: loss 4.4091410636901855\n",
      "Iteration 716: loss 4.264227390289307\n",
      "Iteration 717: loss 5.1648077964782715\n",
      "Iteration 718: loss 4.521707534790039\n",
      "Iteration 719: loss 4.745709419250488\n",
      "Iteration 720: loss 4.346004009246826\n",
      "Iteration 721: loss 4.391914367675781\n",
      "Iteration 722: loss 4.760411739349365\n",
      "Iteration 723: loss 4.035731792449951\n",
      "Iteration 724: loss 4.645277500152588\n",
      "Iteration 725: loss 3.7903378009796143\n",
      "Iteration 726: loss 4.518764019012451\n",
      "Iteration 727: loss 4.708033561706543\n",
      "Iteration 728: loss 4.86237907409668\n",
      "Iteration 729: loss 4.4413604736328125\n",
      "Iteration 730: loss 3.7648119926452637\n",
      "Iteration 731: loss 3.7831027507781982\n",
      "Iteration 732: loss 3.976353645324707\n",
      "Iteration 733: loss 4.214277744293213\n",
      "Iteration 734: loss 3.8598382472991943\n",
      "Iteration 735: loss 4.291111469268799\n",
      "Iteration 736: loss 4.587311267852783\n",
      "Iteration 737: loss 4.029849529266357\n",
      "Iteration 738: loss 4.377910137176514\n",
      "Iteration 739: loss 3.4011003971099854\n",
      "Iteration 740: loss 3.79587459564209\n",
      "Iteration 741: loss 3.8119823932647705\n",
      "Iteration 742: loss 4.261350154876709\n",
      "Iteration 743: loss 4.144744396209717\n",
      "Iteration 744: loss 3.9133877754211426\n",
      "Iteration 745: loss 3.5918450355529785\n",
      "Iteration 746: loss 3.4296834468841553\n",
      "Iteration 747: loss 3.600639581680298\n",
      "Iteration 748: loss 3.9245400428771973\n",
      "Iteration 749: loss 4.620062351226807\n",
      "Iteration 750: loss 3.7894527912139893\n",
      "Iteration 751: loss 3.9133830070495605\n",
      "Iteration 752: loss 3.7954766750335693\n",
      "Iteration 753: loss 3.7451438903808594\n",
      "Iteration 754: loss 3.115999698638916\n",
      "Iteration 755: loss 3.887845277786255\n",
      "Iteration 756: loss 3.555331230163574\n",
      "Iteration 757: loss 3.630542755126953\n",
      "Iteration 758: loss 4.306967735290527\n",
      "Iteration 759: loss 3.261195659637451\n",
      "Iteration 760: loss 3.984246015548706\n",
      "Iteration 761: loss 3.387709856033325\n",
      "Iteration 762: loss 3.7045016288757324\n",
      "Iteration 763: loss 3.628939151763916\n",
      "Iteration 764: loss 3.479499101638794\n",
      "Iteration 765: loss 3.305572986602783\n",
      "Iteration 766: loss 3.70285701751709\n",
      "Iteration 767: loss 2.9432451725006104\n",
      "Iteration 768: loss 3.760251998901367\n",
      "Iteration 769: loss 3.312747001647949\n",
      "Iteration 770: loss 2.9629223346710205\n",
      "Iteration 771: loss 3.904256582260132\n",
      "Iteration 772: loss 3.0739152431488037\n",
      "Iteration 773: loss 3.2005465030670166\n",
      "Iteration 774: loss 3.548720121383667\n",
      "Iteration 775: loss 2.9632177352905273\n",
      "Iteration 776: loss 3.4332962036132812\n",
      "Iteration 777: loss 3.1335461139678955\n",
      "Iteration 778: loss 3.68234920501709\n",
      "Iteration 779: loss 3.4292526245117188\n",
      "Iteration 780: loss 3.758769989013672\n",
      "Iteration 781: loss 3.489312171936035\n",
      "Iteration 782: loss 3.958582639694214\n",
      "Iteration 783: loss 3.4099247455596924\n",
      "Iteration 784: loss 3.4717020988464355\n",
      "Iteration 785: loss 3.385737180709839\n",
      "Iteration 786: loss 3.5936262607574463\n",
      "Iteration 787: loss 3.895268440246582\n",
      "Iteration 788: loss 3.448850393295288\n",
      "Iteration 789: loss 2.648353099822998\n",
      "Iteration 790: loss 3.130058526992798\n",
      "Iteration 791: loss 3.3367762565612793\n",
      "Iteration 792: loss 3.4746334552764893\n",
      "Iteration 793: loss 3.736189126968384\n",
      "Iteration 794: loss 3.37395977973938\n",
      "Iteration 795: loss 2.970128297805786\n",
      "Iteration 796: loss 3.5766372680664062\n",
      "Iteration 797: loss 4.9643473625183105\n",
      "Iteration 798: loss 4.780163288116455\n",
      "Iteration 799: loss 5.26063346862793\n",
      "Iteration 800: loss 5.149544715881348\n",
      "Iteration 801: loss 4.581053256988525\n",
      "Iteration 802: loss 5.420345306396484\n",
      "Iteration 803: loss 5.64078426361084\n",
      "Iteration 804: loss 4.86749267578125\n",
      "Iteration 805: loss 4.202906608581543\n",
      "Iteration 806: loss 4.150824069976807\n",
      "Iteration 807: loss 4.915898323059082\n",
      "Iteration 808: loss 6.100383758544922\n",
      "Iteration 809: loss 4.291433811187744\n",
      "Iteration 810: loss 5.123967170715332\n",
      "Iteration 811: loss 4.189498424530029\n",
      "Iteration 812: loss 3.91701340675354\n",
      "Iteration 813: loss 3.9690303802490234\n",
      "Iteration 814: loss 4.902841567993164\n",
      "Iteration 815: loss 4.225270748138428\n",
      "Iteration 816: loss 4.094231605529785\n",
      "Iteration 817: loss 4.4697771072387695\n",
      "Iteration 818: loss 4.033918857574463\n",
      "Iteration 819: loss 3.784893751144409\n",
      "Iteration 820: loss 3.9384145736694336\n",
      "Iteration 821: loss 4.365390300750732\n",
      "Iteration 822: loss 3.8404319286346436\n",
      "Iteration 823: loss 3.9603347778320312\n",
      "Iteration 824: loss 3.731140613555908\n",
      "Iteration 825: loss 3.438591241836548\n",
      "Iteration 826: loss 4.358715057373047\n",
      "Iteration 827: loss 3.7506582736968994\n",
      "Iteration 828: loss 4.649819850921631\n",
      "Iteration 829: loss 3.865739107131958\n",
      "Iteration 830: loss 3.9063949584960938\n",
      "Iteration 831: loss 3.8331897258758545\n",
      "Iteration 832: loss 3.6244781017303467\n",
      "Iteration 833: loss 3.3757715225219727\n",
      "Iteration 834: loss 3.6925976276397705\n",
      "Iteration 835: loss 3.6897599697113037\n",
      "Iteration 836: loss 3.9725451469421387\n",
      "Iteration 837: loss 3.275960683822632\n",
      "Iteration 838: loss 3.3043248653411865\n",
      "Iteration 839: loss 3.2157413959503174\n",
      "Iteration 840: loss 3.857462167739868\n",
      "Iteration 841: loss 3.420330047607422\n",
      "Iteration 842: loss 3.8290932178497314\n",
      "Iteration 843: loss 3.0649142265319824\n",
      "Iteration 844: loss 3.574547052383423\n",
      "Iteration 845: loss 3.62837290763855\n",
      "Iteration 846: loss 2.848036527633667\n",
      "Iteration 847: loss 3.770059108734131\n",
      "Iteration 848: loss 3.9465160369873047\n",
      "Iteration 849: loss 3.0087788105010986\n",
      "Iteration 850: loss 3.877142906188965\n",
      "Iteration 851: loss 3.534935474395752\n",
      "Iteration 852: loss 3.540381908416748\n",
      "Iteration 853: loss 3.67470121383667\n",
      "Iteration 854: loss 3.012803554534912\n",
      "Iteration 855: loss 2.7340033054351807\n",
      "Iteration 856: loss 3.045076608657837\n",
      "Iteration 857: loss 3.0710785388946533\n",
      "Iteration 858: loss 2.940284013748169\n",
      "Iteration 859: loss 3.20961856842041\n",
      "Iteration 860: loss 3.1645426750183105\n",
      "Iteration 861: loss 2.6605727672576904\n",
      "Iteration 862: loss 2.510532855987549\n",
      "Iteration 863: loss 2.984992504119873\n",
      "Iteration 864: loss 2.967984437942505\n",
      "Iteration 865: loss 3.205019235610962\n",
      "Iteration 866: loss 2.970055103302002\n",
      "Iteration 867: loss 2.708972692489624\n",
      "Iteration 868: loss 2.3610239028930664\n",
      "Iteration 869: loss 2.6977617740631104\n",
      "Iteration 870: loss 3.2113096714019775\n",
      "Iteration 871: loss 2.4982597827911377\n",
      "Iteration 872: loss 2.8830175399780273\n",
      "Iteration 873: loss 2.23449969291687\n",
      "Iteration 874: loss 2.980224609375\n",
      "Iteration 875: loss 2.4833569526672363\n",
      "Iteration 876: loss 3.046334981918335\n",
      "Iteration 877: loss 3.3910601139068604\n",
      "Iteration 878: loss 3.044865608215332\n",
      "Iteration 879: loss 2.5546836853027344\n",
      "Iteration 880: loss 2.8477141857147217\n",
      "Iteration 881: loss 4.434863090515137\n",
      "Iteration 882: loss 4.376287937164307\n",
      "Iteration 883: loss 3.668825626373291\n",
      "Iteration 884: loss 4.0607218742370605\n",
      "Iteration 885: loss 3.8666465282440186\n",
      "Iteration 886: loss 3.1036288738250732\n",
      "Iteration 887: loss 4.157065391540527\n",
      "Iteration 888: loss 3.992805004119873\n",
      "Iteration 889: loss 4.0667877197265625\n",
      "Iteration 890: loss 3.4113922119140625\n",
      "Iteration 891: loss 3.1682162284851074\n",
      "Iteration 892: loss 3.243593692779541\n",
      "Iteration 893: loss 4.0719804763793945\n",
      "Iteration 894: loss 3.65785551071167\n",
      "Iteration 895: loss 3.1037471294403076\n",
      "Iteration 896: loss 3.2489750385284424\n",
      "Iteration 897: loss 3.622739791870117\n",
      "Iteration 898: loss 3.294832706451416\n",
      "Iteration 899: loss 3.002519369125366\n",
      "Iteration 900: loss 2.5914077758789062\n",
      "Iteration 901: loss 2.814624547958374\n",
      "Iteration 902: loss 3.3188366889953613\n",
      "Iteration 903: loss 3.5993590354919434\n",
      "Iteration 904: loss 3.557634115219116\n",
      "Iteration 905: loss 3.214918851852417\n",
      "Iteration 906: loss 3.320652484893799\n",
      "Iteration 907: loss 3.7744600772857666\n",
      "Iteration 908: loss 4.36269998550415\n",
      "Iteration 909: loss 3.5356385707855225\n",
      "Iteration 910: loss 4.9122467041015625\n",
      "Iteration 911: loss 3.3448474407196045\n",
      "Iteration 912: loss 3.4757964611053467\n",
      "Iteration 913: loss 3.4510350227355957\n",
      "Iteration 914: loss 3.5320634841918945\n",
      "Iteration 915: loss 4.587808609008789\n",
      "Iteration 916: loss 3.647237539291382\n",
      "Iteration 917: loss 3.2768402099609375\n",
      "Iteration 918: loss 3.7045738697052\n",
      "Iteration 919: loss 3.0545220375061035\n",
      "Iteration 920: loss 3.099172353744507\n",
      "Iteration 921: loss 3.4108471870422363\n",
      "Iteration 922: loss 3.6819698810577393\n",
      "Iteration 923: loss 3.654430389404297\n",
      "Iteration 924: loss 3.493682861328125\n",
      "Iteration 925: loss 3.305756092071533\n",
      "Iteration 926: loss 3.3540666103363037\n",
      "Iteration 927: loss 3.0072720050811768\n",
      "Iteration 928: loss 3.43188738822937\n",
      "Iteration 929: loss 3.621377468109131\n",
      "Iteration 930: loss 3.935065269470215\n",
      "Iteration 931: loss 3.2585554122924805\n",
      "Iteration 932: loss 3.2177162170410156\n",
      "Iteration 933: loss 2.5910484790802\n",
      "Iteration 934: loss 2.806325674057007\n",
      "Iteration 935: loss 2.5703516006469727\n",
      "Iteration 936: loss 2.745548725128174\n",
      "Iteration 937: loss 3.1838066577911377\n",
      "Iteration 938: loss 2.9375195503234863\n",
      "Iteration 939: loss 2.635812282562256\n",
      "Iteration 940: loss 3.029228687286377\n",
      "Iteration 941: loss 3.703578472137451\n",
      "Iteration 942: loss 2.8976640701293945\n",
      "Iteration 943: loss 3.2574729919433594\n",
      "Iteration 944: loss 3.9704699516296387\n",
      "Iteration 945: loss 3.34920334815979\n",
      "Iteration 946: loss 2.8667404651641846\n",
      "Iteration 947: loss 3.1093156337738037\n",
      "Iteration 948: loss 3.670863151550293\n",
      "Iteration 949: loss 3.101008892059326\n",
      "Iteration 950: loss 2.9711735248565674\n",
      "Iteration 951: loss 3.388622283935547\n",
      "Iteration 952: loss 2.7801156044006348\n",
      "Iteration 953: loss 2.6269068717956543\n",
      "Iteration 954: loss 2.7904770374298096\n",
      "Iteration 955: loss 3.8947906494140625\n",
      "Iteration 956: loss 3.9731264114379883\n",
      "Iteration 957: loss 3.555328845977783\n",
      "Iteration 958: loss 3.827388286590576\n",
      "Iteration 959: loss 3.262418746948242\n",
      "Iteration 960: loss 3.496978759765625\n",
      "Iteration 961: loss 3.2875261306762695\n",
      "Iteration 962: loss 2.9834606647491455\n",
      "Iteration 963: loss 2.9201977252960205\n",
      "Iteration 964: loss 2.710836410522461\n",
      "Iteration 965: loss 3.1224513053894043\n",
      "Iteration 966: loss 3.1573455333709717\n",
      "Iteration 967: loss 2.905853748321533\n",
      "Iteration 968: loss 3.136584997177124\n",
      "Iteration 969: loss 2.5538110733032227\n",
      "Iteration 970: loss 2.1764049530029297\n",
      "Iteration 971: loss 2.4802727699279785\n",
      "Iteration 972: loss 3.2354307174682617\n",
      "Iteration 973: loss 2.7679686546325684\n",
      "Iteration 974: loss 3.2500877380371094\n",
      "Iteration 975: loss 2.2434935569763184\n",
      "Iteration 976: loss 2.7790136337280273\n",
      "Iteration 977: loss 2.7471606731414795\n",
      "Iteration 978: loss 2.026676654815674\n",
      "Iteration 979: loss 2.764836072921753\n",
      "Iteration 980: loss 2.77421236038208\n",
      "Iteration 981: loss 1.9377409219741821\n",
      "Iteration 982: loss 2.789832592010498\n",
      "Iteration 983: loss 2.225482225418091\n",
      "Iteration 984: loss 2.6784486770629883\n",
      "Iteration 985: loss 2.526090383529663\n",
      "Iteration 986: loss 2.5028624534606934\n",
      "Iteration 987: loss 2.907400369644165\n",
      "Iteration 988: loss 3.0093493461608887\n",
      "Iteration 989: loss 2.6891417503356934\n",
      "Iteration 990: loss 2.7039527893066406\n",
      "Iteration 991: loss 3.2980597019195557\n",
      "Iteration 992: loss 3.381808280944824\n",
      "Iteration 993: loss 2.6884093284606934\n",
      "Iteration 994: loss 2.21114182472229\n",
      "Iteration 995: loss 2.663914680480957\n",
      "Iteration 996: loss 3.4289026260375977\n",
      "Iteration 997: loss 2.958820104598999\n",
      "Iteration 998: loss 2.366231679916382\n",
      "Iteration 999: loss 2.6605217456817627\n",
      "Iteration 1000: loss 2.683704137802124\n",
      "Iteration 1001: loss 2.4397387504577637\n",
      "Iteration 1002: loss 2.9102678298950195\n",
      "Iteration 1003: loss 2.661267042160034\n",
      "Iteration 1004: loss 3.10260009765625\n",
      "Iteration 1005: loss 2.9482672214508057\n",
      "Iteration 1006: loss 2.502126693725586\n",
      "Iteration 1007: loss 2.4476070404052734\n",
      "Iteration 1008: loss 2.2854831218719482\n",
      "Iteration 1009: loss 2.64058256149292\n",
      "Iteration 1010: loss 2.5030174255371094\n",
      "Iteration 1011: loss 2.792066812515259\n",
      "Iteration 1012: loss 2.1651015281677246\n",
      "Iteration 1013: loss 2.408212423324585\n",
      "Iteration 1014: loss 2.6614890098571777\n",
      "Iteration 1015: loss 2.918016195297241\n",
      "Iteration 1016: loss 2.616583824157715\n",
      "Iteration 1017: loss 2.49625301361084\n",
      "Iteration 1018: loss 2.4556944370269775\n",
      "Iteration 1019: loss 2.2135469913482666\n",
      "Iteration 1020: loss 2.3661131858825684\n",
      "Iteration 1021: loss 2.256463050842285\n",
      "Iteration 1022: loss 2.089505195617676\n",
      "Iteration 1023: loss 2.338784694671631\n",
      "Iteration 1024: loss 2.0055387020111084\n",
      "Iteration 1025: loss 2.257763147354126\n",
      "Iteration 1026: loss 2.483860731124878\n",
      "Iteration 1027: loss 2.438958168029785\n",
      "Iteration 1028: loss 2.09232497215271\n",
      "Iteration 1029: loss 2.921018600463867\n",
      "Iteration 1030: loss 1.9884899854660034\n",
      "Iteration 1031: loss 2.50044846534729\n",
      "Iteration 1032: loss 2.083235025405884\n",
      "Iteration 1033: loss 2.8490819931030273\n",
      "Iteration 1034: loss 2.578782081604004\n",
      "Iteration 1035: loss 2.957554817199707\n",
      "Iteration 1036: loss 2.4885177612304688\n",
      "Iteration 1037: loss 2.1606595516204834\n",
      "Iteration 1038: loss 2.6191134452819824\n",
      "Iteration 1039: loss 2.4948999881744385\n",
      "Iteration 1040: loss 2.0550758838653564\n",
      "Iteration 1041: loss 1.8075251579284668\n",
      "Iteration 1042: loss 2.357083320617676\n",
      "Iteration 1043: loss 2.21744966506958\n",
      "Iteration 1044: loss 2.4738540649414062\n",
      "Iteration 1045: loss 2.734334945678711\n",
      "Iteration 1046: loss 2.3557615280151367\n",
      "Iteration 1047: loss 2.2727105617523193\n",
      "Iteration 1048: loss 2.3970463275909424\n",
      "Iteration 1049: loss 2.6392977237701416\n",
      "Iteration 1050: loss 2.233091115951538\n",
      "Iteration 1051: loss 1.9481139183044434\n",
      "Iteration 1052: loss 2.68196964263916\n",
      "Iteration 1053: loss 2.3539211750030518\n",
      "Iteration 1054: loss 2.776488780975342\n",
      "Iteration 1055: loss 2.3773696422576904\n",
      "Iteration 1056: loss 1.992900013923645\n",
      "Iteration 1057: loss 3.8915488719940186\n",
      "Iteration 1058: loss 3.5686585903167725\n",
      "Iteration 1059: loss 3.444042205810547\n",
      "Iteration 1060: loss 3.029264211654663\n",
      "Iteration 1061: loss 3.5831170082092285\n",
      "Iteration 1062: loss 3.8116469383239746\n",
      "Iteration 1063: loss 2.976245164871216\n",
      "Iteration 1064: loss 3.6614675521850586\n",
      "Iteration 1065: loss 4.433138847351074\n",
      "Iteration 1066: loss 2.8853907585144043\n",
      "Iteration 1067: loss 3.028818368911743\n",
      "Iteration 1068: loss 3.4440135955810547\n",
      "Iteration 1069: loss 3.673757791519165\n",
      "Iteration 1070: loss 2.9358861446380615\n",
      "Iteration 1071: loss 3.044701099395752\n",
      "Iteration 1072: loss 2.959322214126587\n",
      "Iteration 1073: loss 2.8116676807403564\n",
      "Iteration 1074: loss 2.2267935276031494\n",
      "Iteration 1075: loss 2.5872998237609863\n",
      "Iteration 1076: loss 2.5788726806640625\n",
      "Iteration 1077: loss 2.627239942550659\n",
      "Iteration 1078: loss 2.8483726978302\n",
      "Iteration 1079: loss 2.2929980754852295\n",
      "Iteration 1080: loss 2.355605125427246\n",
      "Iteration 1081: loss 2.540588855743408\n",
      "Iteration 1082: loss 2.556387186050415\n",
      "Iteration 1083: loss 2.396451950073242\n",
      "Iteration 1084: loss 2.4022774696350098\n",
      "Iteration 1085: loss 2.231900930404663\n",
      "Iteration 1086: loss 2.4613564014434814\n",
      "Iteration 1087: loss 1.9716222286224365\n",
      "Iteration 1088: loss 2.9079766273498535\n",
      "Iteration 1089: loss 2.2736458778381348\n",
      "Iteration 1090: loss 2.7003445625305176\n",
      "Iteration 1091: loss 2.504129648208618\n",
      "Iteration 1092: loss 2.005593776702881\n",
      "Iteration 1093: loss 2.443105936050415\n",
      "Iteration 1094: loss 2.905656576156616\n",
      "Iteration 1095: loss 2.6283209323883057\n",
      "Iteration 1096: loss 3.528506278991699\n",
      "Iteration 1097: loss 2.5965001583099365\n",
      "Iteration 1098: loss 3.5837783813476562\n",
      "Iteration 1099: loss 3.56488037109375\n",
      "Iteration 1100: loss 4.025029182434082\n",
      "Iteration 1101: loss 3.801569700241089\n",
      "Iteration 1102: loss 3.4079959392547607\n",
      "Iteration 1103: loss 3.2173101902008057\n",
      "Iteration 1104: loss 3.2601139545440674\n",
      "Iteration 1105: loss 3.2883739471435547\n",
      "Iteration 1106: loss 2.8700783252716064\n",
      "Iteration 1107: loss 3.3532607555389404\n",
      "Iteration 1108: loss 3.022176504135132\n",
      "Iteration 1109: loss 3.3420522212982178\n",
      "Iteration 1110: loss 3.0549373626708984\n",
      "Iteration 1111: loss 2.9833576679229736\n",
      "Iteration 1112: loss 3.0896289348602295\n",
      "Iteration 1113: loss 3.0944783687591553\n",
      "Iteration 1114: loss 3.58258056640625\n",
      "Iteration 1115: loss 3.393808603286743\n",
      "Iteration 1116: loss 2.821303606033325\n",
      "Iteration 1117: loss 2.434767007827759\n",
      "Iteration 1118: loss 2.4133338928222656\n",
      "Iteration 1119: loss 2.7257378101348877\n",
      "Iteration 1120: loss 3.0035502910614014\n",
      "Iteration 1121: loss 4.109133720397949\n",
      "Iteration 1122: loss 3.418720006942749\n",
      "Iteration 1123: loss 8.649131774902344\n",
      "Iteration 1124: loss 9.985211372375488\n",
      "Iteration 1125: loss 10.408373832702637\n",
      "Iteration 1126: loss 8.709038734436035\n",
      "Iteration 1127: loss 10.352043151855469\n",
      "Iteration 1128: loss 8.16164779663086\n",
      "Iteration 1129: loss 8.403430938720703\n",
      "Iteration 1130: loss 9.963202476501465\n",
      "Iteration 1131: loss 8.655412673950195\n",
      "Iteration 1132: loss 7.790978908538818\n",
      "Iteration 1133: loss 8.04617691040039\n",
      "Iteration 1134: loss 8.470138549804688\n",
      "Iteration 1135: loss 7.39217472076416\n",
      "Iteration 1136: loss 7.725472927093506\n",
      "Iteration 1137: loss 8.364449501037598\n",
      "Iteration 1138: loss 6.766848087310791\n",
      "Iteration 1139: loss 8.3045654296875\n",
      "Iteration 1140: loss 6.17441463470459\n",
      "Iteration 1141: loss 6.124021053314209\n",
      "Iteration 1142: loss 6.73624849319458\n",
      "Iteration 1143: loss 7.384130477905273\n",
      "Iteration 1144: loss 6.3232316970825195\n",
      "Iteration 1145: loss 8.000463485717773\n",
      "Iteration 1146: loss 7.094762802124023\n",
      "Iteration 1147: loss 5.654632568359375\n",
      "Iteration 1148: loss 6.717072010040283\n",
      "Iteration 1149: loss 7.532637119293213\n",
      "Iteration 1150: loss 5.622596263885498\n",
      "Iteration 1151: loss 7.154781341552734\n",
      "Iteration 1152: loss 4.520880222320557\n",
      "Iteration 1153: loss 5.478712558746338\n",
      "Iteration 1154: loss 4.13246488571167\n",
      "Iteration 1155: loss 4.491128444671631\n",
      "Iteration 1156: loss 5.403885364532471\n",
      "Iteration 1157: loss 4.294494152069092\n",
      "Iteration 1158: loss 4.617042541503906\n",
      "Iteration 1159: loss 4.582363605499268\n",
      "Iteration 1160: loss 5.571962833404541\n",
      "Iteration 1161: loss 4.910959243774414\n",
      "Iteration 1162: loss 4.441078186035156\n",
      "Iteration 1163: loss 4.19174861907959\n",
      "Iteration 1164: loss 4.12476110458374\n",
      "Iteration 1165: loss 4.560584545135498\n",
      "Iteration 1166: loss 4.977313995361328\n",
      "Iteration 1167: loss 3.6564810276031494\n",
      "Iteration 1168: loss 3.7619235515594482\n",
      "Iteration 1169: loss 4.862678527832031\n",
      "Iteration 1170: loss 4.972711086273193\n",
      "Iteration 1171: loss 4.032101631164551\n",
      "Iteration 1172: loss 3.0868706703186035\n",
      "Iteration 1173: loss 4.02264404296875\n",
      "Iteration 1174: loss 3.380747079849243\n",
      "Iteration 1175: loss 3.482725143432617\n",
      "Iteration 1176: loss 3.993917226791382\n",
      "Iteration 1177: loss 3.801811933517456\n",
      "Iteration 1178: loss 3.7200634479522705\n",
      "Iteration 1179: loss 5.486821174621582\n",
      "Iteration 1180: loss 3.593130111694336\n",
      "Iteration 1181: loss 4.725623607635498\n",
      "Iteration 1182: loss 5.2041473388671875\n",
      "Iteration 1183: loss 3.7812461853027344\n",
      "Iteration 1184: loss 3.814683675765991\n",
      "Iteration 1185: loss 3.593928813934326\n",
      "Iteration 1186: loss 4.1285080909729\n",
      "Iteration 1187: loss 4.827730178833008\n",
      "Iteration 1188: loss 5.898600101470947\n",
      "Iteration 1189: loss 4.743017196655273\n",
      "Iteration 1190: loss 4.065480709075928\n",
      "Iteration 1191: loss 4.477022171020508\n",
      "Iteration 1192: loss 4.970027446746826\n",
      "Iteration 1193: loss 4.702770709991455\n",
      "Iteration 1194: loss 4.760653972625732\n",
      "Iteration 1195: loss 4.810197830200195\n",
      "Iteration 1196: loss 4.067469596862793\n",
      "Iteration 1197: loss 4.9965901374816895\n",
      "Iteration 1198: loss 4.559268474578857\n",
      "Iteration 1199: loss 4.6074371337890625\n",
      "Iteration 1200: loss 4.070709228515625\n",
      "Iteration 1201: loss 3.640324354171753\n",
      "Iteration 1202: loss 4.43654727935791\n",
      "Iteration 1203: loss 3.9806694984436035\n",
      "Iteration 1204: loss 3.985970973968506\n",
      "Iteration 1205: loss 3.828986167907715\n",
      "Iteration 1206: loss 3.3433313369750977\n",
      "Iteration 1207: loss 4.2403974533081055\n",
      "Iteration 1208: loss 3.950967311859131\n",
      "Iteration 1209: loss 4.69315767288208\n",
      "Iteration 1210: loss 3.7511143684387207\n",
      "Iteration 1211: loss 4.212514400482178\n",
      "Iteration 1212: loss 4.1754608154296875\n",
      "Iteration 1213: loss 3.3354692459106445\n",
      "Iteration 1214: loss 4.073442459106445\n",
      "Iteration 1215: loss 3.772505760192871\n",
      "Iteration 1216: loss 3.608584403991699\n",
      "Iteration 1217: loss 4.080456256866455\n",
      "Iteration 1218: loss 3.3309898376464844\n",
      "Iteration 1219: loss 3.6510865688323975\n",
      "Iteration 1220: loss 3.4187092781066895\n",
      "Iteration 1221: loss 3.268805980682373\n",
      "Iteration 1222: loss 3.3732292652130127\n",
      "Iteration 1223: loss 3.673246383666992\n",
      "Iteration 1224: loss 3.561413526535034\n",
      "Iteration 1225: loss 4.261464595794678\n",
      "Iteration 1226: loss 3.281287431716919\n",
      "Iteration 1227: loss 3.1888046264648438\n",
      "Iteration 1228: loss 3.2403154373168945\n",
      "Iteration 1229: loss 3.317213773727417\n",
      "Iteration 1230: loss 3.5536205768585205\n",
      "Iteration 1231: loss 3.3152177333831787\n",
      "Iteration 1232: loss 3.3916409015655518\n",
      "Iteration 1233: loss 3.4240684509277344\n",
      "Iteration 1234: loss 3.2174837589263916\n",
      "Iteration 1235: loss 3.71449875831604\n",
      "Iteration 1236: loss 3.582735538482666\n",
      "Iteration 1237: loss 2.899829387664795\n",
      "Iteration 1238: loss 3.618398904800415\n",
      "Iteration 1239: loss 3.12442684173584\n",
      "Iteration 1240: loss 2.5366199016571045\n",
      "Iteration 1241: loss 2.4622466564178467\n",
      "Iteration 1242: loss 2.940108299255371\n",
      "Iteration 1243: loss 3.239757537841797\n",
      "Iteration 1244: loss 3.06113338470459\n",
      "Iteration 1245: loss 2.608107328414917\n",
      "Iteration 1246: loss 3.0642168521881104\n",
      "Iteration 1247: loss 3.0538201332092285\n",
      "Iteration 1248: loss 2.7339391708374023\n",
      "Iteration 1249: loss 3.254733085632324\n",
      "Iteration 1250: loss 2.9623405933380127\n",
      "Iteration 1251: loss 3.060614585876465\n",
      "Iteration 1252: loss 3.280137300491333\n",
      "Iteration 1253: loss 3.0512449741363525\n",
      "Iteration 1254: loss 4.27778959274292\n",
      "Iteration 1255: loss 3.7942991256713867\n",
      "Iteration 1256: loss 3.8820860385894775\n",
      "Iteration 1257: loss 2.3770530223846436\n",
      "Iteration 1258: loss 3.570463180541992\n",
      "Iteration 1259: loss 3.902238368988037\n",
      "Iteration 1260: loss 3.538161516189575\n",
      "Iteration 1261: loss 2.828496217727661\n",
      "Iteration 1262: loss 3.6029891967773438\n",
      "Iteration 1263: loss 3.6501426696777344\n",
      "Iteration 1264: loss 5.191685676574707\n",
      "Iteration 1265: loss 3.466144561767578\n",
      "Iteration 1266: loss 3.931419610977173\n",
      "Iteration 1267: loss 3.262481689453125\n",
      "Iteration 1268: loss 4.393030166625977\n",
      "Iteration 1269: loss 3.645073175430298\n",
      "Iteration 1270: loss 4.358642101287842\n",
      "Iteration 1271: loss 3.816152334213257\n",
      "Iteration 1272: loss 3.300037145614624\n",
      "Iteration 1273: loss 4.436227798461914\n",
      "Iteration 1274: loss 3.7768523693084717\n",
      "Iteration 1275: loss 3.752936601638794\n",
      "Iteration 1276: loss 2.48012113571167\n",
      "Iteration 1277: loss 2.9189932346343994\n",
      "Iteration 1278: loss 4.239842414855957\n",
      "Iteration 1279: loss 3.684509515762329\n",
      "Iteration 1280: loss 4.357390403747559\n",
      "Iteration 1281: loss 4.344124794006348\n",
      "Iteration 1282: loss 3.6960790157318115\n",
      "Iteration 1283: loss 4.727105140686035\n",
      "Iteration 1284: loss 3.416781187057495\n",
      "Iteration 1285: loss 4.178494453430176\n",
      "Iteration 1286: loss 4.17336893081665\n",
      "Iteration 1287: loss 4.145720958709717\n",
      "Iteration 1288: loss 3.5740089416503906\n",
      "Iteration 1289: loss 3.900897741317749\n",
      "Iteration 1290: loss 4.586224555969238\n",
      "Iteration 1291: loss 3.8498148918151855\n",
      "Iteration 1292: loss 3.6706724166870117\n",
      "Iteration 1293: loss 4.20211124420166\n",
      "Iteration 1294: loss 3.832460880279541\n",
      "Iteration 1295: loss 3.8021578788757324\n",
      "Iteration 1296: loss 3.6041808128356934\n",
      "Iteration 1297: loss 4.1312103271484375\n",
      "Iteration 1298: loss 4.478991985321045\n",
      "Iteration 1299: loss 4.064863204956055\n",
      "Iteration 1300: loss 3.640989303588867\n",
      "Iteration 1301: loss 4.097083568572998\n",
      "Iteration 1302: loss 4.204845428466797\n",
      "Iteration 1303: loss 5.563873291015625\n",
      "Iteration 1304: loss 3.2392616271972656\n",
      "Iteration 1305: loss 4.539449214935303\n",
      "Iteration 1306: loss 4.076736927032471\n",
      "Iteration 1307: loss 4.396788120269775\n",
      "Iteration 1308: loss 3.8842761516571045\n",
      "Iteration 1309: loss 3.3549890518188477\n",
      "Iteration 1310: loss 4.119848728179932\n",
      "Iteration 1311: loss 4.732499122619629\n",
      "Iteration 1312: loss 4.064059257507324\n",
      "Iteration 1313: loss 4.180909156799316\n",
      "Iteration 1314: loss 4.607601165771484\n",
      "Iteration 1315: loss 3.745648145675659\n",
      "Iteration 1316: loss 3.8246898651123047\n",
      "Iteration 1317: loss 3.619112491607666\n",
      "Iteration 1318: loss 4.831885814666748\n",
      "Iteration 1319: loss 4.904397487640381\n",
      "Iteration 1320: loss 4.1333770751953125\n",
      "Iteration 1321: loss 4.056148529052734\n",
      "Iteration 1322: loss 4.033722400665283\n",
      "Iteration 1323: loss 3.1549389362335205\n",
      "Iteration 1324: loss 3.821720838546753\n",
      "Iteration 1325: loss 3.22172474861145\n",
      "Iteration 1326: loss 2.892226457595825\n",
      "Iteration 1327: loss 3.5272412300109863\n",
      "Iteration 1328: loss 3.6827170848846436\n",
      "Iteration 1329: loss 3.831373929977417\n",
      "Iteration 1330: loss 3.624584913253784\n",
      "Iteration 1331: loss 3.2402849197387695\n",
      "Iteration 1332: loss 3.640192985534668\n",
      "Iteration 1333: loss 3.871605634689331\n",
      "Iteration 1334: loss 3.822017192840576\n",
      "Iteration 1335: loss 3.627582311630249\n",
      "Iteration 1336: loss 3.6520564556121826\n",
      "Iteration 1337: loss 3.48669171333313\n",
      "Iteration 1338: loss 3.5420515537261963\n",
      "Iteration 1339: loss 3.2237374782562256\n",
      "Iteration 1340: loss 3.562748908996582\n",
      "Iteration 1341: loss 3.671722412109375\n",
      "Iteration 1342: loss 2.963796377182007\n",
      "Iteration 1343: loss 2.713801145553589\n",
      "Iteration 1344: loss 3.014232635498047\n",
      "Iteration 1345: loss 2.94806170463562\n",
      "Iteration 1346: loss 2.739358425140381\n",
      "Iteration 1347: loss 2.4525647163391113\n",
      "Iteration 1348: loss 2.77974271774292\n",
      "Iteration 1349: loss 2.6280009746551514\n",
      "Iteration 1350: loss 2.6454391479492188\n",
      "Iteration 1351: loss 2.4109067916870117\n",
      "Iteration 1352: loss 2.582554340362549\n",
      "Iteration 1353: loss 2.4733312129974365\n",
      "Iteration 1354: loss 2.313703775405884\n",
      "Iteration 1355: loss 2.4844119548797607\n",
      "Iteration 1356: loss 2.702572822570801\n",
      "Iteration 1357: loss 2.5741233825683594\n",
      "Iteration 1358: loss 2.085170030593872\n",
      "Iteration 1359: loss 2.2199172973632812\n",
      "Iteration 1360: loss 2.5634822845458984\n",
      "Iteration 1361: loss 2.751784324645996\n",
      "Iteration 1362: loss 2.6371371746063232\n",
      "Iteration 1363: loss 2.217993974685669\n",
      "Iteration 1364: loss 1.6610044240951538\n",
      "Iteration 1365: loss 2.015005111694336\n",
      "Iteration 1366: loss 2.0864133834838867\n",
      "Iteration 1367: loss 2.3576388359069824\n",
      "Iteration 1368: loss 2.0641603469848633\n",
      "Iteration 1369: loss 2.266800880432129\n",
      "Iteration 1370: loss 2.1979827880859375\n",
      "Iteration 1371: loss 2.8240060806274414\n",
      "Iteration 1372: loss 2.2207186222076416\n",
      "Iteration 1373: loss 2.0849664211273193\n",
      "Iteration 1374: loss 2.0790657997131348\n",
      "Iteration 1375: loss 2.8225529193878174\n",
      "Iteration 1376: loss 2.0260958671569824\n",
      "Iteration 1377: loss 1.915671944618225\n",
      "Iteration 1378: loss 1.8108210563659668\n",
      "Iteration 1379: loss 2.076669454574585\n",
      "Iteration 1380: loss 1.6776314973831177\n",
      "Iteration 1381: loss 1.9508289098739624\n",
      "Iteration 1382: loss 1.932814598083496\n",
      "Iteration 1383: loss 1.8106380701065063\n",
      "Iteration 1384: loss 2.1523823738098145\n",
      "Iteration 1385: loss 2.810450315475464\n",
      "Iteration 1386: loss 2.0648715496063232\n",
      "Iteration 1387: loss 1.9278011322021484\n",
      "Iteration 1388: loss 2.243089437484741\n",
      "Iteration 1389: loss 2.2265424728393555\n",
      "Iteration 1390: loss 1.9131747484207153\n",
      "Iteration 1391: loss 2.1288180351257324\n",
      "Iteration 1392: loss 1.861236572265625\n",
      "Iteration 1393: loss 2.59395694732666\n",
      "Iteration 1394: loss 1.6633918285369873\n",
      "Iteration 1395: loss 2.0477864742279053\n",
      "Iteration 1396: loss 2.6255385875701904\n",
      "Iteration 1397: loss 2.123889207839966\n",
      "Iteration 1398: loss 1.9322123527526855\n",
      "Iteration 1399: loss 2.608751058578491\n",
      "Iteration 1400: loss 2.7360808849334717\n",
      "Iteration 1401: loss 2.1146764755249023\n",
      "Iteration 1402: loss 2.206801414489746\n",
      "Iteration 1403: loss 2.22275447845459\n",
      "Iteration 1404: loss 1.819051742553711\n",
      "Iteration 1405: loss 2.123002290725708\n",
      "Iteration 1406: loss 1.8340963125228882\n",
      "Iteration 1407: loss 2.0998332500457764\n",
      "Iteration 1408: loss 1.6895965337753296\n",
      "Iteration 1409: loss 1.9022334814071655\n",
      "Iteration 1410: loss 1.8006261587142944\n",
      "Iteration 1411: loss 1.931552529335022\n",
      "Iteration 1412: loss 2.4752888679504395\n",
      "Iteration 1413: loss 1.8222429752349854\n",
      "Iteration 1414: loss 1.5782134532928467\n",
      "Iteration 1415: loss 1.9225051403045654\n",
      "Iteration 1416: loss 1.9868923425674438\n",
      "Iteration 1417: loss 2.0498225688934326\n",
      "Iteration 1418: loss 1.8495876789093018\n",
      "Iteration 1419: loss 2.010361909866333\n",
      "Iteration 1420: loss 1.915716528892517\n",
      "Iteration 1421: loss 1.746327519416809\n",
      "Iteration 1422: loss 1.9648913145065308\n",
      "Iteration 1423: loss 1.5770411491394043\n",
      "Iteration 1424: loss 2.0783324241638184\n",
      "Iteration 1425: loss 1.4644790887832642\n",
      "Iteration 1426: loss 2.037297010421753\n",
      "Iteration 1427: loss 2.0923314094543457\n",
      "Iteration 1428: loss 1.4194833040237427\n",
      "Iteration 1429: loss 2.015908718109131\n",
      "Iteration 1430: loss 1.8472559452056885\n",
      "Iteration 1431: loss 1.959599256515503\n",
      "Iteration 1432: loss 2.4960951805114746\n",
      "Iteration 1433: loss 2.5685667991638184\n",
      "Iteration 1434: loss 2.3315131664276123\n",
      "Iteration 1435: loss 2.3052871227264404\n",
      "Iteration 1436: loss 1.9730559587478638\n",
      "Iteration 1437: loss 2.3355774879455566\n",
      "Iteration 1438: loss 1.8717821836471558\n",
      "Iteration 1439: loss 2.097362518310547\n",
      "Iteration 1440: loss 1.9769407510757446\n",
      "Iteration 1441: loss 2.094702959060669\n",
      "Iteration 1442: loss 1.8559696674346924\n",
      "Iteration 1443: loss 1.502756118774414\n",
      "Iteration 1444: loss 2.032191514968872\n",
      "Iteration 1445: loss 2.0761308670043945\n",
      "Iteration 1446: loss 2.3337318897247314\n",
      "Iteration 1447: loss 2.4249606132507324\n",
      "Iteration 1448: loss 1.9627352952957153\n",
      "Iteration 1449: loss 1.7065423727035522\n",
      "Iteration 1450: loss 1.8305171728134155\n",
      "Iteration 1451: loss 2.0660510063171387\n",
      "Iteration 1452: loss 1.8075517416000366\n",
      "Iteration 1453: loss 1.9376174211502075\n",
      "Iteration 1454: loss 2.230844020843506\n",
      "Iteration 1455: loss 2.134023427963257\n",
      "Iteration 1456: loss 2.0085513591766357\n",
      "Iteration 1457: loss 1.9343490600585938\n",
      "Iteration 1458: loss 2.3199496269226074\n",
      "Iteration 1459: loss 2.216221570968628\n",
      "Iteration 1460: loss 1.7345670461654663\n",
      "Iteration 1461: loss 2.1808454990386963\n",
      "Iteration 1462: loss 1.4130297899246216\n",
      "Iteration 1463: loss 1.3659151792526245\n",
      "Iteration 1464: loss 2.2769062519073486\n",
      "Iteration 1465: loss 2.142270565032959\n",
      "Iteration 1466: loss 1.6701799631118774\n",
      "Iteration 1467: loss 1.739310622215271\n",
      "Iteration 1468: loss 1.9337551593780518\n",
      "Iteration 1469: loss 2.618638277053833\n",
      "Iteration 1470: loss 1.9814203977584839\n",
      "Iteration 1471: loss 2.067861557006836\n",
      "Iteration 1472: loss 2.0358831882476807\n",
      "Iteration 1473: loss 1.6798003911972046\n",
      "Iteration 1474: loss 1.4641414880752563\n",
      "Iteration 1475: loss 1.6123108863830566\n",
      "Iteration 1476: loss 1.864394187927246\n",
      "Iteration 1477: loss 1.8275911808013916\n",
      "Iteration 1478: loss 2.496657371520996\n",
      "Iteration 1479: loss 2.103102684020996\n",
      "Iteration 1480: loss 2.0612385272979736\n",
      "Iteration 1481: loss 2.2455365657806396\n",
      "Iteration 1482: loss 2.201244831085205\n",
      "Iteration 1483: loss 1.687022089958191\n",
      "Iteration 1484: loss 1.9336808919906616\n",
      "Iteration 1485: loss 3.088266611099243\n",
      "Iteration 1486: loss 2.0668485164642334\n",
      "Iteration 1487: loss 2.435462236404419\n",
      "Iteration 1488: loss 1.7945213317871094\n",
      "Iteration 1489: loss 1.9762624502182007\n",
      "Iteration 1490: loss 3.245231866836548\n",
      "Iteration 1491: loss 5.940222263336182\n",
      "Iteration 1492: loss 9.35883617401123\n",
      "Iteration 1493: loss 9.743696212768555\n",
      "Iteration 1494: loss 10.530519485473633\n",
      "Iteration 1495: loss 9.801400184631348\n",
      "Iteration 1496: loss 9.172343254089355\n",
      "Iteration 1497: loss 8.796774864196777\n",
      "Iteration 1498: loss 7.096222877502441\n",
      "Iteration 1499: loss 6.928840160369873\n",
      "Iteration 1500: loss 6.205215930938721\n",
      "Iteration 1501: loss 6.69392728805542\n",
      "Iteration 1502: loss 5.299617767333984\n",
      "Iteration 1503: loss 6.051680564880371\n",
      "Iteration 1504: loss 5.540830135345459\n",
      "Iteration 1505: loss 5.474260330200195\n",
      "Iteration 1506: loss 5.833469867706299\n",
      "Iteration 1507: loss 5.489467620849609\n",
      "Iteration 1508: loss 4.699551582336426\n",
      "Iteration 1509: loss 5.710997104644775\n",
      "Iteration 1510: loss 4.429261684417725\n",
      "Iteration 1511: loss 4.607812881469727\n",
      "Iteration 1512: loss 4.582517623901367\n",
      "Iteration 1513: loss 4.71284818649292\n",
      "Iteration 1514: loss 4.671835899353027\n",
      "Iteration 1515: loss 5.2032389640808105\n",
      "Iteration 1516: loss 4.37241792678833\n",
      "Iteration 1517: loss 4.786335468292236\n",
      "Iteration 1518: loss 5.253237247467041\n",
      "Iteration 1519: loss 4.779748439788818\n",
      "Iteration 1520: loss 4.207657337188721\n",
      "Iteration 1521: loss 4.55334997177124\n",
      "Iteration 1522: loss 4.563660144805908\n",
      "Iteration 1523: loss 3.985443592071533\n",
      "Iteration 1524: loss 4.215339660644531\n",
      "Iteration 1525: loss 4.591320514678955\n",
      "Iteration 1526: loss 4.160078525543213\n",
      "Iteration 1527: loss 3.6414952278137207\n",
      "Iteration 1528: loss 3.36716628074646\n",
      "Iteration 1529: loss 4.387224197387695\n",
      "Iteration 1530: loss 3.511653423309326\n",
      "Iteration 1531: loss 3.7336037158966064\n",
      "Iteration 1532: loss 3.4778685569763184\n",
      "Iteration 1533: loss 3.700798273086548\n",
      "Iteration 1534: loss 3.503338575363159\n",
      "Iteration 1535: loss 3.2786576747894287\n",
      "Iteration 1536: loss 3.1351916790008545\n",
      "Iteration 1537: loss 3.779623031616211\n",
      "Iteration 1538: loss 3.7377400398254395\n",
      "Iteration 1539: loss 3.514164924621582\n",
      "Iteration 1540: loss 3.786088228225708\n",
      "Iteration 1541: loss 3.260608434677124\n",
      "Iteration 1542: loss 4.278079509735107\n",
      "Iteration 1543: loss 5.018697738647461\n",
      "Iteration 1544: loss 4.220202922821045\n",
      "Iteration 1545: loss 4.124223709106445\n",
      "Iteration 1546: loss 4.058298110961914\n",
      "Iteration 1547: loss 3.8687658309936523\n",
      "Iteration 1548: loss 3.200695037841797\n",
      "Iteration 1549: loss 3.82700514793396\n",
      "Iteration 1550: loss 3.726724863052368\n",
      "Iteration 1551: loss 3.5474963188171387\n",
      "Iteration 1552: loss 4.104568958282471\n",
      "Iteration 1553: loss 3.929478883743286\n",
      "Iteration 1554: loss 4.243856906890869\n",
      "Iteration 1555: loss 3.4261844158172607\n",
      "Iteration 1556: loss 3.424974203109741\n",
      "Iteration 1557: loss 3.670494556427002\n",
      "Iteration 1558: loss 3.6182878017425537\n",
      "Iteration 1559: loss 3.181425094604492\n",
      "Iteration 1560: loss 3.2879629135131836\n",
      "Iteration 1561: loss 2.7116994857788086\n",
      "Iteration 1562: loss 3.5181829929351807\n",
      "Iteration 1563: loss 3.966939926147461\n",
      "Iteration 1564: loss 3.9202880859375\n",
      "Iteration 1565: loss 3.3536670207977295\n",
      "Iteration 1566: loss 3.5355005264282227\n",
      "Iteration 1567: loss 3.8452720642089844\n",
      "Iteration 1568: loss 3.6429076194763184\n",
      "Iteration 1569: loss 3.361055850982666\n",
      "Iteration 1570: loss 3.9168922901153564\n",
      "Iteration 1571: loss 4.1846160888671875\n",
      "Iteration 1572: loss 3.923600912094116\n",
      "Iteration 1573: loss 3.4476373195648193\n",
      "Iteration 1574: loss 3.675368070602417\n",
      "Iteration 1575: loss 4.017187595367432\n",
      "Iteration 1576: loss 4.029921054840088\n",
      "Iteration 1577: loss 4.014828681945801\n",
      "Iteration 1578: loss 3.855478525161743\n",
      "Iteration 1579: loss 4.050670146942139\n",
      "Iteration 1580: loss 3.7541067600250244\n",
      "Iteration 1581: loss 3.619802713394165\n",
      "Iteration 1582: loss 3.490631103515625\n",
      "Iteration 1583: loss 3.5192437171936035\n",
      "Iteration 1584: loss 3.4192075729370117\n",
      "Iteration 1585: loss 3.062852621078491\n",
      "Iteration 1586: loss 3.370356321334839\n",
      "Iteration 1587: loss 3.6298272609710693\n",
      "Iteration 1588: loss 3.1007256507873535\n",
      "Iteration 1589: loss 3.2519519329071045\n",
      "Iteration 1590: loss 3.4344940185546875\n",
      "Iteration 1591: loss 3.169790267944336\n",
      "Iteration 1592: loss 3.5705206394195557\n",
      "Iteration 1593: loss 2.813682794570923\n",
      "Iteration 1594: loss 2.58254075050354\n",
      "Iteration 1595: loss 2.8537979125976562\n",
      "Iteration 1596: loss 2.749098062515259\n",
      "Iteration 1597: loss 3.1827456951141357\n",
      "Iteration 1598: loss 3.1333301067352295\n",
      "Iteration 1599: loss 2.689746618270874\n",
      "Iteration 1600: loss 3.2001495361328125\n",
      "Iteration 1601: loss 3.4284069538116455\n",
      "Iteration 1602: loss 2.575606107711792\n",
      "Iteration 1603: loss 3.1704156398773193\n",
      "Iteration 1604: loss 3.0352783203125\n",
      "Iteration 1605: loss 3.058577537536621\n",
      "Iteration 1606: loss 2.066596269607544\n",
      "Iteration 1607: loss 2.6232872009277344\n",
      "Iteration 1608: loss 2.4419965744018555\n",
      "Iteration 1609: loss 2.712205171585083\n",
      "Iteration 1610: loss 2.9770543575286865\n",
      "Iteration 1611: loss 2.5482945442199707\n",
      "Iteration 1612: loss 2.820141315460205\n",
      "Iteration 1613: loss 2.4291458129882812\n",
      "Iteration 1614: loss 2.6033952236175537\n",
      "Iteration 1615: loss 2.4947800636291504\n",
      "Iteration 1616: loss 2.178490400314331\n",
      "Iteration 1617: loss 3.4176080226898193\n",
      "Iteration 1618: loss 2.419875383377075\n",
      "Iteration 1619: loss 2.744464635848999\n",
      "Iteration 1620: loss 2.28104567527771\n",
      "Iteration 1621: loss 2.5181353092193604\n",
      "Iteration 1622: loss 2.51367449760437\n",
      "Iteration 1623: loss 2.2554025650024414\n",
      "Iteration 1624: loss 2.3102505207061768\n",
      "Iteration 1625: loss 2.6670305728912354\n",
      "Iteration 1626: loss 2.5555810928344727\n",
      "Iteration 1627: loss 2.196070432662964\n",
      "Iteration 1628: loss 2.4715969562530518\n",
      "Iteration 1629: loss 2.85355281829834\n",
      "Iteration 1630: loss 2.1737098693847656\n",
      "Iteration 1631: loss 2.7109947204589844\n",
      "Iteration 1632: loss 2.281205654144287\n",
      "Iteration 1633: loss 2.162029266357422\n",
      "Iteration 1634: loss 1.715839147567749\n",
      "Iteration 1635: loss 2.4532036781311035\n",
      "Iteration 1636: loss 1.9691152572631836\n",
      "Iteration 1637: loss 1.8938552141189575\n",
      "Iteration 1638: loss 2.0037841796875\n",
      "Iteration 1639: loss 2.632486581802368\n",
      "Iteration 1640: loss 2.1543612480163574\n",
      "Iteration 1641: loss 1.8416332006454468\n",
      "Iteration 1642: loss 2.331947088241577\n",
      "Iteration 1643: loss 2.404015064239502\n",
      "Iteration 1644: loss 2.3140814304351807\n",
      "Iteration 1645: loss 2.55434250831604\n",
      "Iteration 1646: loss 2.089531660079956\n",
      "Iteration 1647: loss 2.3512120246887207\n",
      "Iteration 1648: loss 2.2456464767456055\n",
      "Iteration 1649: loss 2.101205587387085\n",
      "Iteration 1650: loss 2.4711077213287354\n",
      "Iteration 1651: loss 2.3628947734832764\n",
      "Iteration 1652: loss 1.8464921712875366\n",
      "Iteration 1653: loss 1.8583070039749146\n",
      "Iteration 1654: loss 2.370786428451538\n",
      "Iteration 1655: loss 2.1264429092407227\n",
      "Iteration 1656: loss 2.095191240310669\n",
      "Iteration 1657: loss 2.132678747177124\n",
      "Iteration 1658: loss 2.313164234161377\n",
      "Iteration 1659: loss 1.681710124015808\n",
      "Iteration 1660: loss 1.9566677808761597\n",
      "Iteration 1661: loss 1.6892255544662476\n",
      "Iteration 1662: loss 2.222879648208618\n",
      "Iteration 1663: loss 1.982871413230896\n",
      "Iteration 1664: loss 1.7980233430862427\n",
      "Iteration 1665: loss 1.6558141708374023\n",
      "Iteration 1666: loss 1.6274186372756958\n",
      "Iteration 1667: loss 1.7986369132995605\n",
      "Iteration 1668: loss 1.9300686120986938\n",
      "Iteration 1669: loss 2.499459743499756\n",
      "Iteration 1670: loss 1.972490906715393\n",
      "Iteration 1671: loss 1.8146799802780151\n",
      "Iteration 1672: loss 3.3363397121429443\n",
      "Iteration 1673: loss 2.680612087249756\n",
      "Iteration 1674: loss 2.84401798248291\n",
      "Iteration 1675: loss 2.0628678798675537\n",
      "Iteration 1676: loss 2.4154605865478516\n",
      "Iteration 1677: loss 1.8909450769424438\n",
      "Iteration 1678: loss 1.9838742017745972\n",
      "Iteration 1679: loss 1.8452731370925903\n",
      "Iteration 1680: loss 1.8723284006118774\n",
      "Iteration 1681: loss 2.3721466064453125\n",
      "Iteration 1682: loss 2.377666473388672\n",
      "Iteration 1683: loss 1.9287372827529907\n",
      "Iteration 1684: loss 2.157611608505249\n",
      "Iteration 1685: loss 2.2108511924743652\n",
      "Iteration 1686: loss 1.5901033878326416\n",
      "Iteration 1687: loss 1.7629903554916382\n",
      "Iteration 1688: loss 1.6465662717819214\n",
      "Iteration 1689: loss 1.70850670337677\n",
      "Iteration 1690: loss 2.1378371715545654\n",
      "Iteration 1691: loss 1.7310341596603394\n",
      "Iteration 1692: loss 1.8268074989318848\n",
      "Iteration 1693: loss 1.537408709526062\n",
      "Iteration 1694: loss 1.4281203746795654\n",
      "Iteration 1695: loss 1.8444329500198364\n",
      "Iteration 1696: loss 1.8180330991744995\n",
      "Iteration 1697: loss 2.1915197372436523\n",
      "Iteration 1698: loss 1.354896068572998\n",
      "Iteration 1699: loss 2.001767873764038\n",
      "Iteration 1700: loss 2.019895076751709\n",
      "Iteration 1701: loss 1.6478110551834106\n",
      "Iteration 1702: loss 1.9815348386764526\n",
      "Iteration 1703: loss 2.024165630340576\n",
      "Iteration 1704: loss 1.3933007717132568\n",
      "Iteration 1705: loss 1.9627037048339844\n",
      "Iteration 1706: loss 2.1396725177764893\n",
      "Iteration 1707: loss 1.9891058206558228\n",
      "Iteration 1708: loss 1.8630036115646362\n",
      "Iteration 1709: loss 2.1110048294067383\n",
      "Iteration 1710: loss 1.7388712167739868\n",
      "Iteration 1711: loss 1.8065606355667114\n",
      "Iteration 1712: loss 1.5026154518127441\n",
      "Iteration 1713: loss 1.1889445781707764\n",
      "Iteration 1714: loss 1.7062914371490479\n",
      "Iteration 1715: loss 1.6745009422302246\n",
      "Iteration 1716: loss 1.7197856903076172\n",
      "Iteration 1717: loss 1.3908517360687256\n",
      "Iteration 1718: loss 2.068997383117676\n",
      "Iteration 1719: loss 1.9895037412643433\n",
      "Iteration 1720: loss 1.7484599351882935\n",
      "Iteration 1721: loss 1.3818084001541138\n",
      "Iteration 1722: loss 1.4648492336273193\n",
      "Iteration 1723: loss 1.6725149154663086\n",
      "Iteration 1724: loss 2.1696717739105225\n",
      "Iteration 1725: loss 1.8234237432479858\n",
      "Iteration 1726: loss 1.7533926963806152\n",
      "Iteration 1727: loss 1.266695499420166\n",
      "Iteration 1728: loss 1.9079720973968506\n",
      "Iteration 1729: loss 1.3573585748672485\n",
      "Iteration 1730: loss 1.4177205562591553\n",
      "Iteration 1731: loss 1.6494383811950684\n",
      "Iteration 1732: loss 1.7006906270980835\n",
      "Iteration 1733: loss 1.7869454622268677\n",
      "Iteration 1734: loss 1.6260497570037842\n",
      "Iteration 1735: loss 1.55216383934021\n",
      "Iteration 1736: loss 1.6971157789230347\n",
      "Iteration 1737: loss 1.5701720714569092\n",
      "Iteration 1738: loss 1.8714425563812256\n",
      "Iteration 1739: loss 1.7155557870864868\n",
      "Iteration 1740: loss 1.3613232374191284\n",
      "Iteration 1741: loss 1.419670820236206\n",
      "Iteration 1742: loss 1.2327308654785156\n",
      "Iteration 1743: loss 1.232125997543335\n",
      "Iteration 1744: loss 1.4137948751449585\n",
      "Iteration 1745: loss 1.6821523904800415\n",
      "Iteration 1746: loss 1.721391201019287\n",
      "Iteration 1747: loss 1.129259467124939\n",
      "Iteration 1748: loss 1.4003769159317017\n",
      "Iteration 1749: loss 1.4176665544509888\n",
      "Iteration 1750: loss 1.5353525876998901\n",
      "Iteration 1751: loss 1.3909870386123657\n",
      "Iteration 1752: loss 1.8645755052566528\n",
      "Iteration 1753: loss 1.6935820579528809\n",
      "Iteration 1754: loss 1.7615364789962769\n",
      "Iteration 1755: loss 1.1510794162750244\n",
      "Iteration 1756: loss 1.7360525131225586\n",
      "Iteration 1757: loss 1.4301567077636719\n",
      "Iteration 1758: loss 1.5766749382019043\n",
      "Iteration 1759: loss 1.587070107460022\n",
      "Iteration 1760: loss 2.0526394844055176\n",
      "Iteration 1761: loss 1.5141777992248535\n",
      "Iteration 1762: loss 1.214468240737915\n",
      "Iteration 1763: loss 1.7177680730819702\n",
      "Iteration 1764: loss 1.8014434576034546\n",
      "Iteration 1765: loss 1.8878358602523804\n",
      "Iteration 1766: loss 1.4709174633026123\n",
      "Iteration 1767: loss 1.5897512435913086\n",
      "Iteration 1768: loss 1.571738600730896\n",
      "Iteration 1769: loss 1.3208088874816895\n",
      "Iteration 1770: loss 1.4347976446151733\n",
      "Iteration 1771: loss 1.5228382349014282\n",
      "Iteration 1772: loss 2.5004019737243652\n",
      "Iteration 1773: loss 2.530247688293457\n",
      "Iteration 1774: loss 2.2233569622039795\n",
      "Iteration 1775: loss 3.1654672622680664\n",
      "Iteration 1776: loss 2.8980133533477783\n",
      "Iteration 1777: loss 2.6499388217926025\n",
      "Iteration 1778: loss 3.07433819770813\n",
      "Iteration 1779: loss 3.994737386703491\n",
      "Iteration 1780: loss 5.340236663818359\n",
      "Iteration 1781: loss 5.795833587646484\n",
      "Iteration 1782: loss 5.710741996765137\n",
      "Iteration 1783: loss 5.776029109954834\n",
      "Iteration 1784: loss 8.279919624328613\n",
      "Iteration 1785: loss 6.111854076385498\n",
      "Iteration 1786: loss 5.859842300415039\n",
      "Iteration 1787: loss 5.089080810546875\n",
      "Iteration 1788: loss 4.668037414550781\n",
      "Iteration 1789: loss 5.654835224151611\n",
      "Iteration 1790: loss 5.555662155151367\n",
      "Iteration 1791: loss 6.279526710510254\n",
      "Iteration 1792: loss 4.9382219314575195\n",
      "Iteration 1793: loss 4.798096179962158\n",
      "Iteration 1794: loss 6.12054443359375\n",
      "Iteration 1795: loss 6.864068508148193\n",
      "Iteration 1796: loss 7.467288017272949\n",
      "Iteration 1797: loss 7.59541130065918\n",
      "Iteration 1798: loss 7.104522705078125\n",
      "Iteration 1799: loss 6.683645248413086\n",
      "Iteration 1800: loss 9.10248851776123\n",
      "Iteration 1801: loss 7.117339134216309\n",
      "Iteration 1802: loss 7.1481218338012695\n",
      "Iteration 1803: loss 6.110307693481445\n",
      "Iteration 1804: loss 7.871317386627197\n",
      "Iteration 1805: loss 7.352190971374512\n",
      "Iteration 1806: loss 6.415810585021973\n",
      "Iteration 1807: loss 6.615318298339844\n",
      "Iteration 1808: loss 6.8946404457092285\n",
      "Iteration 1809: loss 6.120814800262451\n",
      "Iteration 1810: loss 6.715360641479492\n",
      "Iteration 1811: loss 6.726887226104736\n",
      "Iteration 1812: loss 5.8997483253479\n",
      "Iteration 1813: loss 6.569264888763428\n",
      "Iteration 1814: loss 5.023301124572754\n",
      "Iteration 1815: loss 6.105800151824951\n",
      "Iteration 1816: loss 5.937891483306885\n",
      "Iteration 1817: loss 4.988759517669678\n",
      "Iteration 1818: loss 5.7990593910217285\n",
      "Iteration 1819: loss 5.934873104095459\n",
      "Iteration 1820: loss 5.981936931610107\n",
      "Iteration 1821: loss 7.008311748504639\n",
      "Iteration 1822: loss 5.537148475646973\n",
      "Iteration 1823: loss 4.2025275230407715\n",
      "Iteration 1824: loss 6.478758811950684\n",
      "Iteration 1825: loss 4.627004146575928\n",
      "Iteration 1826: loss 5.5294694900512695\n",
      "Iteration 1827: loss 4.261021614074707\n",
      "Iteration 1828: loss 6.030556678771973\n",
      "Iteration 1829: loss 7.471700668334961\n",
      "Iteration 1830: loss 4.630123615264893\n",
      "Iteration 1831: loss 4.822112083435059\n",
      "Iteration 1832: loss 4.823772430419922\n",
      "Iteration 1833: loss 5.076109886169434\n",
      "Iteration 1834: loss 5.27777624130249\n",
      "Iteration 1835: loss 4.516286849975586\n",
      "Iteration 1836: loss 3.663588285446167\n",
      "Iteration 1837: loss 4.032926559448242\n",
      "Iteration 1838: loss 3.9767680168151855\n",
      "Iteration 1839: loss 4.32243537902832\n",
      "Iteration 1840: loss 4.057858943939209\n",
      "Iteration 1841: loss 3.8239922523498535\n",
      "Iteration 1842: loss 3.683830499649048\n",
      "Iteration 1843: loss 3.9341626167297363\n",
      "Iteration 1844: loss 3.6377532482147217\n",
      "Iteration 1845: loss 3.2621421813964844\n",
      "Iteration 1846: loss 3.494764566421509\n",
      "Iteration 1847: loss 3.9540319442749023\n",
      "Iteration 1848: loss 3.503169059753418\n",
      "Iteration 1849: loss 3.606724262237549\n",
      "Iteration 1850: loss 3.8410441875457764\n",
      "Iteration 1851: loss 3.8911242485046387\n",
      "Iteration 1852: loss 3.2345399856567383\n",
      "Iteration 1853: loss 3.1120598316192627\n",
      "Iteration 1854: loss 3.3436315059661865\n",
      "Iteration 1855: loss 3.405755043029785\n",
      "Iteration 1856: loss 3.3526628017425537\n",
      "Iteration 1857: loss 3.390719413757324\n",
      "Iteration 1858: loss 3.3059942722320557\n",
      "Iteration 1859: loss 2.989184856414795\n",
      "Iteration 1860: loss 2.85929799079895\n",
      "Iteration 1861: loss 2.692767858505249\n",
      "Iteration 1862: loss 3.18881893157959\n",
      "Iteration 1863: loss 2.917180061340332\n",
      "Iteration 1864: loss 3.226454496383667\n",
      "Iteration 1865: loss 3.2649126052856445\n",
      "Iteration 1866: loss 3.300596237182617\n",
      "Iteration 1867: loss 3.2071917057037354\n",
      "Iteration 1868: loss 2.824251651763916\n",
      "Iteration 1869: loss 2.966125965118408\n",
      "Iteration 1870: loss 2.758408784866333\n",
      "Iteration 1871: loss 2.9249696731567383\n",
      "Iteration 1872: loss 3.277122974395752\n",
      "Iteration 1873: loss 2.5941221714019775\n",
      "Iteration 1874: loss 2.7897331714630127\n",
      "Iteration 1875: loss 2.334092617034912\n",
      "Iteration 1876: loss 2.2968289852142334\n",
      "Iteration 1877: loss 2.3037829399108887\n",
      "Iteration 1878: loss 2.871863842010498\n",
      "Iteration 1879: loss 2.428403377532959\n",
      "Iteration 1880: loss 2.1823058128356934\n",
      "Iteration 1881: loss 2.4767611026763916\n",
      "Iteration 1882: loss 2.15678071975708\n",
      "Iteration 1883: loss 2.9392311573028564\n",
      "Iteration 1884: loss 2.2849020957946777\n",
      "Iteration 1885: loss 2.2886507511138916\n",
      "Iteration 1886: loss 2.798283815383911\n",
      "Iteration 1887: loss 2.310509204864502\n",
      "Iteration 1888: loss 2.2989141941070557\n",
      "Iteration 1889: loss 2.4119715690612793\n",
      "Iteration 1890: loss 2.3914828300476074\n",
      "Iteration 1891: loss 2.381344795227051\n",
      "Iteration 1892: loss 2.027604818344116\n",
      "Iteration 1893: loss 2.3043360710144043\n",
      "Iteration 1894: loss 1.9500789642333984\n",
      "Iteration 1895: loss 2.232114315032959\n",
      "Iteration 1896: loss 2.446345806121826\n",
      "Iteration 1897: loss 2.220357656478882\n",
      "Iteration 1898: loss 2.654008626937866\n",
      "Iteration 1899: loss 2.2892582416534424\n",
      "Iteration 1900: loss 2.126555919647217\n",
      "Iteration 1901: loss 2.501241683959961\n",
      "Iteration 1902: loss 2.3116161823272705\n",
      "Iteration 1903: loss 2.3881280422210693\n",
      "Iteration 1904: loss 2.240631580352783\n",
      "Iteration 1905: loss 1.9136135578155518\n",
      "Iteration 1906: loss 2.200039863586426\n",
      "Iteration 1907: loss 2.4725067615509033\n",
      "Iteration 1908: loss 2.084906578063965\n",
      "Iteration 1909: loss 1.9018492698669434\n",
      "Iteration 1910: loss 1.622286319732666\n",
      "Iteration 1911: loss 2.1677937507629395\n",
      "Iteration 1912: loss 2.020454168319702\n",
      "Iteration 1913: loss 1.84881591796875\n",
      "Iteration 1914: loss 2.090470552444458\n",
      "Iteration 1915: loss 1.9424179792404175\n",
      "Iteration 1916: loss 2.006878137588501\n",
      "Iteration 1917: loss 2.035845994949341\n",
      "Iteration 1918: loss 2.1235976219177246\n",
      "Iteration 1919: loss 2.371912717819214\n",
      "Iteration 1920: loss 1.754426121711731\n",
      "Iteration 1921: loss 1.8582611083984375\n",
      "Iteration 1922: loss 1.991591453552246\n",
      "Iteration 1923: loss 2.2183358669281006\n",
      "Iteration 1924: loss 2.414278030395508\n",
      "Iteration 1925: loss 2.0403037071228027\n",
      "Iteration 1926: loss 1.9573264122009277\n",
      "Iteration 1927: loss 2.169860601425171\n",
      "Iteration 1928: loss 2.466426134109497\n",
      "Iteration 1929: loss 2.1441574096679688\n",
      "Iteration 1930: loss 1.7881312370300293\n",
      "Iteration 1931: loss 2.066356658935547\n",
      "Iteration 1932: loss 2.5494601726531982\n",
      "Iteration 1933: loss 2.021071195602417\n",
      "Iteration 1934: loss 2.2829174995422363\n",
      "Iteration 1935: loss 2.1548776626586914\n",
      "Iteration 1936: loss 2.4555211067199707\n",
      "Iteration 1937: loss 2.2655606269836426\n",
      "Iteration 1938: loss 2.5548129081726074\n",
      "Iteration 1939: loss 2.5266265869140625\n",
      "Iteration 1940: loss 2.220893144607544\n",
      "Iteration 1941: loss 2.4433417320251465\n",
      "Iteration 1942: loss 2.4893829822540283\n",
      "Iteration 1943: loss 2.8550777435302734\n",
      "Iteration 1944: loss 3.3712804317474365\n",
      "Iteration 1945: loss 3.3094310760498047\n",
      "Iteration 1946: loss 2.8923933506011963\n",
      "Iteration 1947: loss 3.1112489700317383\n",
      "Iteration 1948: loss 3.389376401901245\n",
      "Iteration 1949: loss 3.711193799972534\n",
      "Iteration 1950: loss 3.8989293575286865\n",
      "Iteration 1951: loss 3.337711811065674\n",
      "Iteration 1952: loss 3.1921727657318115\n",
      "Iteration 1953: loss 3.2628979682922363\n",
      "Iteration 1954: loss 3.2118618488311768\n",
      "Iteration 1955: loss 3.4126651287078857\n",
      "Iteration 1956: loss 3.5820858478546143\n",
      "Iteration 1957: loss 3.9346563816070557\n",
      "Iteration 1958: loss 3.990426540374756\n",
      "Iteration 1959: loss 4.181426048278809\n",
      "Iteration 1960: loss 5.060245037078857\n",
      "Iteration 1961: loss 4.859536170959473\n",
      "Iteration 1962: loss 3.717217206954956\n",
      "Iteration 1963: loss 4.286252498626709\n",
      "Iteration 1964: loss 4.435306072235107\n",
      "Iteration 1965: loss 4.093495845794678\n",
      "Iteration 1966: loss 4.701005935668945\n",
      "Iteration 1967: loss 4.791671276092529\n",
      "Iteration 1968: loss 4.1483306884765625\n",
      "Iteration 1969: loss 3.460887908935547\n",
      "Iteration 1970: loss 4.2847185134887695\n",
      "Iteration 1971: loss 3.945451021194458\n",
      "Iteration 1972: loss 3.9678611755371094\n",
      "Iteration 1973: loss 3.6858227252960205\n",
      "Iteration 1974: loss 3.8081395626068115\n",
      "Iteration 1975: loss 3.4931182861328125\n",
      "Iteration 1976: loss 4.032266616821289\n",
      "Iteration 1977: loss 3.626981735229492\n",
      "Iteration 1978: loss 3.7310075759887695\n",
      "Iteration 1979: loss 3.9192161560058594\n",
      "Iteration 1980: loss 3.5950429439544678\n",
      "Iteration 1981: loss 3.907231330871582\n",
      "Iteration 1982: loss 4.6460137367248535\n",
      "Iteration 1983: loss 4.452120304107666\n",
      "Iteration 1984: loss 3.9632551670074463\n",
      "Iteration 1985: loss 4.078457355499268\n",
      "Iteration 1986: loss 4.67317008972168\n",
      "Iteration 1987: loss 3.996779680252075\n",
      "Iteration 1988: loss 4.281792640686035\n",
      "Iteration 1989: loss 4.240836143493652\n",
      "Iteration 1990: loss 3.5343050956726074\n",
      "Iteration 1991: loss 3.284666061401367\n",
      "Iteration 1992: loss 3.8202016353607178\n",
      "Iteration 1993: loss 4.2818732261657715\n",
      "Iteration 1994: loss 3.428182363510132\n",
      "Iteration 1995: loss 3.5732405185699463\n",
      "Iteration 1996: loss 3.9539244174957275\n",
      "Iteration 1997: loss 3.6971850395202637\n",
      "Iteration 1998: loss 3.472834348678589\n",
      "Iteration 1999: loss 3.3311963081359863\n",
      "Iteration 2000: loss 3.727450370788574\n",
      "Iteration 2001: loss 3.4864556789398193\n",
      "Iteration 2002: loss 2.93325138092041\n",
      "Iteration 2003: loss 3.7468414306640625\n",
      "Iteration 2004: loss 3.285281181335449\n",
      "Iteration 2005: loss 3.173933744430542\n",
      "Iteration 2006: loss 3.4807989597320557\n",
      "Iteration 2007: loss 3.0923259258270264\n",
      "Iteration 2008: loss 3.2017338275909424\n",
      "Iteration 2009: loss 3.0763299465179443\n",
      "Iteration 2010: loss 3.292374610900879\n",
      "Iteration 2011: loss 3.147176742553711\n",
      "Iteration 2012: loss 2.9827308654785156\n",
      "Iteration 2013: loss 3.0843443870544434\n",
      "Iteration 2014: loss 3.6130449771881104\n",
      "Iteration 2015: loss 2.7048873901367188\n",
      "Iteration 2016: loss 2.593179941177368\n",
      "Iteration 2017: loss 2.6034438610076904\n",
      "Iteration 2018: loss 2.9250407218933105\n",
      "Iteration 2019: loss 2.4384453296661377\n",
      "Iteration 2020: loss 2.730440378189087\n",
      "Iteration 2021: loss 2.935319185256958\n",
      "Iteration 2022: loss 2.724540948867798\n",
      "Iteration 2023: loss 2.6597044467926025\n",
      "Iteration 2024: loss 2.6490838527679443\n",
      "Iteration 2025: loss 2.6369051933288574\n",
      "Iteration 2026: loss 2.592613697052002\n",
      "Iteration 2027: loss 2.2830429077148438\n",
      "Iteration 2028: loss 2.588696241378784\n",
      "Iteration 2029: loss 2.6692168712615967\n",
      "Iteration 2030: loss 2.177175760269165\n",
      "Iteration 2031: loss 2.585388422012329\n",
      "Iteration 2032: loss 2.7882487773895264\n",
      "Iteration 2033: loss 2.0610263347625732\n",
      "Iteration 2034: loss 2.4981682300567627\n",
      "Iteration 2035: loss 2.2421040534973145\n",
      "Iteration 2036: loss 2.365802764892578\n",
      "Iteration 2037: loss 2.6553735733032227\n",
      "Iteration 2038: loss 2.3446943759918213\n",
      "Iteration 2039: loss 2.2904906272888184\n",
      "Iteration 2040: loss 2.5448100566864014\n",
      "Iteration 2041: loss 2.301981210708618\n",
      "Iteration 2042: loss 2.0145442485809326\n",
      "Iteration 2043: loss 2.3808369636535645\n",
      "Iteration 2044: loss 2.0998551845550537\n",
      "Iteration 2045: loss 2.0435214042663574\n",
      "Iteration 2046: loss 2.561828851699829\n",
      "Iteration 2047: loss 1.9605034589767456\n",
      "Iteration 2048: loss 2.1761627197265625\n",
      "Iteration 2049: loss 2.088651180267334\n",
      "Iteration 2050: loss 2.008178472518921\n",
      "Iteration 2051: loss 2.0872926712036133\n",
      "Iteration 2052: loss 2.4118547439575195\n",
      "Iteration 2053: loss 2.036355972290039\n",
      "Iteration 2054: loss 2.420182228088379\n",
      "Iteration 2055: loss 1.8567920923233032\n",
      "Iteration 2056: loss 2.0188472270965576\n",
      "Iteration 2057: loss 2.1015496253967285\n",
      "Iteration 2058: loss 2.007612466812134\n",
      "Iteration 2059: loss 1.925193190574646\n",
      "Iteration 2060: loss 2.0715935230255127\n",
      "Iteration 2061: loss 2.0842485427856445\n",
      "Iteration 2062: loss 1.9528210163116455\n",
      "Iteration 2063: loss 2.0910732746124268\n",
      "Iteration 2064: loss 2.262489080429077\n",
      "Iteration 2065: loss 2.2903661727905273\n",
      "Iteration 2066: loss 2.3303112983703613\n",
      "Iteration 2067: loss 2.271928310394287\n",
      "Iteration 2068: loss 1.997544527053833\n",
      "Iteration 2069: loss 2.2443647384643555\n",
      "Iteration 2070: loss 2.2627480030059814\n",
      "Iteration 2071: loss 2.5871760845184326\n",
      "Iteration 2072: loss 2.115696668624878\n",
      "Iteration 2073: loss 2.2596065998077393\n",
      "Iteration 2074: loss 2.7805819511413574\n",
      "Iteration 2075: loss 2.3766345977783203\n",
      "Iteration 2076: loss 2.3222646713256836\n",
      "Iteration 2077: loss 2.4705722332000732\n",
      "Iteration 2078: loss 2.5339515209198\n",
      "Iteration 2079: loss 2.5435328483581543\n",
      "Iteration 2080: loss 2.414334297180176\n",
      "Iteration 2081: loss 2.429065227508545\n",
      "Iteration 2082: loss 2.103475332260132\n",
      "Iteration 2083: loss 2.25004506111145\n",
      "Iteration 2084: loss 2.747920274734497\n",
      "Iteration 2085: loss 1.96855890750885\n",
      "Iteration 2086: loss 2.217233896255493\n",
      "Iteration 2087: loss 2.2744951248168945\n",
      "Iteration 2088: loss 2.0288240909576416\n",
      "Iteration 2089: loss 2.1092374324798584\n",
      "Iteration 2090: loss 2.2557411193847656\n",
      "Iteration 2091: loss 1.8762553930282593\n",
      "Iteration 2092: loss 2.3514833450317383\n",
      "Iteration 2093: loss 2.0823557376861572\n",
      "Iteration 2094: loss 2.0776076316833496\n",
      "Iteration 2095: loss 2.165132522583008\n",
      "Iteration 2096: loss 1.6952548027038574\n",
      "Iteration 2097: loss 2.262044906616211\n",
      "Iteration 2098: loss 1.7817264795303345\n",
      "Iteration 2099: loss 2.0086803436279297\n",
      "Iteration 2100: loss 1.845828652381897\n",
      "Iteration 2101: loss 2.2120375633239746\n",
      "Iteration 2102: loss 1.6426589488983154\n",
      "Iteration 2103: loss 1.6719411611557007\n",
      "Iteration 2104: loss 1.9962676763534546\n",
      "Iteration 2105: loss 1.5404082536697388\n",
      "Iteration 2106: loss 1.669527530670166\n",
      "Iteration 2107: loss 1.3746334314346313\n",
      "Iteration 2108: loss 1.6355162858963013\n",
      "Iteration 2109: loss 2.2632617950439453\n",
      "Iteration 2110: loss 1.9040862321853638\n",
      "Iteration 2111: loss 2.2406115531921387\n",
      "Iteration 2112: loss 1.7318941354751587\n",
      "Iteration 2113: loss 1.8354555368423462\n",
      "Iteration 2114: loss 2.290360927581787\n",
      "Iteration 2115: loss 1.6916179656982422\n",
      "Iteration 2116: loss 1.7751226425170898\n",
      "Iteration 2117: loss 1.789271354675293\n",
      "Iteration 2118: loss 1.703856110572815\n",
      "Iteration 2119: loss 1.9192125797271729\n",
      "Iteration 2120: loss 1.635062575340271\n",
      "Iteration 2121: loss 1.6731928586959839\n",
      "Iteration 2122: loss 1.8765558004379272\n",
      "Iteration 2123: loss 1.728912591934204\n",
      "Iteration 2124: loss 1.430677890777588\n",
      "Iteration 2125: loss 2.147304058074951\n",
      "Iteration 2126: loss 1.9944438934326172\n",
      "Iteration 2127: loss 2.4046359062194824\n",
      "Iteration 2128: loss 2.6713337898254395\n",
      "Iteration 2129: loss 1.9125698804855347\n",
      "Iteration 2130: loss 1.6364566087722778\n",
      "Iteration 2131: loss 2.079911470413208\n",
      "Iteration 2132: loss 2.335038423538208\n",
      "Iteration 2133: loss 2.3346266746520996\n",
      "Iteration 2134: loss 2.23856782913208\n",
      "Iteration 2135: loss 2.2164690494537354\n",
      "Iteration 2136: loss 2.4090023040771484\n",
      "Iteration 2137: loss 2.138546943664551\n",
      "Iteration 2138: loss 2.2321982383728027\n",
      "Iteration 2139: loss 2.214327573776245\n",
      "Iteration 2140: loss 2.210376262664795\n",
      "Iteration 2141: loss 1.97394597530365\n",
      "Iteration 2142: loss 1.9407964944839478\n",
      "Iteration 2143: loss 1.8166683912277222\n",
      "Iteration 2144: loss 1.8821767568588257\n",
      "Iteration 2145: loss 2.171104669570923\n",
      "Iteration 2146: loss 1.96104896068573\n",
      "Iteration 2147: loss 1.8899565935134888\n",
      "Iteration 2148: loss 1.8868552446365356\n",
      "Iteration 2149: loss 1.8693300485610962\n",
      "Iteration 2150: loss 2.1281120777130127\n",
      "Iteration 2151: loss 1.8349093198776245\n",
      "Iteration 2152: loss 1.4479851722717285\n",
      "Iteration 2153: loss 1.9164457321166992\n",
      "Iteration 2154: loss 2.230192184448242\n",
      "Iteration 2155: loss 1.7195345163345337\n",
      "Iteration 2156: loss 2.817922830581665\n",
      "Iteration 2157: loss 2.7031335830688477\n",
      "Iteration 2158: loss 2.506446361541748\n",
      "Iteration 2159: loss 2.498110055923462\n",
      "Iteration 2160: loss 2.6872611045837402\n",
      "Iteration 2161: loss 1.9937996864318848\n",
      "Iteration 2162: loss 2.547896146774292\n",
      "Iteration 2163: loss 2.686739444732666\n",
      "Iteration 2164: loss 2.4859461784362793\n",
      "Iteration 2165: loss 2.025569200515747\n",
      "Iteration 2166: loss 2.401505470275879\n",
      "Iteration 2167: loss 2.2446086406707764\n",
      "Iteration 2168: loss 2.3777928352355957\n",
      "Iteration 2169: loss 2.3306405544281006\n",
      "Iteration 2170: loss 1.9947344064712524\n",
      "Iteration 2171: loss 2.123591184616089\n",
      "Iteration 2172: loss 1.915393352508545\n",
      "Iteration 2173: loss 1.9653185606002808\n",
      "Iteration 2174: loss 2.029616117477417\n",
      "Iteration 2175: loss 1.718281865119934\n",
      "Iteration 2176: loss 1.8546459674835205\n",
      "Iteration 2177: loss 2.125357151031494\n",
      "Iteration 2178: loss 1.81605064868927\n",
      "Iteration 2179: loss 1.708513617515564\n",
      "Iteration 2180: loss 1.9545758962631226\n",
      "Iteration 2181: loss 1.9964567422866821\n",
      "Iteration 2182: loss 1.6684658527374268\n",
      "Iteration 2183: loss 1.8419283628463745\n",
      "Iteration 2184: loss 1.9924935102462769\n",
      "Iteration 2185: loss 1.816506028175354\n",
      "Iteration 2186: loss 1.7821323871612549\n",
      "Iteration 2187: loss 1.8748682737350464\n",
      "Iteration 2188: loss 1.6330044269561768\n",
      "Iteration 2189: loss 1.491636872291565\n",
      "Iteration 2190: loss 1.6583876609802246\n",
      "Iteration 2191: loss 1.8143221139907837\n",
      "Iteration 2192: loss 1.6982024908065796\n",
      "Iteration 2193: loss 1.8991506099700928\n",
      "Iteration 2194: loss 1.6294689178466797\n",
      "Iteration 2195: loss 1.6985065937042236\n",
      "Iteration 2196: loss 1.5306655168533325\n",
      "Iteration 2197: loss 1.7752423286437988\n",
      "Iteration 2198: loss 1.7139123678207397\n",
      "Iteration 2199: loss 1.8699694871902466\n",
      "Iteration 2200: loss 1.4598488807678223\n",
      "Iteration 2201: loss 1.4482520818710327\n",
      "Iteration 2202: loss 1.7696484327316284\n",
      "Iteration 2203: loss 1.4385545253753662\n",
      "Iteration 2204: loss 1.610769510269165\n",
      "Iteration 2205: loss 1.669293761253357\n",
      "Iteration 2206: loss 1.8092048168182373\n",
      "Iteration 2207: loss 1.5717812776565552\n",
      "Iteration 2208: loss 1.865322232246399\n",
      "Iteration 2209: loss 1.64803946018219\n",
      "Iteration 2210: loss 1.7609113454818726\n",
      "Iteration 2211: loss 1.6104034185409546\n",
      "Iteration 2212: loss 1.305181860923767\n",
      "Iteration 2213: loss 1.7792861461639404\n",
      "Iteration 2214: loss 1.9143295288085938\n",
      "Iteration 2215: loss 1.6509722471237183\n",
      "Iteration 2216: loss 1.2959328889846802\n",
      "Iteration 2217: loss 1.8224401473999023\n",
      "Iteration 2218: loss 1.495955467224121\n",
      "Iteration 2219: loss 1.9537895917892456\n",
      "Iteration 2220: loss 1.523023009300232\n",
      "Iteration 2221: loss 1.852371335029602\n",
      "Iteration 2222: loss 2.0020530223846436\n",
      "Iteration 2223: loss 2.0191731452941895\n",
      "Iteration 2224: loss 1.880648136138916\n",
      "Iteration 2225: loss 2.146857261657715\n",
      "Iteration 2226: loss 1.690226674079895\n",
      "Iteration 2227: loss 1.7856194972991943\n",
      "Iteration 2228: loss 1.764355182647705\n",
      "Iteration 2229: loss 1.435802936553955\n",
      "Iteration 2230: loss 1.9021351337432861\n",
      "Iteration 2231: loss 2.1911747455596924\n",
      "Iteration 2232: loss 1.8378472328186035\n",
      "Iteration 2233: loss 2.011031150817871\n",
      "Iteration 2234: loss 1.7278423309326172\n",
      "Iteration 2235: loss 1.5684863328933716\n",
      "Iteration 2236: loss 1.7178374528884888\n",
      "Iteration 2237: loss 1.853182315826416\n",
      "Iteration 2238: loss 1.394407033920288\n",
      "Iteration 2239: loss 1.8124581575393677\n",
      "Iteration 2240: loss 1.6103984117507935\n",
      "Iteration 2241: loss 1.4837839603424072\n",
      "Iteration 2242: loss 1.217815637588501\n",
      "Iteration 2243: loss 1.5737367868423462\n",
      "Iteration 2244: loss 1.4973100423812866\n",
      "Iteration 2245: loss 1.553955316543579\n",
      "Iteration 2246: loss 1.9465796947479248\n",
      "Iteration 2247: loss 1.5286483764648438\n",
      "Iteration 2248: loss 1.7784924507141113\n",
      "Iteration 2249: loss 1.7502644062042236\n",
      "Iteration 2250: loss 2.117743730545044\n",
      "Iteration 2251: loss 1.7027713060379028\n",
      "Iteration 2252: loss 1.8681071996688843\n",
      "Iteration 2253: loss 1.4611082077026367\n",
      "Iteration 2254: loss 1.5830827951431274\n",
      "Iteration 2255: loss 1.7534981966018677\n",
      "Iteration 2256: loss 1.6842153072357178\n",
      "Iteration 2257: loss 1.7528862953186035\n",
      "Iteration 2258: loss 1.745150089263916\n",
      "Iteration 2259: loss 1.4093636274337769\n",
      "Iteration 2260: loss 1.516573190689087\n",
      "Iteration 2261: loss 1.4129226207733154\n",
      "Iteration 2262: loss 1.2928919792175293\n",
      "Iteration 2263: loss 1.6866647005081177\n",
      "Iteration 2264: loss 2.06492018699646\n",
      "Iteration 2265: loss 1.9365005493164062\n",
      "Iteration 2266: loss 1.7851793766021729\n",
      "Iteration 2267: loss 1.910197377204895\n",
      "Iteration 2268: loss 1.5969717502593994\n",
      "Iteration 2269: loss 2.2090890407562256\n",
      "Iteration 2270: loss 2.035813331604004\n",
      "Iteration 2271: loss 1.5829020738601685\n",
      "Iteration 2272: loss 2.068425416946411\n",
      "Iteration 2273: loss 1.8183740377426147\n",
      "Iteration 2274: loss 1.9986013174057007\n",
      "Iteration 2275: loss 1.724414348602295\n",
      "Iteration 2276: loss 1.545318841934204\n",
      "Iteration 2277: loss 1.6849310398101807\n",
      "Iteration 2278: loss 1.4810776710510254\n",
      "Iteration 2279: loss 1.43123197555542\n",
      "Iteration 2280: loss 1.589677095413208\n",
      "Iteration 2281: loss 1.5900061130523682\n",
      "Iteration 2282: loss 1.2354230880737305\n",
      "Iteration 2283: loss 1.3106791973114014\n",
      "Iteration 2284: loss 1.627905249595642\n",
      "Iteration 2285: loss 1.529367446899414\n",
      "Iteration 2286: loss 1.6274001598358154\n",
      "Iteration 2287: loss 1.6016732454299927\n",
      "Iteration 2288: loss 1.1996196508407593\n",
      "Iteration 2289: loss 1.8682875633239746\n",
      "Iteration 2290: loss 1.4961313009262085\n",
      "Iteration 2291: loss 1.5379095077514648\n",
      "Iteration 2292: loss 1.616903305053711\n",
      "Iteration 2293: loss 1.686584234237671\n",
      "Iteration 2294: loss 1.831769347190857\n",
      "Iteration 2295: loss 1.7157281637191772\n",
      "Iteration 2296: loss 1.6879239082336426\n",
      "Iteration 2297: loss 1.5144789218902588\n",
      "Iteration 2298: loss 1.873775601387024\n",
      "Iteration 2299: loss 1.4675467014312744\n",
      "Iteration 2300: loss 1.3658416271209717\n",
      "Iteration 2301: loss 1.7469265460968018\n",
      "Iteration 2302: loss 1.3164488077163696\n",
      "Iteration 2303: loss 1.5086290836334229\n",
      "Iteration 2304: loss 1.5282601118087769\n",
      "Iteration 2305: loss 1.6187461614608765\n",
      "Iteration 2306: loss 1.561079978942871\n",
      "Iteration 2307: loss 1.9916375875473022\n",
      "Iteration 2308: loss 1.983496069908142\n",
      "Iteration 2309: loss 2.3056657314300537\n",
      "Iteration 2310: loss 2.909940719604492\n",
      "Iteration 2311: loss 2.449967384338379\n",
      "Iteration 2312: loss 2.673088312149048\n",
      "Iteration 2313: loss 2.5578041076660156\n",
      "Iteration 2314: loss 2.235267400741577\n",
      "Iteration 2315: loss 2.0028762817382812\n",
      "Iteration 2316: loss 2.0996663570404053\n",
      "Iteration 2317: loss 2.0424368381500244\n",
      "Iteration 2318: loss 2.5644960403442383\n",
      "Iteration 2319: loss 2.5232956409454346\n",
      "Iteration 2320: loss 2.299030065536499\n",
      "Iteration 2321: loss 2.3069894313812256\n",
      "Iteration 2322: loss 2.3747689723968506\n",
      "Iteration 2323: loss 1.9768023490905762\n",
      "Iteration 2324: loss 2.357652187347412\n",
      "Iteration 2325: loss 2.0392940044403076\n",
      "Iteration 2326: loss 2.1050970554351807\n",
      "Iteration 2327: loss 2.124973773956299\n",
      "Iteration 2328: loss 1.4667060375213623\n",
      "Iteration 2329: loss 1.9737783670425415\n",
      "Iteration 2330: loss 1.8641915321350098\n",
      "Iteration 2331: loss 2.2234208583831787\n",
      "Iteration 2332: loss 1.7449792623519897\n",
      "Iteration 2333: loss 1.8834048509597778\n",
      "Iteration 2334: loss 1.6467684507369995\n",
      "Iteration 2335: loss 1.8318828344345093\n",
      "Iteration 2336: loss 1.8457287549972534\n",
      "Iteration 2337: loss 1.573887586593628\n",
      "Iteration 2338: loss 1.9390891790390015\n",
      "Iteration 2339: loss 1.7991863489151\n",
      "Iteration 2340: loss 1.5191328525543213\n",
      "Iteration 2341: loss 1.8044053316116333\n",
      "Iteration 2342: loss 1.7323867082595825\n",
      "Iteration 2343: loss 1.168540596961975\n",
      "Iteration 2344: loss 1.3040642738342285\n",
      "Iteration 2345: loss 1.6132131814956665\n",
      "Iteration 2346: loss 1.4960616827011108\n",
      "Iteration 2347: loss 1.4013887643814087\n",
      "Iteration 2348: loss 1.5809459686279297\n",
      "Iteration 2349: loss 1.75252103805542\n",
      "Iteration 2350: loss 1.690425992012024\n",
      "Iteration 2351: loss 1.669054627418518\n",
      "Iteration 2352: loss 1.6065300703048706\n",
      "Iteration 2353: loss 1.2218632698059082\n",
      "Iteration 2354: loss 1.572308897972107\n",
      "Iteration 2355: loss 1.7952184677124023\n",
      "Iteration 2356: loss 1.3890982866287231\n",
      "Iteration 2357: loss 1.0835810899734497\n",
      "Iteration 2358: loss 1.4396032094955444\n",
      "Iteration 2359: loss 1.362402319908142\n",
      "Iteration 2360: loss 1.6336140632629395\n",
      "Iteration 2361: loss 1.7389248609542847\n",
      "Iteration 2362: loss 1.3814775943756104\n",
      "Iteration 2363: loss 1.947735071182251\n",
      "Iteration 2364: loss 2.563016891479492\n",
      "Iteration 2365: loss 2.0802688598632812\n",
      "Iteration 2366: loss 2.1515920162200928\n",
      "Iteration 2367: loss 2.217545509338379\n",
      "Iteration 2368: loss 2.19287109375\n",
      "Iteration 2369: loss 1.9353054761886597\n",
      "Iteration 2370: loss 1.9692119359970093\n",
      "Iteration 2371: loss 2.028369903564453\n",
      "Iteration 2372: loss 1.6741256713867188\n",
      "Iteration 2373: loss 1.40496027469635\n",
      "Iteration 2374: loss 1.9536868333816528\n",
      "Iteration 2375: loss 1.9043999910354614\n",
      "Iteration 2376: loss 1.5161081552505493\n",
      "Iteration 2377: loss 1.9058239459991455\n",
      "Iteration 2378: loss 1.5603115558624268\n",
      "Iteration 2379: loss 5.373161315917969\n",
      "Iteration 2380: loss 3.8396966457366943\n",
      "Iteration 2381: loss 4.268733501434326\n",
      "Iteration 2382: loss 4.701749324798584\n",
      "Iteration 2383: loss 4.191882610321045\n",
      "Iteration 2384: loss 4.806524276733398\n",
      "Iteration 2385: loss 3.215064525604248\n",
      "Iteration 2386: loss 4.69240665435791\n",
      "Iteration 2387: loss 3.632516384124756\n",
      "Iteration 2388: loss 2.9601433277130127\n",
      "Iteration 2389: loss 3.3042972087860107\n",
      "Iteration 2390: loss 2.945675849914551\n",
      "Iteration 2391: loss 2.870905637741089\n",
      "Iteration 2392: loss 3.4697539806365967\n",
      "Iteration 2393: loss 2.43548583984375\n",
      "Iteration 2394: loss 3.2268526554107666\n",
      "Iteration 2395: loss 2.56597900390625\n",
      "Iteration 2396: loss 2.637612819671631\n",
      "Iteration 2397: loss 2.9482035636901855\n",
      "Iteration 2398: loss 2.390267848968506\n",
      "Iteration 2399: loss 2.602189302444458\n",
      "Iteration 2400: loss 2.8042540550231934\n",
      "Iteration 2401: loss 3.081124782562256\n",
      "Iteration 2402: loss 2.3092215061187744\n",
      "Iteration 2403: loss 2.7906901836395264\n",
      "Iteration 2404: loss 2.360804319381714\n",
      "Iteration 2405: loss 2.7226030826568604\n",
      "Iteration 2406: loss 2.593911647796631\n",
      "Iteration 2407: loss 2.3051750659942627\n",
      "Iteration 2408: loss 2.7746195793151855\n",
      "Iteration 2409: loss 2.3291099071502686\n",
      "Iteration 2410: loss 2.6370177268981934\n",
      "Iteration 2411: loss 2.7448558807373047\n",
      "Iteration 2412: loss 2.727309465408325\n",
      "Iteration 2413: loss 2.7885453701019287\n",
      "Iteration 2414: loss 2.4928131103515625\n",
      "Iteration 2415: loss 2.02535080909729\n",
      "Iteration 2416: loss 2.4437856674194336\n",
      "Iteration 2417: loss 2.5836904048919678\n",
      "Iteration 2418: loss 2.4706802368164062\n",
      "Iteration 2419: loss 2.4724361896514893\n",
      "Iteration 2420: loss 2.934811592102051\n",
      "Iteration 2421: loss 2.5786120891571045\n",
      "Iteration 2422: loss 2.785961866378784\n",
      "Iteration 2423: loss 2.8389623165130615\n",
      "Iteration 2424: loss 2.346815824508667\n",
      "Iteration 2425: loss 2.1759145259857178\n",
      "Iteration 2426: loss 2.4882888793945312\n",
      "Iteration 2427: loss 2.594369411468506\n",
      "Iteration 2428: loss 2.312455654144287\n",
      "Iteration 2429: loss 3.011786460876465\n",
      "Iteration 2430: loss 2.8290491104125977\n",
      "Iteration 2431: loss 2.209800958633423\n",
      "Iteration 2432: loss 2.53743577003479\n",
      "Iteration 2433: loss 2.4175126552581787\n",
      "Iteration 2434: loss 2.7623205184936523\n",
      "Iteration 2435: loss 2.452449321746826\n",
      "Iteration 2436: loss 2.656672716140747\n",
      "Iteration 2437: loss 2.468740701675415\n",
      "Iteration 2438: loss 2.289884567260742\n",
      "Iteration 2439: loss 2.411996603012085\n",
      "Iteration 2440: loss 2.205300807952881\n",
      "Iteration 2441: loss 2.5280463695526123\n",
      "Iteration 2442: loss 2.3502120971679688\n",
      "Iteration 2443: loss 2.0194053649902344\n",
      "Iteration 2444: loss 2.440547227859497\n",
      "Iteration 2445: loss 1.984403133392334\n",
      "Iteration 2446: loss 2.576619863510132\n",
      "Iteration 2447: loss 2.322573184967041\n",
      "Iteration 2448: loss 2.6678526401519775\n",
      "Iteration 2449: loss 2.2126624584198\n",
      "Iteration 2450: loss 2.725847005844116\n",
      "Iteration 2451: loss 2.6028871536254883\n",
      "Iteration 2452: loss 2.5471627712249756\n",
      "Iteration 2453: loss 2.9839799404144287\n",
      "Iteration 2454: loss 2.9073235988616943\n",
      "Iteration 2455: loss 2.567262887954712\n",
      "Iteration 2456: loss 2.27508807182312\n",
      "Iteration 2457: loss 2.2735540866851807\n",
      "Iteration 2458: loss 2.1606853008270264\n",
      "Iteration 2459: loss 2.4898886680603027\n",
      "Iteration 2460: loss 2.338785409927368\n",
      "Iteration 2461: loss 2.6624884605407715\n",
      "Iteration 2462: loss 2.21183705329895\n",
      "Iteration 2463: loss 1.9551159143447876\n",
      "Iteration 2464: loss 2.489028215408325\n",
      "Iteration 2465: loss 2.29677414894104\n",
      "Iteration 2466: loss 2.5542519092559814\n",
      "Iteration 2467: loss 2.2817413806915283\n",
      "Iteration 2468: loss 2.035923719406128\n",
      "Iteration 2469: loss 2.229668378829956\n",
      "Iteration 2470: loss 2.4673943519592285\n",
      "Iteration 2471: loss 2.3892030715942383\n",
      "Iteration 2472: loss 2.4088940620422363\n",
      "Iteration 2473: loss 2.358048439025879\n",
      "Iteration 2474: loss 2.4707484245300293\n",
      "Iteration 2475: loss 1.9026108980178833\n",
      "Iteration 2476: loss 2.4866549968719482\n",
      "Iteration 2477: loss 2.4759387969970703\n",
      "Iteration 2478: loss 2.3702378273010254\n",
      "Iteration 2479: loss 2.389326333999634\n",
      "Iteration 2480: loss 2.2357699871063232\n",
      "Iteration 2481: loss 2.2759480476379395\n",
      "Iteration 2482: loss 2.5351827144622803\n",
      "Iteration 2483: loss 2.3522419929504395\n",
      "Iteration 2484: loss 2.401456356048584\n",
      "Iteration 2485: loss 2.2385520935058594\n",
      "Iteration 2486: loss 2.8987724781036377\n",
      "Iteration 2487: loss 2.476715087890625\n",
      "Iteration 2488: loss 2.2248427867889404\n",
      "Iteration 2489: loss 2.396912097930908\n",
      "Iteration 2490: loss 2.543566942214966\n",
      "Iteration 2491: loss 2.364288568496704\n",
      "Iteration 2492: loss 2.3102867603302\n",
      "Iteration 2493: loss 2.089343547821045\n",
      "Iteration 2494: loss 2.3570828437805176\n",
      "Iteration 2495: loss 2.1450352668762207\n",
      "Iteration 2496: loss 2.430548906326294\n",
      "Iteration 2497: loss 2.3284835815429688\n",
      "Iteration 2498: loss 2.1332168579101562\n",
      "Iteration 2499: loss 2.136564016342163\n",
      "Iteration 2500: loss 2.780735731124878\n",
      "Iteration 2501: loss 2.0855178833007812\n",
      "Iteration 2502: loss 2.33331298828125\n",
      "Iteration 2503: loss 1.9658520221710205\n",
      "Iteration 2504: loss 2.350693464279175\n",
      "Iteration 2505: loss 2.3378243446350098\n",
      "Iteration 2506: loss 2.4281599521636963\n",
      "Iteration 2507: loss 2.7570555210113525\n",
      "Iteration 2508: loss 2.2050697803497314\n",
      "Iteration 2509: loss 2.4595377445220947\n",
      "Iteration 2510: loss 2.383903741836548\n",
      "Iteration 2511: loss 2.411663770675659\n",
      "Iteration 2512: loss 2.4153313636779785\n",
      "Iteration 2513: loss 2.4761762619018555\n",
      "Iteration 2514: loss 2.4050090312957764\n",
      "Iteration 2515: loss 2.113948345184326\n",
      "Iteration 2516: loss 2.2060599327087402\n",
      "Iteration 2517: loss 2.541339635848999\n",
      "Iteration 2518: loss 2.4256277084350586\n",
      "Iteration 2519: loss 2.3781964778900146\n",
      "Iteration 2520: loss 2.221879005432129\n",
      "Iteration 2521: loss 2.4282004833221436\n",
      "Iteration 2522: loss 1.9946447610855103\n",
      "Iteration 2523: loss 2.188624858856201\n",
      "Iteration 2524: loss 2.162635326385498\n",
      "Iteration 2525: loss 2.045438528060913\n",
      "Iteration 2526: loss 2.2534854412078857\n",
      "Iteration 2527: loss 2.532402753829956\n",
      "Iteration 2528: loss 2.4573793411254883\n",
      "Iteration 2529: loss 2.142439603805542\n",
      "Iteration 2530: loss 2.429722309112549\n",
      "Iteration 2531: loss 1.8880903720855713\n",
      "Iteration 2532: loss 2.257777452468872\n",
      "Iteration 2533: loss 2.275516986846924\n",
      "Iteration 2534: loss 2.3493685722351074\n",
      "Iteration 2535: loss 2.0260541439056396\n",
      "Iteration 2536: loss 2.2809817790985107\n",
      "Iteration 2537: loss 2.458343267440796\n",
      "Iteration 2538: loss 2.5158464908599854\n",
      "Iteration 2539: loss 2.5471999645233154\n",
      "Iteration 2540: loss 2.1738667488098145\n",
      "Iteration 2541: loss 2.8560314178466797\n",
      "Iteration 2542: loss 2.6749284267425537\n",
      "Iteration 2543: loss 2.416083812713623\n",
      "Iteration 2544: loss 2.4061572551727295\n",
      "Iteration 2545: loss 2.237443685531616\n",
      "Iteration 2546: loss 1.8375152349472046\n",
      "Iteration 2547: loss 2.5938754081726074\n",
      "Iteration 2548: loss 2.6519389152526855\n",
      "Iteration 2549: loss 2.4047679901123047\n",
      "Iteration 2550: loss 2.1630749702453613\n",
      "Iteration 2551: loss 2.027362585067749\n",
      "Iteration 2552: loss 2.4496593475341797\n",
      "Iteration 2553: loss 2.301314353942871\n",
      "Iteration 2554: loss 2.3544540405273438\n",
      "Iteration 2555: loss 1.9205042123794556\n",
      "Iteration 2556: loss 1.9325125217437744\n",
      "Iteration 2557: loss 2.180027961730957\n",
      "Iteration 2558: loss 1.819745421409607\n",
      "Iteration 2559: loss 2.101515531539917\n",
      "Iteration 2560: loss 2.272057056427002\n",
      "Iteration 2561: loss 2.136705160140991\n",
      "Iteration 2562: loss 2.0600340366363525\n",
      "Iteration 2563: loss 2.344134569168091\n",
      "Iteration 2564: loss 2.102707624435425\n",
      "Iteration 2565: loss 2.292119026184082\n",
      "Iteration 2566: loss 2.173633575439453\n",
      "Iteration 2567: loss 1.954647183418274\n",
      "Iteration 2568: loss 2.5530452728271484\n",
      "Iteration 2569: loss 2.2875471115112305\n",
      "Iteration 2570: loss 2.3137505054473877\n",
      "Iteration 2571: loss 1.7309529781341553\n",
      "Iteration 2572: loss 2.1321632862091064\n",
      "Iteration 2573: loss 1.880886435508728\n",
      "Iteration 2574: loss 2.2015326023101807\n",
      "Iteration 2575: loss 2.603391647338867\n",
      "Iteration 2576: loss 1.7639118432998657\n",
      "Iteration 2577: loss 2.0766186714172363\n",
      "Iteration 2578: loss 2.2799060344696045\n",
      "Iteration 2579: loss 1.737730622291565\n",
      "Iteration 2580: loss 2.3030080795288086\n",
      "Iteration 2581: loss 2.1294658184051514\n",
      "Iteration 2582: loss 1.5378566980361938\n",
      "Iteration 2583: loss 1.8520101308822632\n",
      "Iteration 2584: loss 2.11318039894104\n",
      "Iteration 2585: loss 1.9045559167861938\n",
      "Iteration 2586: loss 1.834574580192566\n",
      "Iteration 2587: loss 1.801047921180725\n",
      "Iteration 2588: loss 2.1107053756713867\n",
      "Iteration 2589: loss 1.7315603494644165\n",
      "Iteration 2590: loss 1.6559749841690063\n",
      "Iteration 2591: loss 1.8531033992767334\n",
      "Iteration 2592: loss 2.1901957988739014\n",
      "Iteration 2593: loss 1.9593158960342407\n",
      "Iteration 2594: loss 1.7350298166275024\n",
      "Iteration 2595: loss 2.127089738845825\n",
      "Iteration 2596: loss 1.7230010032653809\n",
      "Iteration 2597: loss 1.3009283542633057\n",
      "Iteration 2598: loss 1.754723072052002\n",
      "Iteration 2599: loss 1.896533727645874\n",
      "Iteration 2600: loss 1.7104363441467285\n",
      "Iteration 2601: loss 2.332364082336426\n",
      "Iteration 2602: loss 2.1890997886657715\n",
      "Iteration 2603: loss 1.8253775835037231\n",
      "Iteration 2604: loss 2.0610644817352295\n",
      "Iteration 2605: loss 1.5829802751541138\n",
      "Iteration 2606: loss 1.9907546043395996\n",
      "Iteration 2607: loss 1.6026159524917603\n",
      "Iteration 2608: loss 1.6656157970428467\n",
      "Iteration 2609: loss 1.9680402278900146\n",
      "Iteration 2610: loss 1.8281110525131226\n",
      "Iteration 2611: loss 1.7645626068115234\n",
      "Iteration 2612: loss 1.922023892402649\n",
      "Iteration 2613: loss 1.4771913290023804\n",
      "Iteration 2614: loss 1.9252325296401978\n",
      "Iteration 2615: loss 1.9852153062820435\n",
      "Iteration 2616: loss 1.8320609331130981\n",
      "Iteration 2617: loss 1.9161230325698853\n",
      "Iteration 2618: loss 1.9485938549041748\n",
      "Iteration 2619: loss 2.0756115913391113\n",
      "Iteration 2620: loss 2.0732083320617676\n",
      "Iteration 2621: loss 2.1738972663879395\n",
      "Iteration 2622: loss 1.7457042932510376\n",
      "Iteration 2623: loss 2.123523712158203\n",
      "Iteration 2624: loss 2.1760001182556152\n",
      "Iteration 2625: loss 2.0885848999023438\n",
      "Iteration 2626: loss 2.1151516437530518\n",
      "Iteration 2627: loss 2.2280969619750977\n",
      "Iteration 2628: loss 2.17142391204834\n",
      "Iteration 2629: loss 1.9312766790390015\n",
      "Iteration 2630: loss 2.0401315689086914\n",
      "Iteration 2631: loss 2.454190254211426\n",
      "Iteration 2632: loss 1.8116092681884766\n",
      "Iteration 2633: loss 2.2771856784820557\n",
      "Iteration 2634: loss 1.5453770160675049\n",
      "Iteration 2635: loss 1.7506308555603027\n",
      "Iteration 2636: loss 2.165693998336792\n",
      "Iteration 2637: loss 2.219808578491211\n",
      "Iteration 2638: loss 2.134049415588379\n",
      "Iteration 2639: loss 1.493956208229065\n",
      "Iteration 2640: loss 2.004915475845337\n",
      "Iteration 2641: loss 1.7292519807815552\n",
      "Iteration 2642: loss 1.938037395477295\n",
      "Iteration 2643: loss 1.8152276277542114\n",
      "Iteration 2644: loss 2.2757186889648438\n",
      "Iteration 2645: loss 1.9297902584075928\n",
      "Iteration 2646: loss 1.887327790260315\n",
      "Iteration 2647: loss 2.4178502559661865\n",
      "Iteration 2648: loss 2.1949663162231445\n",
      "Iteration 2649: loss 2.0545966625213623\n",
      "Iteration 2650: loss 1.9185218811035156\n",
      "Iteration 2651: loss 2.030121326446533\n",
      "Iteration 2652: loss 1.8659354448318481\n",
      "Iteration 2653: loss 2.306852102279663\n",
      "Iteration 2654: loss 2.0240478515625\n",
      "Iteration 2655: loss 2.4022133350372314\n",
      "Iteration 2656: loss 2.42252779006958\n",
      "Iteration 2657: loss 2.189858913421631\n",
      "Iteration 2658: loss 2.06583309173584\n",
      "Iteration 2659: loss 1.6829581260681152\n",
      "Iteration 2660: loss 2.13041615486145\n",
      "Iteration 2661: loss 2.0267322063446045\n",
      "Iteration 2662: loss 2.18095064163208\n",
      "Iteration 2663: loss 1.7379467487335205\n",
      "Iteration 2664: loss 1.7891148328781128\n",
      "Iteration 2665: loss 2.3316709995269775\n",
      "Iteration 2666: loss 1.8146533966064453\n",
      "Iteration 2667: loss 2.185870885848999\n",
      "Iteration 2668: loss 1.9956190586090088\n",
      "Iteration 2669: loss 1.7581530809402466\n",
      "Iteration 2670: loss 1.895691156387329\n",
      "Iteration 2671: loss 1.9271615743637085\n",
      "Iteration 2672: loss 1.6994342803955078\n",
      "Iteration 2673: loss 2.101656675338745\n",
      "Iteration 2674: loss 1.8770599365234375\n",
      "Iteration 2675: loss 2.0799214839935303\n",
      "Iteration 2676: loss 2.2057034969329834\n",
      "Iteration 2677: loss 1.8693664073944092\n",
      "Iteration 2678: loss 1.860933780670166\n",
      "Iteration 2679: loss 1.9326931238174438\n",
      "Iteration 2680: loss 1.4726712703704834\n",
      "Iteration 2681: loss 2.0235342979431152\n",
      "Iteration 2682: loss 2.0773885250091553\n",
      "Iteration 2683: loss 1.9026849269866943\n",
      "Iteration 2684: loss 1.792022705078125\n",
      "Iteration 2685: loss 1.5003951787948608\n",
      "Iteration 2686: loss 1.9241628646850586\n",
      "Iteration 2687: loss 2.008143663406372\n",
      "Iteration 2688: loss 1.9667916297912598\n",
      "Iteration 2689: loss 1.9351340532302856\n",
      "Iteration 2690: loss 1.9095404148101807\n",
      "Iteration 2691: loss 1.7922255992889404\n",
      "Iteration 2692: loss 1.5203851461410522\n",
      "Iteration 2693: loss 1.7658971548080444\n",
      "Iteration 2694: loss 2.2004387378692627\n",
      "Iteration 2695: loss 1.745532512664795\n",
      "Iteration 2696: loss 2.182490587234497\n",
      "Iteration 2697: loss 1.5260244607925415\n",
      "Iteration 2698: loss 1.9401342868804932\n",
      "Iteration 2699: loss 2.1182897090911865\n",
      "Iteration 2700: loss 1.860244870185852\n",
      "Iteration 2701: loss 1.9664723873138428\n",
      "Iteration 2702: loss 1.8445730209350586\n",
      "Iteration 2703: loss 1.7927809953689575\n",
      "Iteration 2704: loss 1.8260940313339233\n",
      "Iteration 2705: loss 1.7997055053710938\n",
      "Iteration 2706: loss 1.806295394897461\n",
      "Iteration 2707: loss 1.9563742876052856\n",
      "Iteration 2708: loss 1.8331563472747803\n",
      "Iteration 2709: loss 2.072598934173584\n",
      "Iteration 2710: loss 2.1028621196746826\n",
      "Iteration 2711: loss 1.6875337362289429\n",
      "Iteration 2712: loss 2.60683012008667\n",
      "Iteration 2713: loss 2.9225118160247803\n",
      "Iteration 2714: loss 2.7904961109161377\n",
      "Iteration 2715: loss 3.1884236335754395\n",
      "Iteration 2716: loss 3.7445640563964844\n",
      "Iteration 2717: loss 3.1574742794036865\n",
      "Iteration 2718: loss 3.3980793952941895\n",
      "Iteration 2719: loss 3.4906718730926514\n",
      "Iteration 2720: loss 3.0318126678466797\n",
      "Iteration 2721: loss 3.514901638031006\n",
      "Iteration 2722: loss 3.294938564300537\n",
      "Iteration 2723: loss 2.9458439350128174\n",
      "Iteration 2724: loss 2.889404773712158\n",
      "Iteration 2725: loss 3.1292855739593506\n",
      "Iteration 2726: loss 3.120063304901123\n",
      "Iteration 2727: loss 2.9175169467926025\n",
      "Iteration 2728: loss 3.0725579261779785\n",
      "Iteration 2729: loss 2.8888821601867676\n",
      "Iteration 2730: loss 3.164351224899292\n",
      "Iteration 2731: loss 2.2910373210906982\n",
      "Iteration 2732: loss 3.06791615486145\n",
      "Iteration 2733: loss 2.7843165397644043\n",
      "Iteration 2734: loss 3.027529239654541\n",
      "Iteration 2735: loss 3.1420583724975586\n",
      "Iteration 2736: loss 2.8108246326446533\n",
      "Iteration 2737: loss 2.7584681510925293\n",
      "Iteration 2738: loss 2.910374164581299\n",
      "Iteration 2739: loss 2.857952117919922\n",
      "Iteration 2740: loss 3.189577579498291\n",
      "Iteration 2741: loss 2.791959524154663\n",
      "Iteration 2742: loss 2.618260383605957\n",
      "Iteration 2743: loss 2.7078611850738525\n",
      "Iteration 2744: loss 2.4904863834381104\n",
      "Iteration 2745: loss 2.72479510307312\n",
      "Iteration 2746: loss 2.9475841522216797\n",
      "Iteration 2747: loss 3.035562038421631\n",
      "Iteration 2748: loss 2.79378604888916\n",
      "Iteration 2749: loss 2.314483642578125\n",
      "Iteration 2750: loss 2.923558235168457\n",
      "Iteration 2751: loss 3.224040985107422\n",
      "Iteration 2752: loss 2.2489192485809326\n",
      "Iteration 2753: loss 2.238462448120117\n",
      "Iteration 2754: loss 2.2191386222839355\n",
      "Iteration 2755: loss 2.8437798023223877\n",
      "Iteration 2756: loss 2.421010971069336\n",
      "Iteration 2757: loss 2.9885406494140625\n",
      "Iteration 2758: loss 2.1337645053863525\n",
      "Iteration 2759: loss 2.8309106826782227\n",
      "Iteration 2760: loss 2.461268424987793\n",
      "Iteration 2761: loss 2.9551329612731934\n",
      "Iteration 2762: loss 2.6177868843078613\n",
      "Iteration 2763: loss 2.526608467102051\n",
      "Iteration 2764: loss 2.472432851791382\n",
      "Iteration 2765: loss 2.203568696975708\n",
      "Iteration 2766: loss 2.7963414192199707\n",
      "Iteration 2767: loss 2.4126908779144287\n",
      "Iteration 2768: loss 2.543076515197754\n",
      "Iteration 2769: loss 2.4269914627075195\n",
      "Iteration 2770: loss 2.580876350402832\n",
      "Iteration 2771: loss 2.495262622833252\n",
      "Iteration 2772: loss 2.7420809268951416\n",
      "Iteration 2773: loss 2.312756299972534\n",
      "Iteration 2774: loss 2.5291781425476074\n",
      "Iteration 2775: loss 2.4433000087738037\n",
      "Iteration 2776: loss 2.767595052719116\n",
      "Iteration 2777: loss 2.3136110305786133\n",
      "Iteration 2778: loss 2.899613618850708\n",
      "Iteration 2779: loss 2.6272528171539307\n",
      "Iteration 2780: loss 2.2040176391601562\n",
      "Iteration 2781: loss 2.5056252479553223\n",
      "Iteration 2782: loss 2.318234920501709\n",
      "Iteration 2783: loss 2.4331307411193848\n",
      "Iteration 2784: loss 2.0988638401031494\n",
      "Iteration 2785: loss 2.475844144821167\n",
      "Iteration 2786: loss 2.3835952281951904\n",
      "Iteration 2787: loss 2.0779008865356445\n",
      "Iteration 2788: loss 2.2179341316223145\n",
      "Iteration 2789: loss 2.524895668029785\n",
      "Iteration 2790: loss 2.1067135334014893\n",
      "Iteration 2791: loss 2.042210102081299\n",
      "Iteration 2792: loss 2.3155879974365234\n",
      "Iteration 2793: loss 2.1945788860321045\n",
      "Iteration 2794: loss 1.9837216138839722\n",
      "Iteration 2795: loss 1.9300860166549683\n",
      "Iteration 2796: loss 2.1574020385742188\n",
      "Iteration 2797: loss 2.222557544708252\n",
      "Iteration 2798: loss 1.848128318786621\n",
      "Iteration 2799: loss 2.315199851989746\n",
      "Iteration 2800: loss 2.1754040718078613\n",
      "Iteration 2801: loss 1.6150028705596924\n",
      "Iteration 2802: loss 2.1062045097351074\n",
      "Iteration 2803: loss 2.133833885192871\n",
      "Iteration 2804: loss 1.8937934637069702\n",
      "Iteration 2805: loss 2.076613664627075\n",
      "Iteration 2806: loss 2.1657724380493164\n",
      "Iteration 2807: loss 2.1992266178131104\n",
      "Iteration 2808: loss 2.0419108867645264\n",
      "Iteration 2809: loss 2.3762989044189453\n",
      "Iteration 2810: loss 1.8443807363510132\n",
      "Iteration 2811: loss 2.6236958503723145\n",
      "Iteration 2812: loss 2.456930160522461\n",
      "Iteration 2813: loss 2.3515238761901855\n",
      "Iteration 2814: loss 2.403327465057373\n",
      "Iteration 2815: loss 2.1358675956726074\n",
      "Iteration 2816: loss 2.422994375228882\n",
      "Iteration 2817: loss 2.0501372814178467\n",
      "Iteration 2818: loss 2.438081741333008\n",
      "Iteration 2819: loss 2.0178966522216797\n",
      "Iteration 2820: loss 1.9877407550811768\n",
      "Iteration 2821: loss 2.1015312671661377\n",
      "Iteration 2822: loss 2.3992254734039307\n",
      "Iteration 2823: loss 2.0478596687316895\n",
      "Iteration 2824: loss 1.9784570932388306\n",
      "Iteration 2825: loss 2.2988240718841553\n",
      "Iteration 2826: loss 2.090505838394165\n",
      "Iteration 2827: loss 1.9755398035049438\n",
      "Iteration 2828: loss 2.240558385848999\n",
      "Iteration 2829: loss 2.5335922241210938\n",
      "Iteration 2830: loss 2.519435167312622\n",
      "Iteration 2831: loss 2.330461263656616\n",
      "Iteration 2832: loss 1.9906929731369019\n",
      "Iteration 2833: loss 2.2776243686676025\n",
      "Iteration 2834: loss 2.1752114295959473\n",
      "Iteration 2835: loss 2.1075844764709473\n",
      "Iteration 2836: loss 2.2269182205200195\n",
      "Iteration 2837: loss 2.3784706592559814\n",
      "Iteration 2838: loss 2.128413438796997\n",
      "Iteration 2839: loss 2.3819944858551025\n",
      "Iteration 2840: loss 1.7006466388702393\n",
      "Iteration 2841: loss 2.0580172538757324\n",
      "Iteration 2842: loss 2.1639435291290283\n",
      "Iteration 2843: loss 2.375293731689453\n",
      "Iteration 2844: loss 2.0385940074920654\n",
      "Iteration 2845: loss 2.1632156372070312\n",
      "Iteration 2846: loss 2.396042823791504\n",
      "Iteration 2847: loss 1.7866357564926147\n",
      "Iteration 2848: loss 2.3102080821990967\n",
      "Iteration 2849: loss 2.3132290840148926\n",
      "Iteration 2850: loss 2.149489164352417\n",
      "Iteration 2851: loss 2.3330068588256836\n",
      "Iteration 2852: loss 2.3591227531433105\n",
      "Iteration 2853: loss 2.101893424987793\n",
      "Iteration 2854: loss 2.0707755088806152\n",
      "Iteration 2855: loss 1.9081286191940308\n",
      "Iteration 2856: loss 2.352403402328491\n",
      "Iteration 2857: loss 1.932145357131958\n",
      "Iteration 2858: loss 1.7699450254440308\n",
      "Iteration 2859: loss 1.8217462301254272\n",
      "Iteration 2860: loss 1.8744741678237915\n",
      "Iteration 2861: loss 1.9556286334991455\n",
      "Iteration 2862: loss 1.9976012706756592\n",
      "Iteration 2863: loss 1.9727627038955688\n",
      "Iteration 2864: loss 2.2773523330688477\n",
      "Iteration 2865: loss 1.82826566696167\n",
      "Iteration 2866: loss 1.7561553716659546\n",
      "Iteration 2867: loss 1.9117720127105713\n",
      "Iteration 2868: loss 2.3131301403045654\n",
      "Iteration 2869: loss 1.8843955993652344\n",
      "Iteration 2870: loss 1.630643606185913\n",
      "Iteration 2871: loss 2.175733804702759\n",
      "Iteration 2872: loss 2.0629782676696777\n",
      "Iteration 2873: loss 1.7602907419204712\n",
      "Iteration 2874: loss 1.9376962184906006\n",
      "Iteration 2875: loss 1.8283218145370483\n",
      "Iteration 2876: loss 1.9741778373718262\n",
      "Iteration 2877: loss 2.0983996391296387\n",
      "Iteration 2878: loss 1.942687749862671\n",
      "Iteration 2879: loss 1.5931987762451172\n",
      "Iteration 2880: loss 1.8417397737503052\n",
      "Iteration 2881: loss 1.9721273183822632\n",
      "Iteration 2882: loss 1.6012804508209229\n",
      "Iteration 2883: loss 2.0158092975616455\n",
      "Iteration 2884: loss 1.9759453535079956\n",
      "Iteration 2885: loss 2.253791093826294\n",
      "Iteration 2886: loss 1.52503502368927\n",
      "Iteration 2887: loss 1.937013864517212\n",
      "Iteration 2888: loss 2.0439822673797607\n",
      "Iteration 2889: loss 2.038576364517212\n",
      "Iteration 2890: loss 2.177201271057129\n",
      "Iteration 2891: loss 1.7772109508514404\n",
      "Iteration 2892: loss 1.9386152029037476\n",
      "Iteration 2893: loss 1.838009238243103\n",
      "Iteration 2894: loss 2.1400890350341797\n",
      "Iteration 2895: loss 2.242090940475464\n",
      "Iteration 2896: loss 1.9365686178207397\n",
      "Iteration 2897: loss 1.4324373006820679\n",
      "Iteration 2898: loss 2.109095335006714\n",
      "Iteration 2899: loss 1.9841920137405396\n",
      "Iteration 2900: loss 2.4689548015594482\n",
      "Iteration 2901: loss 2.0612471103668213\n",
      "Iteration 2902: loss 2.17722487449646\n",
      "Iteration 2903: loss 1.6961628198623657\n",
      "Iteration 2904: loss 1.771242380142212\n",
      "Iteration 2905: loss 1.8815783262252808\n",
      "Iteration 2906: loss 1.693922519683838\n",
      "Iteration 2907: loss 1.86514413356781\n",
      "Iteration 2908: loss 1.6823278665542603\n",
      "Iteration 2909: loss 1.7841161489486694\n",
      "Iteration 2910: loss 1.5430773496627808\n",
      "Iteration 2911: loss 1.7116838693618774\n",
      "Iteration 2912: loss 2.276698112487793\n",
      "Iteration 2913: loss 1.8892791271209717\n",
      "Iteration 2914: loss 2.008578062057495\n",
      "Iteration 2915: loss 1.6847912073135376\n",
      "Iteration 2916: loss 1.8126981258392334\n",
      "Iteration 2917: loss 1.6268633604049683\n",
      "Iteration 2918: loss 1.7240976095199585\n",
      "Iteration 2919: loss 1.7852027416229248\n",
      "Iteration 2920: loss 2.0829427242279053\n",
      "Iteration 2921: loss 1.786590814590454\n",
      "Iteration 2922: loss 1.441595435142517\n",
      "Iteration 2923: loss 1.5746783018112183\n",
      "Iteration 2924: loss 1.9380831718444824\n",
      "Iteration 2925: loss 1.9138134717941284\n",
      "Iteration 2926: loss 2.1698999404907227\n",
      "Iteration 2927: loss 1.8840490579605103\n",
      "Iteration 2928: loss 1.7636587619781494\n",
      "Iteration 2929: loss 1.7272553443908691\n",
      "Iteration 2930: loss 1.8522518873214722\n",
      "Iteration 2931: loss 1.9381873607635498\n",
      "Iteration 2932: loss 1.932430624961853\n",
      "Iteration 2933: loss 1.8897005319595337\n",
      "Iteration 2934: loss 1.7014107704162598\n",
      "Iteration 2935: loss 1.7166521549224854\n",
      "Iteration 2936: loss 2.1294500827789307\n",
      "Iteration 2937: loss 1.807496190071106\n",
      "Iteration 2938: loss 1.5717601776123047\n",
      "Iteration 2939: loss 1.7878642082214355\n",
      "Iteration 2940: loss 1.6736843585968018\n",
      "Iteration 2941: loss 1.689317226409912\n",
      "Iteration 2942: loss 1.6965476274490356\n",
      "Iteration 2943: loss 1.6202900409698486\n",
      "Iteration 2944: loss 1.8806928396224976\n",
      "Iteration 2945: loss 1.5242698192596436\n",
      "Iteration 2946: loss 1.758475661277771\n",
      "Iteration 2947: loss 1.6455364227294922\n",
      "Iteration 2948: loss 1.4493801593780518\n",
      "Iteration 2949: loss 2.0844244956970215\n",
      "Iteration 2950: loss 1.5762865543365479\n",
      "Iteration 2951: loss 1.6962487697601318\n",
      "Iteration 2952: loss 1.8123664855957031\n",
      "Iteration 2953: loss 2.094893217086792\n",
      "Iteration 2954: loss 2.1157331466674805\n",
      "Iteration 2955: loss 1.344527006149292\n",
      "Iteration 2956: loss 1.989858627319336\n",
      "Iteration 2957: loss 1.3228799104690552\n",
      "Iteration 2958: loss 1.3549607992172241\n",
      "Iteration 2959: loss 1.6721798181533813\n",
      "Iteration 2960: loss 1.3105095624923706\n",
      "Iteration 2961: loss 2.048203468322754\n",
      "Iteration 2962: loss 1.5958205461502075\n",
      "Iteration 2963: loss 1.7528884410858154\n",
      "Iteration 2964: loss 1.9306451082229614\n",
      "Iteration 2965: loss 1.2478082180023193\n",
      "Iteration 2966: loss 1.6879626512527466\n",
      "Iteration 2967: loss 1.3671447038650513\n",
      "Iteration 2968: loss 1.6172235012054443\n",
      "Iteration 2969: loss 1.644885540008545\n",
      "Iteration 2970: loss 1.6997443437576294\n",
      "Iteration 2971: loss 1.7559517621994019\n",
      "Iteration 2972: loss 1.789467692375183\n",
      "Iteration 2973: loss 1.823180913925171\n",
      "Iteration 2974: loss 2.215712070465088\n",
      "Iteration 2975: loss 1.5230098962783813\n",
      "Iteration 2976: loss 2.07785701751709\n",
      "Iteration 2977: loss 2.0266144275665283\n",
      "Iteration 2978: loss 1.6046624183654785\n",
      "Iteration 2979: loss 1.6639821529388428\n",
      "Iteration 2980: loss 1.1926385164260864\n",
      "Iteration 2981: loss 1.8970179557800293\n",
      "Iteration 2982: loss 2.0240113735198975\n",
      "Iteration 2983: loss 1.5449742078781128\n",
      "Iteration 2984: loss 2.0129106044769287\n",
      "Iteration 2985: loss 1.7640914916992188\n",
      "Iteration 2986: loss 1.6130067110061646\n",
      "Iteration 2987: loss 1.5945004224777222\n",
      "Iteration 2988: loss 1.5917690992355347\n",
      "Iteration 2989: loss 2.0817041397094727\n",
      "Iteration 2990: loss 2.296316385269165\n",
      "Iteration 2991: loss 2.547276020050049\n",
      "Iteration 2992: loss 2.424147605895996\n",
      "Iteration 2993: loss 2.5669310092926025\n",
      "Iteration 2994: loss 2.7067925930023193\n",
      "Iteration 2995: loss 2.6721785068511963\n",
      "Iteration 2996: loss 3.02316951751709\n",
      "Iteration 2997: loss 2.4835057258605957\n",
      "Iteration 2998: loss 2.973893880844116\n",
      "Iteration 2999: loss 2.906733989715576\n",
      "Iteration 3000: loss 2.666475772857666\n",
      "Iteration 3001: loss 2.570849895477295\n",
      "Iteration 3002: loss 2.4869790077209473\n",
      "Iteration 3003: loss 2.709505319595337\n",
      "Iteration 3004: loss 2.71518611907959\n",
      "Iteration 3005: loss 2.6859071254730225\n",
      "Iteration 3006: loss 2.8309166431427\n",
      "Iteration 3007: loss 2.77825927734375\n",
      "Iteration 3008: loss 1.88612699508667\n",
      "Iteration 3009: loss 2.7177515029907227\n",
      "Iteration 3010: loss 2.5853958129882812\n",
      "Iteration 3011: loss 2.211484670639038\n",
      "Iteration 3012: loss 2.2600769996643066\n",
      "Iteration 3013: loss 2.1317405700683594\n",
      "Iteration 3014: loss 2.44053053855896\n",
      "Iteration 3015: loss 2.566422700881958\n",
      "Iteration 3016: loss 2.1545321941375732\n",
      "Iteration 3017: loss 2.0987391471862793\n",
      "Iteration 3018: loss 2.2477872371673584\n",
      "Iteration 3019: loss 2.0007083415985107\n",
      "Iteration 3020: loss 1.9632827043533325\n",
      "Iteration 3021: loss 1.841788649559021\n",
      "Iteration 3022: loss 2.1618034839630127\n",
      "Iteration 3023: loss 1.9505492448806763\n",
      "Iteration 3024: loss 2.2898876667022705\n",
      "Iteration 3025: loss 1.949941635131836\n",
      "Iteration 3026: loss 2.608107328414917\n",
      "Iteration 3027: loss 2.308242082595825\n",
      "Iteration 3028: loss 2.2207400798797607\n",
      "Iteration 3029: loss 2.0373120307922363\n",
      "Iteration 3030: loss 2.4178860187530518\n",
      "Iteration 3031: loss 2.244354248046875\n",
      "Iteration 3032: loss 1.453201413154602\n",
      "Iteration 3033: loss 2.6307320594787598\n",
      "Iteration 3034: loss 2.2243597507476807\n",
      "Iteration 3035: loss 3.1699109077453613\n",
      "Iteration 3036: loss 2.009830951690674\n",
      "Iteration 3037: loss 2.04621958732605\n",
      "Iteration 3038: loss 2.344923973083496\n",
      "Iteration 3039: loss 2.329760789871216\n",
      "Iteration 3040: loss 1.9582457542419434\n",
      "Iteration 3041: loss 2.055903196334839\n",
      "Iteration 3042: loss 2.3351476192474365\n",
      "Iteration 3043: loss 2.072925329208374\n",
      "Iteration 3044: loss 2.1649370193481445\n",
      "Iteration 3045: loss 2.1732559204101562\n",
      "Iteration 3046: loss 2.712667465209961\n",
      "Iteration 3047: loss 2.8040688037872314\n",
      "Iteration 3048: loss 2.8338916301727295\n",
      "Iteration 3049: loss 2.9973742961883545\n",
      "Iteration 3050: loss 3.247343063354492\n",
      "Iteration 3051: loss 3.559720993041992\n",
      "Iteration 3052: loss 3.2066359519958496\n",
      "Iteration 3053: loss 3.0981926918029785\n",
      "Iteration 3054: loss 2.919809579849243\n",
      "Iteration 3055: loss 2.613102436065674\n",
      "Iteration 3056: loss 3.1726913452148438\n",
      "Iteration 3057: loss 3.123791217803955\n",
      "Iteration 3058: loss 2.8809380531311035\n",
      "Iteration 3059: loss 2.7112877368927\n",
      "Iteration 3060: loss 2.8157098293304443\n",
      "Iteration 3061: loss 3.0697343349456787\n",
      "Iteration 3062: loss 3.0204601287841797\n",
      "Iteration 3063: loss 2.92547607421875\n",
      "Iteration 3064: loss 2.8037474155426025\n",
      "Iteration 3065: loss 2.496345043182373\n",
      "Iteration 3066: loss 2.5988399982452393\n",
      "Iteration 3067: loss 2.5135629177093506\n",
      "Iteration 3068: loss 2.7903900146484375\n",
      "Iteration 3069: loss 2.55983304977417\n",
      "Iteration 3070: loss 2.858907461166382\n",
      "Iteration 3071: loss 3.283672332763672\n",
      "Iteration 3072: loss 2.579671621322632\n",
      "Iteration 3073: loss 2.3851637840270996\n",
      "Iteration 3074: loss 2.466182231903076\n",
      "Iteration 3075: loss 2.9830162525177\n",
      "Iteration 3076: loss 2.895775318145752\n",
      "Iteration 3077: loss 2.815849542617798\n",
      "Iteration 3078: loss 2.903937578201294\n",
      "Iteration 3079: loss 2.945990800857544\n",
      "Iteration 3080: loss 2.842275857925415\n",
      "Iteration 3081: loss 2.7961177825927734\n",
      "Iteration 3082: loss 2.609391212463379\n",
      "Iteration 3083: loss 2.822115659713745\n",
      "Iteration 3084: loss 2.6326498985290527\n",
      "Iteration 3085: loss 2.360889434814453\n",
      "Iteration 3086: loss 2.370666980743408\n",
      "Iteration 3087: loss 2.368015766143799\n",
      "Iteration 3088: loss 2.212198495864868\n",
      "Iteration 3089: loss 2.209941864013672\n",
      "Iteration 3090: loss 2.1557958126068115\n",
      "Iteration 3091: loss 2.325742244720459\n",
      "Iteration 3092: loss 2.1690399646759033\n",
      "Iteration 3093: loss 2.005749225616455\n",
      "Iteration 3094: loss 2.32211971282959\n",
      "Iteration 3095: loss 2.4028689861297607\n",
      "Iteration 3096: loss 1.9459203481674194\n",
      "Iteration 3097: loss 2.2118136882781982\n",
      "Iteration 3098: loss 2.1933557987213135\n",
      "Iteration 3099: loss 1.8587990999221802\n",
      "Iteration 3100: loss 2.0469565391540527\n",
      "Iteration 3101: loss 2.1955649852752686\n",
      "Iteration 3102: loss 1.9307540655136108\n",
      "Iteration 3103: loss 1.848240613937378\n",
      "Iteration 3104: loss 1.9474471807479858\n",
      "Iteration 3105: loss 1.8738831281661987\n",
      "Iteration 3106: loss 1.8238064050674438\n",
      "Iteration 3107: loss 2.0204217433929443\n",
      "Iteration 3108: loss 2.225289821624756\n",
      "Iteration 3109: loss 2.07138729095459\n",
      "Iteration 3110: loss 1.8099780082702637\n",
      "Iteration 3111: loss 1.6756678819656372\n",
      "Iteration 3112: loss 1.5812748670578003\n",
      "Iteration 3113: loss 1.7901740074157715\n",
      "Iteration 3114: loss 1.892307996749878\n",
      "Iteration 3115: loss 1.8540538549423218\n",
      "Iteration 3116: loss 1.8854496479034424\n",
      "Iteration 3117: loss 1.8655451536178589\n",
      "Iteration 3118: loss 2.1114187240600586\n",
      "Iteration 3119: loss 1.6531636714935303\n",
      "Iteration 3120: loss 1.7548274993896484\n",
      "Iteration 3121: loss 1.9819583892822266\n",
      "Iteration 3122: loss 1.9618172645568848\n",
      "Iteration 3123: loss 1.834599256515503\n",
      "Iteration 3124: loss 1.7985196113586426\n",
      "Iteration 3125: loss 1.6855261325836182\n",
      "Iteration 3126: loss 2.1174333095550537\n",
      "Iteration 3127: loss 1.8020418882369995\n",
      "Iteration 3128: loss 1.7404590845108032\n",
      "Iteration 3129: loss 1.9195126295089722\n",
      "Iteration 3130: loss 1.515435814857483\n",
      "Iteration 3131: loss 1.8755639791488647\n",
      "Iteration 3132: loss 2.0479207038879395\n",
      "Iteration 3133: loss 1.708592176437378\n",
      "Iteration 3134: loss 1.8973259925842285\n",
      "Iteration 3135: loss 1.7643929719924927\n",
      "Iteration 3136: loss 1.7976856231689453\n",
      "Iteration 3137: loss 1.6816833019256592\n",
      "Iteration 3138: loss 1.676156759262085\n",
      "Iteration 3139: loss 1.6437287330627441\n",
      "Iteration 3140: loss 1.9819430112838745\n",
      "Iteration 3141: loss 1.7490755319595337\n",
      "Iteration 3142: loss 1.7423794269561768\n",
      "Iteration 3143: loss 1.6066398620605469\n",
      "Iteration 3144: loss 1.859263300895691\n",
      "Iteration 3145: loss 1.9830172061920166\n",
      "Iteration 3146: loss 1.6084154844284058\n",
      "Iteration 3147: loss 1.731396198272705\n",
      "Iteration 3148: loss 2.1367874145507812\n",
      "Iteration 3149: loss 2.338994264602661\n",
      "Iteration 3150: loss 1.702380657196045\n",
      "Iteration 3151: loss 1.9456582069396973\n",
      "Iteration 3152: loss 2.0818679332733154\n",
      "Iteration 3153: loss 1.7507779598236084\n",
      "Iteration 3154: loss 1.6894384622573853\n",
      "Iteration 3155: loss 1.6103919744491577\n",
      "Iteration 3156: loss 1.3205790519714355\n",
      "Iteration 3157: loss 2.0888421535491943\n",
      "Iteration 3158: loss 1.4948397874832153\n",
      "Iteration 3159: loss 1.8888906240463257\n",
      "Iteration 3160: loss 1.7692745923995972\n",
      "Iteration 3161: loss 2.0704541206359863\n",
      "Iteration 3162: loss 1.7024340629577637\n",
      "Iteration 3163: loss 1.6586462259292603\n",
      "Iteration 3164: loss 1.5182677507400513\n",
      "Iteration 3165: loss 1.9137099981307983\n",
      "Iteration 3166: loss 1.8796496391296387\n",
      "Iteration 3167: loss 1.7567239999771118\n",
      "Iteration 3168: loss 1.4797347784042358\n",
      "Iteration 3169: loss 1.611741304397583\n",
      "Iteration 3170: loss 1.8251166343688965\n",
      "Iteration 3171: loss 1.520731806755066\n",
      "Iteration 3172: loss 2.0025880336761475\n",
      "Iteration 3173: loss 1.3978121280670166\n",
      "Iteration 3174: loss 1.982987880706787\n",
      "Iteration 3175: loss 1.7060739994049072\n",
      "Iteration 3176: loss 1.6230894327163696\n",
      "Iteration 3177: loss 1.6782102584838867\n",
      "Iteration 3178: loss 1.7802250385284424\n",
      "Iteration 3179: loss 1.4617431163787842\n",
      "Iteration 3180: loss 1.604675531387329\n",
      "Iteration 3181: loss 2.0118911266326904\n",
      "Iteration 3182: loss 1.9127099514007568\n",
      "Iteration 3183: loss 1.7013473510742188\n",
      "Iteration 3184: loss 1.8294495344161987\n",
      "Iteration 3185: loss 1.465623140335083\n",
      "Iteration 3186: loss 1.593889832496643\n",
      "Iteration 3187: loss 1.9143542051315308\n",
      "Iteration 3188: loss 1.7638500928878784\n",
      "Iteration 3189: loss 1.7864643335342407\n",
      "Iteration 3190: loss 1.6834113597869873\n",
      "Iteration 3191: loss 1.4548144340515137\n",
      "Iteration 3192: loss 1.6237221956253052\n",
      "Iteration 3193: loss 1.7205801010131836\n",
      "Iteration 3194: loss 1.8953304290771484\n",
      "Iteration 3195: loss 1.707934021949768\n",
      "Iteration 3196: loss 1.7704381942749023\n",
      "Iteration 3197: loss 1.620119571685791\n",
      "Iteration 3198: loss 2.214707612991333\n",
      "Iteration 3199: loss 2.213047504425049\n",
      "Iteration 3200: loss 2.1308791637420654\n",
      "Iteration 3201: loss 2.064077615737915\n",
      "Iteration 3202: loss 1.723912000656128\n",
      "Iteration 3203: loss 2.505798816680908\n",
      "Iteration 3204: loss 2.0577354431152344\n",
      "Iteration 3205: loss 1.9722710847854614\n",
      "Iteration 3206: loss 2.0691704750061035\n",
      "Iteration 3207: loss 2.1850485801696777\n",
      "Iteration 3208: loss 1.90421462059021\n",
      "Iteration 3209: loss 2.1978399753570557\n",
      "Iteration 3210: loss 2.1582868099212646\n",
      "Iteration 3211: loss 1.8578503131866455\n",
      "Iteration 3212: loss 2.1186749935150146\n",
      "Iteration 3213: loss 1.6623393297195435\n",
      "Iteration 3214: loss 2.047966957092285\n",
      "Iteration 3215: loss 2.0386056900024414\n",
      "Iteration 3216: loss 1.93696129322052\n",
      "Iteration 3217: loss 1.2879449129104614\n",
      "Iteration 3218: loss 1.865224003791809\n",
      "Iteration 3219: loss 1.3761132955551147\n",
      "Iteration 3220: loss 1.803690791130066\n",
      "Iteration 3221: loss 1.6883330345153809\n",
      "Iteration 3222: loss 1.9822204113006592\n",
      "Iteration 3223: loss 1.3125540018081665\n",
      "Iteration 3224: loss 1.565392255783081\n",
      "Iteration 3225: loss 1.5945475101470947\n",
      "Iteration 3226: loss 1.457436203956604\n",
      "Iteration 3227: loss 1.5491175651550293\n",
      "Iteration 3228: loss 1.5151053667068481\n",
      "Iteration 3229: loss 1.49696946144104\n",
      "Iteration 3230: loss 1.3327443599700928\n",
      "Iteration 3231: loss 1.4527496099472046\n",
      "Iteration 3232: loss 1.3594757318496704\n",
      "Iteration 3233: loss 1.5071368217468262\n",
      "Iteration 3234: loss 1.3097076416015625\n",
      "Iteration 3235: loss 1.6012107133865356\n",
      "Iteration 3236: loss 1.602520227432251\n",
      "Iteration 3237: loss 1.2886608839035034\n",
      "Iteration 3238: loss 1.5244134664535522\n",
      "Iteration 3239: loss 1.626954436302185\n",
      "Iteration 3240: loss 1.4290771484375\n",
      "Iteration 3241: loss 1.291297435760498\n",
      "Iteration 3242: loss 1.4150686264038086\n",
      "Iteration 3243: loss 1.364301085472107\n",
      "Iteration 3244: loss 1.4943002462387085\n",
      "Iteration 3245: loss 1.584032416343689\n",
      "Iteration 3246: loss 1.15589439868927\n",
      "Iteration 3247: loss 1.1736503839492798\n",
      "Iteration 3248: loss 1.1479973793029785\n",
      "Iteration 3249: loss 1.2021321058273315\n",
      "Iteration 3250: loss 1.3958402872085571\n",
      "Iteration 3251: loss 1.0903314352035522\n",
      "Iteration 3252: loss 1.417033076286316\n",
      "Iteration 3253: loss 1.4510784149169922\n",
      "Iteration 3254: loss 1.5588629245758057\n",
      "Iteration 3255: loss 1.6631312370300293\n",
      "Iteration 3256: loss 1.650179147720337\n",
      "Iteration 3257: loss 1.4396425485610962\n",
      "Iteration 3258: loss 1.4933525323867798\n",
      "Iteration 3259: loss 1.3011349439620972\n",
      "Iteration 3260: loss 1.412667989730835\n",
      "Iteration 3261: loss 1.0792388916015625\n",
      "Iteration 3262: loss 1.062295913696289\n",
      "Iteration 3263: loss 1.4940882921218872\n",
      "Iteration 3264: loss 1.6041628122329712\n",
      "Iteration 3265: loss 1.249611496925354\n",
      "Iteration 3266: loss 1.6033082008361816\n",
      "Iteration 3267: loss 1.66390860080719\n",
      "Iteration 3268: loss 1.2948545217514038\n",
      "Iteration 3269: loss 1.46978759765625\n",
      "Iteration 3270: loss 1.1635221242904663\n",
      "Iteration 3271: loss 1.255656361579895\n",
      "Iteration 3272: loss 1.012699842453003\n",
      "Iteration 3273: loss 1.0903648138046265\n",
      "Iteration 3274: loss 1.4264631271362305\n",
      "Iteration 3275: loss 1.1700975894927979\n",
      "Iteration 3276: loss 1.6535404920578003\n",
      "Iteration 3277: loss 1.409226655960083\n",
      "Iteration 3278: loss 1.3524205684661865\n",
      "Iteration 3279: loss 2.1363813877105713\n",
      "Iteration 3280: loss 1.3892110586166382\n",
      "Iteration 3281: loss 2.4044618606567383\n",
      "Iteration 3282: loss 2.0001273155212402\n",
      "Iteration 3283: loss 2.1859524250030518\n",
      "Iteration 3284: loss 2.4261229038238525\n",
      "Iteration 3285: loss 1.5525323152542114\n",
      "Iteration 3286: loss 2.1007802486419678\n",
      "Iteration 3287: loss 1.737728238105774\n",
      "Iteration 3288: loss 1.8349143266677856\n",
      "Iteration 3289: loss 1.9401088953018188\n",
      "Iteration 3290: loss 1.8393423557281494\n",
      "Iteration 3291: loss 2.268118143081665\n",
      "Iteration 3292: loss 1.8660869598388672\n",
      "Iteration 3293: loss 1.8655062913894653\n",
      "Iteration 3294: loss 1.9337819814682007\n",
      "Iteration 3295: loss 2.2372145652770996\n",
      "Iteration 3296: loss 1.5545883178710938\n",
      "Iteration 3297: loss 2.007341146469116\n",
      "Iteration 3298: loss 2.7609236240386963\n",
      "Iteration 3299: loss 1.7985681295394897\n",
      "Iteration 3300: loss 2.879652738571167\n",
      "Iteration 3301: loss 2.3060741424560547\n",
      "Iteration 3302: loss 1.7614455223083496\n",
      "Iteration 3303: loss 2.376479387283325\n",
      "Iteration 3304: loss 2.4237024784088135\n",
      "Iteration 3305: loss 1.6148966550827026\n",
      "Iteration 3306: loss 1.7837356328964233\n",
      "Iteration 3307: loss 1.590405821800232\n",
      "Iteration 3308: loss 1.7649717330932617\n",
      "Iteration 3309: loss 1.535152554512024\n",
      "Iteration 3310: loss 1.754410982131958\n",
      "Iteration 3311: loss 1.5629839897155762\n",
      "Iteration 3312: loss 1.779596209526062\n",
      "Iteration 3313: loss 1.6441203355789185\n",
      "Iteration 3314: loss 1.1857872009277344\n",
      "Iteration 3315: loss 1.284658432006836\n",
      "Iteration 3316: loss 1.7492278814315796\n",
      "Iteration 3317: loss 1.7175676822662354\n",
      "Iteration 3318: loss 1.696669578552246\n",
      "Iteration 3319: loss 1.3724266290664673\n",
      "Iteration 3320: loss 1.2176251411437988\n",
      "Iteration 3321: loss 1.479251503944397\n",
      "Iteration 3322: loss 1.2854254245758057\n",
      "Iteration 3323: loss 0.9345496296882629\n",
      "Iteration 3324: loss 1.6620376110076904\n",
      "Iteration 3325: loss 1.5034652948379517\n",
      "Iteration 3326: loss 1.2795031070709229\n",
      "Iteration 3327: loss 1.3535113334655762\n",
      "Iteration 3328: loss 1.0661911964416504\n",
      "Iteration 3329: loss 1.4530493021011353\n",
      "Iteration 3330: loss 1.3177490234375\n",
      "Iteration 3331: loss 1.1446113586425781\n",
      "Iteration 3332: loss 1.3088089227676392\n",
      "Iteration 3333: loss 1.1911253929138184\n",
      "Iteration 3334: loss 1.3615570068359375\n",
      "Iteration 3335: loss 1.4741030931472778\n",
      "Iteration 3336: loss 1.6773476600646973\n",
      "Iteration 3337: loss 1.4052228927612305\n",
      "Iteration 3338: loss 1.2742854356765747\n",
      "Iteration 3339: loss 1.3625566959381104\n",
      "Iteration 3340: loss 1.199363350868225\n",
      "Iteration 3341: loss 1.2425734996795654\n",
      "Iteration 3342: loss 1.3418585062026978\n",
      "Iteration 3343: loss 1.5139293670654297\n",
      "Iteration 3344: loss 1.130776286125183\n",
      "Iteration 3345: loss 1.084187626838684\n",
      "Iteration 3346: loss 1.1434736251831055\n",
      "Iteration 3347: loss 1.2357667684555054\n",
      "Iteration 3348: loss 1.182039499282837\n",
      "Iteration 3349: loss 1.5094571113586426\n",
      "Iteration 3350: loss 1.772400975227356\n",
      "Iteration 3351: loss 1.2753857374191284\n",
      "Iteration 3352: loss 1.8985936641693115\n",
      "Iteration 3353: loss 1.3147175312042236\n",
      "Iteration 3354: loss 1.9995918273925781\n",
      "Iteration 3355: loss 1.9728810787200928\n",
      "Iteration 3356: loss 1.6746288537979126\n",
      "Iteration 3357: loss 1.5000033378601074\n",
      "Iteration 3358: loss 0.924268901348114\n",
      "Iteration 3359: loss 2.247239112854004\n",
      "Iteration 3360: loss 1.462158203125\n",
      "Iteration 3361: loss 1.8532564640045166\n",
      "Iteration 3362: loss 1.367675542831421\n",
      "Iteration 3363: loss 1.5249874591827393\n",
      "Iteration 3364: loss 1.6493974924087524\n",
      "Iteration 3365: loss 1.1276767253875732\n",
      "Iteration 3366: loss 1.9950493574142456\n",
      "Iteration 3367: loss 1.5605469942092896\n",
      "Iteration 3368: loss 1.330487847328186\n",
      "Iteration 3369: loss 1.9937719106674194\n",
      "Iteration 3370: loss 1.516783356666565\n",
      "Iteration 3371: loss 1.5415524244308472\n",
      "Iteration 3372: loss 1.7692233324050903\n",
      "Iteration 3373: loss 1.2479454278945923\n",
      "Iteration 3374: loss 1.5037118196487427\n",
      "Iteration 3375: loss 1.3457512855529785\n",
      "Iteration 3376: loss 1.1894640922546387\n",
      "Iteration 3377: loss 1.1878950595855713\n",
      "Iteration 3378: loss 1.3347125053405762\n",
      "Iteration 3379: loss 1.274425745010376\n",
      "Iteration 3380: loss 1.6398464441299438\n",
      "Iteration 3381: loss 1.471407413482666\n",
      "Iteration 3382: loss 1.18052077293396\n",
      "Iteration 3383: loss 1.494154930114746\n",
      "Iteration 3384: loss 1.4704729318618774\n",
      "Iteration 3385: loss 1.215626835823059\n",
      "Iteration 3386: loss 1.3202673196792603\n",
      "Iteration 3387: loss 1.4444231986999512\n",
      "Iteration 3388: loss 1.600526213645935\n",
      "Iteration 3389: loss 1.4570064544677734\n",
      "Iteration 3390: loss 1.283896565437317\n",
      "Iteration 3391: loss 1.1142497062683105\n",
      "Iteration 3392: loss 1.0211753845214844\n",
      "Iteration 3393: loss 1.3669122457504272\n",
      "Iteration 3394: loss 1.740120768547058\n",
      "Iteration 3395: loss 1.5938665866851807\n",
      "Iteration 3396: loss 1.3544747829437256\n",
      "Iteration 3397: loss 1.42000150680542\n",
      "Iteration 3398: loss 1.533560037612915\n",
      "Iteration 3399: loss 1.8043172359466553\n",
      "Iteration 3400: loss 1.7143338918685913\n",
      "Iteration 3401: loss 1.7502535581588745\n",
      "Iteration 3402: loss 1.6218056678771973\n",
      "Iteration 3403: loss 1.7368454933166504\n",
      "Iteration 3404: loss 1.8137668371200562\n",
      "Iteration 3405: loss 2.227506399154663\n",
      "Iteration 3406: loss 1.5185705423355103\n",
      "Iteration 3407: loss 1.831887125968933\n",
      "Iteration 3408: loss 1.9694405794143677\n",
      "Iteration 3409: loss 1.350810170173645\n",
      "Iteration 3410: loss 1.7317396402359009\n",
      "Iteration 3411: loss 1.839559555053711\n",
      "Iteration 3412: loss 1.3148452043533325\n",
      "Iteration 3413: loss 0.9831666350364685\n",
      "Iteration 3414: loss 1.6229320764541626\n",
      "Iteration 3415: loss 1.6107630729675293\n",
      "Iteration 3416: loss 1.5359824895858765\n",
      "Iteration 3417: loss 1.602537989616394\n",
      "Iteration 3418: loss 1.0842105150222778\n",
      "Iteration 3419: loss 1.7160290479660034\n",
      "Iteration 3420: loss 1.5858349800109863\n",
      "Iteration 3421: loss 1.6745461225509644\n",
      "Iteration 3422: loss 1.67607581615448\n",
      "Iteration 3423: loss 0.9489937424659729\n",
      "Iteration 3424: loss 1.7809497117996216\n",
      "Iteration 3425: loss 1.0536980628967285\n",
      "Iteration 3426: loss 1.5038189888000488\n",
      "Iteration 3427: loss 1.2760839462280273\n",
      "Iteration 3428: loss 1.1514205932617188\n",
      "Iteration 3429: loss 1.3907722234725952\n",
      "Iteration 3430: loss 1.5158089399337769\n",
      "Iteration 3431: loss 1.43259859085083\n",
      "Iteration 3432: loss 1.2631142139434814\n",
      "Iteration 3433: loss 1.096847414970398\n",
      "Iteration 3434: loss 1.2566553354263306\n",
      "Iteration 3435: loss 1.4902206659317017\n",
      "Iteration 3436: loss 1.3060897588729858\n",
      "Iteration 3437: loss 1.28769052028656\n",
      "Iteration 3438: loss 1.470903992652893\n",
      "Iteration 3439: loss 1.5899102687835693\n",
      "Iteration 3440: loss 1.499489665031433\n",
      "Iteration 3441: loss 1.059622883796692\n",
      "Iteration 3442: loss 1.1539798974990845\n",
      "Iteration 3443: loss 1.5606310367584229\n",
      "Iteration 3444: loss 1.5470731258392334\n",
      "Iteration 3445: loss 1.30509352684021\n",
      "Iteration 3446: loss 1.0555487871170044\n",
      "Iteration 3447: loss 1.272424578666687\n",
      "Iteration 3448: loss 1.3828203678131104\n",
      "Iteration 3449: loss 1.4603543281555176\n",
      "Iteration 3450: loss 1.2136934995651245\n",
      "Iteration 3451: loss 1.0958658456802368\n",
      "Iteration 3452: loss 1.3556357622146606\n",
      "Iteration 3453: loss 1.446284294128418\n",
      "Iteration 3454: loss 1.3939718008041382\n",
      "Iteration 3455: loss 1.0622864961624146\n",
      "Iteration 3456: loss 1.4348150491714478\n",
      "Iteration 3457: loss 0.9862143397331238\n",
      "Iteration 3458: loss 1.4077175855636597\n",
      "Iteration 3459: loss 1.234875202178955\n",
      "Iteration 3460: loss 1.2735216617584229\n",
      "Iteration 3461: loss 1.2767808437347412\n",
      "Iteration 3462: loss 1.1343506574630737\n",
      "Iteration 3463: loss 1.2111914157867432\n",
      "Iteration 3464: loss 1.3027164936065674\n",
      "Iteration 3465: loss 1.043020248413086\n",
      "Iteration 3466: loss 0.9633566737174988\n",
      "Iteration 3467: loss 1.0516005754470825\n",
      "Iteration 3468: loss 1.1322309970855713\n",
      "Iteration 3469: loss 1.2963073253631592\n",
      "Iteration 3470: loss 1.0802313089370728\n",
      "Iteration 3471: loss 1.1969492435455322\n",
      "Iteration 3472: loss 1.3132973909378052\n",
      "Iteration 3473: loss 1.2416837215423584\n",
      "Iteration 3474: loss 0.9384343028068542\n",
      "Iteration 3475: loss 1.0149290561676025\n",
      "Iteration 3476: loss 1.2782732248306274\n",
      "Iteration 3477: loss 0.7135124206542969\n",
      "Iteration 3478: loss 1.2949469089508057\n",
      "Iteration 3479: loss 1.0287541151046753\n",
      "Iteration 3480: loss 1.0119630098342896\n",
      "Iteration 3481: loss 1.3277814388275146\n",
      "Iteration 3482: loss 1.0763232707977295\n",
      "Iteration 3483: loss 1.229684829711914\n",
      "Iteration 3484: loss 0.9114203453063965\n",
      "Iteration 3485: loss 1.0608439445495605\n",
      "Iteration 3486: loss 1.5012599229812622\n",
      "Iteration 3487: loss 1.3107256889343262\n",
      "Iteration 3488: loss 1.0499364137649536\n",
      "Iteration 3489: loss 1.4812040328979492\n",
      "Iteration 3490: loss 1.2090845108032227\n",
      "Iteration 3491: loss 0.5509451031684875\n",
      "Iteration 3492: loss 1.1013997793197632\n",
      "Iteration 3493: loss 1.2705156803131104\n",
      "Iteration 3494: loss 1.22005295753479\n",
      "Iteration 3495: loss 1.1153219938278198\n",
      "Iteration 3496: loss 1.1981192827224731\n",
      "Iteration 3497: loss 0.832224428653717\n",
      "Iteration 3498: loss 1.005791425704956\n",
      "Iteration 3499: loss 1.3639538288116455\n",
      "Iteration 3500: loss 1.2659274339675903\n",
      "Iteration 3501: loss 0.9916227459907532\n",
      "Iteration 3502: loss 1.5056923627853394\n",
      "Iteration 3503: loss 1.7030880451202393\n",
      "Iteration 3504: loss 1.3316396474838257\n",
      "Iteration 3505: loss 1.44693124294281\n",
      "Iteration 3506: loss 1.2090634107589722\n",
      "Iteration 3507: loss 1.4079993963241577\n",
      "Iteration 3508: loss 1.533625602722168\n",
      "Iteration 3509: loss 1.2362675666809082\n",
      "Iteration 3510: loss 1.4396685361862183\n",
      "Iteration 3511: loss 0.8676961064338684\n",
      "Iteration 3512: loss 1.287001609802246\n",
      "Iteration 3513: loss 1.362455129623413\n",
      "Iteration 3514: loss 1.7533178329467773\n",
      "Iteration 3515: loss 1.5646030902862549\n",
      "Iteration 3516: loss 1.3818103075027466\n",
      "Iteration 3517: loss 1.2126296758651733\n",
      "Iteration 3518: loss 1.6422144174575806\n",
      "Iteration 3519: loss 1.431117057800293\n",
      "Iteration 3520: loss 1.5691967010498047\n",
      "Iteration 3521: loss 1.1245081424713135\n",
      "Iteration 3522: loss 1.7011126279830933\n",
      "Iteration 3523: loss 1.1466478109359741\n",
      "Iteration 3524: loss 1.3119555711746216\n",
      "Iteration 3525: loss 1.2706596851348877\n",
      "Iteration 3526: loss 1.5423439741134644\n",
      "Iteration 3527: loss 2.6903481483459473\n",
      "Iteration 3528: loss 1.8612710237503052\n",
      "Iteration 3529: loss 2.300811529159546\n",
      "Iteration 3530: loss 1.8387726545333862\n",
      "Iteration 3531: loss 2.034144401550293\n",
      "Iteration 3532: loss 2.204926013946533\n",
      "Iteration 3533: loss 1.620880365371704\n",
      "Iteration 3534: loss 1.7483874559402466\n",
      "Iteration 3535: loss 1.1988935470581055\n",
      "Iteration 3536: loss 1.742892861366272\n",
      "Iteration 3537: loss 1.7858283519744873\n",
      "Iteration 3538: loss 1.443795919418335\n",
      "Iteration 3539: loss 1.8706605434417725\n",
      "Iteration 3540: loss 1.752026915550232\n",
      "Iteration 3541: loss 1.4648836851119995\n",
      "Iteration 3542: loss 2.072129964828491\n",
      "Iteration 3543: loss 1.6795145273208618\n",
      "Iteration 3544: loss 1.545556902885437\n",
      "Iteration 3545: loss 1.5490490198135376\n",
      "Iteration 3546: loss 1.7155158519744873\n",
      "Iteration 3547: loss 1.2682883739471436\n",
      "Iteration 3548: loss 1.839524745941162\n",
      "Iteration 3549: loss 1.6231292486190796\n",
      "Iteration 3550: loss 1.800694227218628\n",
      "Iteration 3551: loss 1.366572380065918\n",
      "Iteration 3552: loss 1.707177996635437\n",
      "Iteration 3553: loss 1.7877938747406006\n",
      "Iteration 3554: loss 1.233306646347046\n",
      "Iteration 3555: loss 1.5656973123550415\n",
      "Iteration 3556: loss 1.7964059114456177\n",
      "Iteration 3557: loss 1.475975513458252\n",
      "Iteration 3558: loss 1.6051735877990723\n",
      "Iteration 3559: loss 1.6754506826400757\n",
      "Iteration 3560: loss 1.4301360845565796\n",
      "Iteration 3561: loss 1.994295358657837\n",
      "Iteration 3562: loss 1.7003320455551147\n",
      "Iteration 3563: loss 1.5777587890625\n",
      "Iteration 3564: loss 1.0331168174743652\n",
      "Iteration 3565: loss 1.5072710514068604\n",
      "Iteration 3566: loss 1.1568715572357178\n",
      "Iteration 3567: loss 1.210466742515564\n",
      "Iteration 3568: loss 1.5188003778457642\n",
      "Iteration 3569: loss 1.474387288093567\n",
      "Iteration 3570: loss 1.4355264902114868\n",
      "Iteration 3571: loss 1.4097262620925903\n",
      "Iteration 3572: loss 1.0439621210098267\n",
      "Iteration 3573: loss 1.185782790184021\n",
      "Iteration 3574: loss 1.6721596717834473\n",
      "Iteration 3575: loss 1.37650728225708\n",
      "Iteration 3576: loss 1.270505666732788\n",
      "Iteration 3577: loss 1.3528660535812378\n",
      "Iteration 3578: loss 1.3742327690124512\n",
      "Iteration 3579: loss 1.314170002937317\n",
      "Iteration 3580: loss 1.1481952667236328\n",
      "Iteration 3581: loss 1.7866190671920776\n",
      "Iteration 3582: loss 1.0394519567489624\n",
      "Iteration 3583: loss 1.0235402584075928\n",
      "Iteration 3584: loss 1.2554045915603638\n",
      "Iteration 3585: loss 1.4763457775115967\n",
      "Iteration 3586: loss 1.1670950651168823\n",
      "Iteration 3587: loss 1.2074323892593384\n",
      "Iteration 3588: loss 0.9654262065887451\n",
      "Iteration 3589: loss 1.5248990058898926\n",
      "Iteration 3590: loss 1.1566520929336548\n",
      "Iteration 3591: loss 1.1162129640579224\n",
      "Iteration 3592: loss 1.114505410194397\n",
      "Iteration 3593: loss 1.2104133367538452\n",
      "Iteration 3594: loss 1.3375775814056396\n",
      "Iteration 3595: loss 1.655029058456421\n",
      "Iteration 3596: loss 1.4448739290237427\n",
      "Iteration 3597: loss 1.441975712776184\n",
      "Iteration 3598: loss 1.0875883102416992\n",
      "Iteration 3599: loss 1.3063040971755981\n",
      "Iteration 3600: loss 1.8789466619491577\n",
      "Iteration 3601: loss 1.2968297004699707\n",
      "Iteration 3602: loss 1.2550760507583618\n",
      "Iteration 3603: loss 1.2152228355407715\n",
      "Iteration 3604: loss 1.5057828426361084\n",
      "Iteration 3605: loss 1.0709381103515625\n",
      "Iteration 3606: loss 1.256466031074524\n",
      "Iteration 3607: loss 1.2813525199890137\n",
      "Iteration 3608: loss 1.5003212690353394\n",
      "Iteration 3609: loss 0.8807373046875\n",
      "Iteration 3610: loss 1.2364460229873657\n",
      "Iteration 3611: loss 1.305166244506836\n",
      "Iteration 3612: loss 1.1114262342453003\n",
      "Iteration 3613: loss 1.603005290031433\n",
      "Iteration 3614: loss 1.201258897781372\n",
      "Iteration 3615: loss 1.5947526693344116\n",
      "Iteration 3616: loss 1.1208674907684326\n",
      "Iteration 3617: loss 1.5711147785186768\n",
      "Iteration 3618: loss 1.3858004808425903\n",
      "Iteration 3619: loss 1.4534661769866943\n",
      "Iteration 3620: loss 1.1850446462631226\n",
      "Iteration 3621: loss 1.4336211681365967\n",
      "Iteration 3622: loss 1.1971219778060913\n",
      "Iteration 3623: loss 1.72271728515625\n",
      "Iteration 3624: loss 1.2491616010665894\n",
      "Iteration 3625: loss 1.2133755683898926\n",
      "Iteration 3626: loss 1.5362745523452759\n",
      "Iteration 3627: loss 1.5644128322601318\n",
      "Iteration 3628: loss 1.251984715461731\n",
      "Iteration 3629: loss 0.9387080073356628\n",
      "Iteration 3630: loss 1.4963027238845825\n",
      "Iteration 3631: loss 1.4365018606185913\n",
      "Iteration 3632: loss 1.5909500122070312\n",
      "Iteration 3633: loss 1.4579503536224365\n",
      "Iteration 3634: loss 1.4856561422348022\n",
      "Iteration 3635: loss 1.3009811639785767\n",
      "Iteration 3636: loss 1.120344638824463\n",
      "Iteration 3637: loss 1.901174545288086\n",
      "Iteration 3638: loss 1.1858892440795898\n",
      "Iteration 3639: loss 1.3890844583511353\n",
      "Iteration 3640: loss 1.2886877059936523\n",
      "Iteration 3641: loss 1.546020746231079\n",
      "Iteration 3642: loss 1.0978617668151855\n",
      "Iteration 3643: loss 1.552072286605835\n",
      "Iteration 3644: loss 1.3279759883880615\n",
      "Iteration 3645: loss 1.2716760635375977\n",
      "Iteration 3646: loss 1.136051058769226\n",
      "Iteration 3647: loss 1.2917274236679077\n",
      "Iteration 3648: loss 1.1812299489974976\n",
      "Iteration 3649: loss 1.7977080345153809\n",
      "Iteration 3650: loss 1.881632924079895\n",
      "Iteration 3651: loss 1.809301733970642\n",
      "Iteration 3652: loss 1.6810309886932373\n",
      "Iteration 3653: loss 2.242626190185547\n",
      "Iteration 3654: loss 1.963974118232727\n",
      "Iteration 3655: loss 2.1558523178100586\n",
      "Iteration 3656: loss 1.39650559425354\n",
      "Iteration 3657: loss 1.5150551795959473\n",
      "Iteration 3658: loss 1.8836843967437744\n",
      "Iteration 3659: loss 1.366345763206482\n",
      "Iteration 3660: loss 1.616914987564087\n",
      "Iteration 3661: loss 1.5574349164962769\n",
      "Iteration 3662: loss 1.6481918096542358\n",
      "Iteration 3663: loss 1.524357557296753\n",
      "Iteration 3664: loss 2.009053945541382\n",
      "Iteration 3665: loss 1.6905081272125244\n",
      "Iteration 3666: loss 1.5872255563735962\n",
      "Iteration 3667: loss 1.4213181734085083\n",
      "Iteration 3668: loss 1.689549207687378\n",
      "Iteration 3669: loss 1.835523009300232\n",
      "Iteration 3670: loss 1.8259122371673584\n",
      "Iteration 3671: loss 1.3706676959991455\n",
      "Iteration 3672: loss 1.4441723823547363\n",
      "Iteration 3673: loss 1.3049993515014648\n",
      "Iteration 3674: loss 1.3315722942352295\n",
      "Iteration 3675: loss 1.462437391281128\n",
      "Iteration 3676: loss 1.98640775680542\n",
      "Iteration 3677: loss 1.5041029453277588\n",
      "Iteration 3678: loss 1.7466182708740234\n",
      "Iteration 3679: loss 1.7069848775863647\n",
      "Iteration 3680: loss 1.8174210786819458\n",
      "Iteration 3681: loss 1.7288837432861328\n",
      "Iteration 3682: loss 1.5659246444702148\n",
      "Iteration 3683: loss 1.798714280128479\n",
      "Iteration 3684: loss 1.7190121412277222\n",
      "Iteration 3685: loss 1.6110917329788208\n",
      "Iteration 3686: loss 1.8888564109802246\n",
      "Iteration 3687: loss 1.6630357503890991\n",
      "Iteration 3688: loss 1.7801989316940308\n",
      "Iteration 3689: loss 1.5268315076828003\n",
      "Iteration 3690: loss 1.5029537677764893\n",
      "Iteration 3691: loss 1.408790111541748\n",
      "Iteration 3692: loss 1.6311804056167603\n",
      "Iteration 3693: loss 1.498956561088562\n",
      "Iteration 3694: loss 1.2111603021621704\n",
      "Iteration 3695: loss 1.6198557615280151\n",
      "Iteration 3696: loss 1.6193450689315796\n",
      "Iteration 3697: loss 1.5464117527008057\n",
      "Iteration 3698: loss 1.1858632564544678\n",
      "Iteration 3699: loss 1.3116596937179565\n",
      "Iteration 3700: loss 1.2469804286956787\n",
      "Iteration 3701: loss 1.9948623180389404\n",
      "Iteration 3702: loss 1.9095934629440308\n",
      "Iteration 3703: loss 1.505246877670288\n",
      "Iteration 3704: loss 2.375058889389038\n",
      "Iteration 3705: loss 1.2271091938018799\n",
      "Iteration 3706: loss 1.7074456214904785\n",
      "Iteration 3707: loss 1.636053442955017\n",
      "Iteration 3708: loss 1.4163053035736084\n",
      "Iteration 3709: loss 1.625227451324463\n",
      "Iteration 3710: loss 1.3190560340881348\n",
      "Iteration 3711: loss 1.4169946908950806\n",
      "Iteration 3712: loss 1.2304574251174927\n",
      "Iteration 3713: loss 1.59236478805542\n",
      "Iteration 3714: loss 1.586838960647583\n",
      "Iteration 3715: loss 1.3888607025146484\n",
      "Iteration 3716: loss 1.280375361442566\n",
      "Iteration 3717: loss 1.022964358329773\n",
      "Iteration 3718: loss 1.5335443019866943\n",
      "Iteration 3719: loss 1.5560318231582642\n",
      "Iteration 3720: loss 1.3295409679412842\n",
      "Iteration 3721: loss 1.32285475730896\n",
      "Iteration 3722: loss 1.4096132516860962\n",
      "Iteration 3723: loss 1.2891082763671875\n",
      "Iteration 3724: loss 1.5060516595840454\n",
      "Iteration 3725: loss 1.734232783317566\n",
      "Iteration 3726: loss 1.455403447151184\n",
      "Iteration 3727: loss 1.5133335590362549\n",
      "Iteration 3728: loss 0.8860918879508972\n",
      "Iteration 3729: loss 1.2469630241394043\n",
      "Iteration 3730: loss 1.3184468746185303\n",
      "Iteration 3731: loss 1.6152958869934082\n",
      "Iteration 3732: loss 1.677367925643921\n",
      "Iteration 3733: loss 1.2700196504592896\n",
      "Iteration 3734: loss 1.0369148254394531\n",
      "Iteration 3735: loss 1.436118483543396\n",
      "Iteration 3736: loss 1.1870025396347046\n",
      "Iteration 3737: loss 1.3868361711502075\n",
      "Iteration 3738: loss 1.5333987474441528\n",
      "Iteration 3739: loss 1.322314739227295\n",
      "Iteration 3740: loss 1.640444278717041\n",
      "Iteration 3741: loss 1.542722463607788\n",
      "Iteration 3742: loss 1.4146363735198975\n",
      "Iteration 3743: loss 1.3146321773529053\n",
      "Iteration 3744: loss 1.4796907901763916\n",
      "Iteration 3745: loss 1.3626879453659058\n",
      "Iteration 3746: loss 1.892738938331604\n",
      "Iteration 3747: loss 1.6848485469818115\n",
      "Iteration 3748: loss 1.2929688692092896\n",
      "Iteration 3749: loss 1.5268923044204712\n",
      "Iteration 3750: loss 1.2029340267181396\n",
      "Iteration 3751: loss 1.3025732040405273\n",
      "Iteration 3752: loss 1.512760877609253\n",
      "Iteration 3753: loss 1.4769246578216553\n",
      "Iteration 3754: loss 1.3303688764572144\n",
      "Iteration 3755: loss 1.7581783533096313\n",
      "Iteration 3756: loss 1.510376214981079\n",
      "Iteration 3757: loss 1.6000574827194214\n",
      "Iteration 3758: loss 1.331044316291809\n",
      "Iteration 3759: loss 1.6051772832870483\n",
      "Iteration 3760: loss 1.728798508644104\n",
      "Iteration 3761: loss 1.6878299713134766\n",
      "Iteration 3762: loss 1.6886811256408691\n",
      "Iteration 3763: loss 1.1642144918441772\n",
      "Iteration 3764: loss 1.429768681526184\n",
      "Iteration 3765: loss 1.3535783290863037\n",
      "Iteration 3766: loss 1.6133853197097778\n",
      "Iteration 3767: loss 1.3424084186553955\n",
      "Iteration 3768: loss 1.0107680559158325\n",
      "Iteration 3769: loss 1.3800604343414307\n",
      "Iteration 3770: loss 0.8936837315559387\n",
      "Iteration 3771: loss 1.250577688217163\n",
      "Iteration 3772: loss 1.3505053520202637\n",
      "Iteration 3773: loss 1.2353979349136353\n",
      "Iteration 3774: loss 1.278692603111267\n",
      "Iteration 3775: loss 0.96657794713974\n",
      "Iteration 3776: loss 1.2050552368164062\n",
      "Iteration 3777: loss 1.135914921760559\n",
      "Iteration 3778: loss 1.268017053604126\n",
      "Iteration 3779: loss 1.7033333778381348\n",
      "Iteration 3780: loss 0.8415005207061768\n",
      "Iteration 3781: loss 1.3527086973190308\n",
      "Iteration 3782: loss 1.2965987920761108\n",
      "Iteration 3783: loss 1.399963617324829\n",
      "Iteration 3784: loss 1.2473866939544678\n",
      "Iteration 3785: loss 1.635924220085144\n",
      "Iteration 3786: loss 1.1931296586990356\n",
      "Iteration 3787: loss 1.3893667459487915\n",
      "Iteration 3788: loss 1.2588039636611938\n",
      "Iteration 3789: loss 1.293445110321045\n",
      "Iteration 3790: loss 1.219154953956604\n",
      "Iteration 3791: loss 1.2073767185211182\n",
      "Iteration 3792: loss 1.2929425239562988\n",
      "Iteration 3793: loss 1.0837830305099487\n",
      "Iteration 3794: loss 1.3714120388031006\n",
      "Iteration 3795: loss 0.9148917198181152\n",
      "Iteration 3796: loss 1.189620018005371\n",
      "Iteration 3797: loss 1.3935041427612305\n",
      "Iteration 3798: loss 0.9563032984733582\n",
      "Iteration 3799: loss 1.3824245929718018\n",
      "Iteration 3800: loss 0.6700071692466736\n",
      "Iteration 3801: loss 1.496597409248352\n",
      "Iteration 3802: loss 1.014284372329712\n",
      "Iteration 3803: loss 0.991747260093689\n",
      "Iteration 3804: loss 1.363911747932434\n",
      "Iteration 3805: loss 0.8827695250511169\n",
      "Iteration 3806: loss 1.3357632160186768\n",
      "Iteration 3807: loss 1.5786290168762207\n",
      "Iteration 3808: loss 1.408327579498291\n",
      "Iteration 3809: loss 2.2731339931488037\n",
      "Iteration 3810: loss 1.9421852827072144\n",
      "Iteration 3811: loss 2.6135759353637695\n",
      "Iteration 3812: loss 2.118381977081299\n",
      "Iteration 3813: loss 1.66232430934906\n",
      "Iteration 3814: loss 2.050682306289673\n",
      "Iteration 3815: loss 1.9581103324890137\n",
      "Iteration 3816: loss 2.04736590385437\n",
      "Iteration 3817: loss 1.926695466041565\n",
      "Iteration 3818: loss 1.7076903581619263\n",
      "Iteration 3819: loss 1.9782445430755615\n",
      "Iteration 3820: loss 1.727632999420166\n",
      "Iteration 3821: loss 1.4748950004577637\n",
      "Iteration 3822: loss 2.0129098892211914\n",
      "Iteration 3823: loss 2.1960439682006836\n",
      "Iteration 3824: loss 2.2416436672210693\n",
      "Iteration 3825: loss 2.1680898666381836\n",
      "Iteration 3826: loss 1.9576199054718018\n",
      "Iteration 3827: loss 1.987384557723999\n",
      "Iteration 3828: loss 2.485344171524048\n",
      "Iteration 3829: loss 2.1696434020996094\n",
      "Iteration 3830: loss 1.7695999145507812\n",
      "Iteration 3831: loss 2.0335726737976074\n",
      "Iteration 3832: loss 2.098499298095703\n",
      "Iteration 3833: loss 1.8018478155136108\n",
      "Iteration 3834: loss 1.8638999462127686\n",
      "Iteration 3835: loss 1.6268032789230347\n",
      "Iteration 3836: loss 1.7078015804290771\n",
      "Iteration 3837: loss 1.5051519870758057\n",
      "Iteration 3838: loss 1.8199172019958496\n",
      "Iteration 3839: loss 1.3895553350448608\n",
      "Iteration 3840: loss 2.00358247756958\n",
      "Iteration 3841: loss 1.7844942808151245\n",
      "Iteration 3842: loss 1.6357488632202148\n",
      "Iteration 3843: loss 1.3381447792053223\n",
      "Iteration 3844: loss 1.4260365962982178\n",
      "Iteration 3845: loss 1.662932276725769\n",
      "Iteration 3846: loss 1.7522895336151123\n",
      "Iteration 3847: loss 2.0788800716400146\n",
      "Iteration 3848: loss 1.6382933855056763\n",
      "Iteration 3849: loss 1.5258249044418335\n",
      "Iteration 3850: loss 1.5437601804733276\n",
      "Iteration 3851: loss 1.5969398021697998\n",
      "Iteration 3852: loss 1.575594425201416\n",
      "Iteration 3853: loss 1.5380001068115234\n",
      "Iteration 3854: loss 1.8699090480804443\n",
      "Iteration 3855: loss 1.2016000747680664\n",
      "Iteration 3856: loss 1.46699857711792\n",
      "Iteration 3857: loss 1.3652551174163818\n",
      "Iteration 3858: loss 1.6430721282958984\n",
      "Iteration 3859: loss 1.2723628282546997\n",
      "Iteration 3860: loss 2.042052745819092\n",
      "Iteration 3861: loss 1.3908300399780273\n",
      "Iteration 3862: loss 1.1591373682022095\n",
      "Iteration 3863: loss 1.326764464378357\n",
      "Iteration 3864: loss 1.5165934562683105\n",
      "Iteration 3865: loss 1.2744632959365845\n",
      "Iteration 3866: loss 1.639782428741455\n",
      "Iteration 3867: loss 1.1082054376602173\n",
      "Iteration 3868: loss 1.5585825443267822\n",
      "Iteration 3869: loss 1.2700539827346802\n",
      "Iteration 3870: loss 1.2876005172729492\n",
      "Iteration 3871: loss 1.409740924835205\n",
      "Iteration 3872: loss 1.0207165479660034\n",
      "Iteration 3873: loss 1.4241256713867188\n",
      "Iteration 3874: loss 1.6918702125549316\n",
      "Iteration 3875: loss 1.5345155000686646\n",
      "Iteration 3876: loss 1.6635537147521973\n",
      "Iteration 3877: loss 1.4656835794448853\n",
      "Iteration 3878: loss 1.729330062866211\n",
      "Iteration 3879: loss 1.4951387643814087\n",
      "Iteration 3880: loss 1.385681390762329\n",
      "Iteration 3881: loss 1.4468472003936768\n",
      "Iteration 3882: loss 1.1211373805999756\n",
      "Iteration 3883: loss 1.7438305616378784\n",
      "Iteration 3884: loss 1.4332146644592285\n",
      "Iteration 3885: loss 1.2449758052825928\n",
      "Iteration 3886: loss 1.417625904083252\n",
      "Iteration 3887: loss 1.5077128410339355\n",
      "Iteration 3888: loss 1.3226743936538696\n",
      "Iteration 3889: loss 1.4286941289901733\n",
      "Iteration 3890: loss 1.44868004322052\n",
      "Iteration 3891: loss 1.2515687942504883\n",
      "Iteration 3892: loss 1.1403177976608276\n",
      "Iteration 3893: loss 1.0688889026641846\n",
      "Iteration 3894: loss 1.435325026512146\n",
      "Iteration 3895: loss 1.4660619497299194\n",
      "Iteration 3896: loss 1.1824944019317627\n",
      "Iteration 3897: loss 1.2333416938781738\n",
      "Iteration 3898: loss 1.2615966796875\n",
      "Iteration 3899: loss 1.622135877609253\n",
      "Iteration 3900: loss 1.254604458808899\n",
      "Iteration 3901: loss 1.4321283102035522\n",
      "Iteration 3902: loss 0.9758927226066589\n",
      "Iteration 3903: loss 1.243762493133545\n",
      "Iteration 3904: loss 1.666166067123413\n",
      "Iteration 3905: loss 1.211174488067627\n",
      "Iteration 3906: loss 1.2542921304702759\n",
      "Iteration 3907: loss 1.3096578121185303\n",
      "Iteration 3908: loss 1.1086368560791016\n",
      "Iteration 3909: loss 1.5313398838043213\n",
      "Iteration 3910: loss 0.5333861112594604\n",
      "Iteration 3911: loss 1.3948407173156738\n",
      "Iteration 3912: loss 1.6268481016159058\n",
      "Iteration 3913: loss 0.9481147527694702\n",
      "Iteration 3914: loss 1.6552178859710693\n",
      "Iteration 3915: loss 1.4912340641021729\n",
      "Iteration 3916: loss 1.2763280868530273\n",
      "Iteration 3917: loss 1.5533220767974854\n",
      "Iteration 3918: loss 1.223169207572937\n",
      "Iteration 3919: loss 1.5739480257034302\n",
      "Iteration 3920: loss 1.4042894840240479\n",
      "Iteration 3921: loss 1.1581631898880005\n",
      "Iteration 3922: loss 1.1321197748184204\n",
      "Iteration 3923: loss 1.7910022735595703\n",
      "Iteration 3924: loss 1.4250032901763916\n",
      "Iteration 3925: loss 0.6727354526519775\n",
      "Iteration 3926: loss 1.3013029098510742\n",
      "Iteration 3927: loss 0.8763787746429443\n",
      "Iteration 3928: loss 1.0874494314193726\n",
      "Iteration 3929: loss 0.9374518990516663\n",
      "Iteration 3930: loss 1.210499882698059\n",
      "Iteration 3931: loss 0.9186009168624878\n",
      "Iteration 3932: loss 1.6066774129867554\n",
      "Iteration 3933: loss 1.2735289335250854\n",
      "Iteration 3934: loss 0.9215271472930908\n",
      "Iteration 3935: loss 1.2221237421035767\n",
      "Iteration 3936: loss 1.2033934593200684\n",
      "Iteration 3937: loss 1.209445595741272\n",
      "Iteration 3938: loss 0.9874261021614075\n",
      "Iteration 3939: loss 1.015616536140442\n",
      "Iteration 3940: loss 1.1250782012939453\n",
      "Iteration 3941: loss 1.0527276992797852\n",
      "Iteration 3942: loss 1.4038944244384766\n",
      "Iteration 3943: loss 0.8887007236480713\n",
      "Iteration 3944: loss 1.1569867134094238\n",
      "Iteration 3945: loss 0.8550698161125183\n",
      "Iteration 3946: loss 1.0591880083084106\n",
      "Iteration 3947: loss 0.8941952586174011\n",
      "Iteration 3948: loss 1.6273123025894165\n",
      "Iteration 3949: loss 1.2870278358459473\n",
      "Iteration 3950: loss 0.9027397632598877\n",
      "Iteration 3951: loss 1.617693305015564\n",
      "Iteration 3952: loss 1.2130672931671143\n",
      "Iteration 3953: loss 1.1989977359771729\n",
      "Iteration 3954: loss 1.0785753726959229\n",
      "Iteration 3955: loss 1.5146920680999756\n",
      "Iteration 3956: loss 1.452857494354248\n",
      "Iteration 3957: loss 1.443928599357605\n",
      "Iteration 3958: loss 1.4093598127365112\n",
      "Iteration 3959: loss 2.0708742141723633\n",
      "Iteration 3960: loss 1.406119704246521\n",
      "Iteration 3961: loss 1.5425835847854614\n",
      "Iteration 3962: loss 1.3685734272003174\n",
      "Iteration 3963: loss 1.4692764282226562\n",
      "Iteration 3964: loss 1.2237459421157837\n",
      "Iteration 3965: loss 1.3139770030975342\n",
      "Iteration 3966: loss 1.0529183149337769\n",
      "Iteration 3967: loss 1.5603142976760864\n",
      "Iteration 3968: loss 1.2174222469329834\n",
      "Iteration 3969: loss 1.2664265632629395\n",
      "Iteration 3970: loss 1.490803837776184\n",
      "Iteration 3971: loss 0.9248659014701843\n",
      "Iteration 3972: loss 1.0162415504455566\n",
      "Iteration 3973: loss 1.4207476377487183\n",
      "Iteration 3974: loss 0.9346246123313904\n",
      "Iteration 3975: loss 1.1824746131896973\n",
      "Iteration 3976: loss 1.3060369491577148\n",
      "Iteration 3977: loss 1.1444342136383057\n",
      "Iteration 3978: loss 1.485579013824463\n",
      "Iteration 3979: loss 1.9163883924484253\n",
      "Iteration 3980: loss 1.4549996852874756\n",
      "Iteration 3981: loss 1.2209522724151611\n",
      "Iteration 3982: loss 1.756813883781433\n",
      "Iteration 3983: loss 1.5246250629425049\n",
      "Iteration 3984: loss 1.0244364738464355\n",
      "Iteration 3985: loss 1.4768608808517456\n",
      "Iteration 3986: loss 0.6846266984939575\n",
      "Iteration 3987: loss 1.102603793144226\n",
      "Iteration 3988: loss 1.6518189907073975\n",
      "Iteration 3989: loss 1.4319332838058472\n",
      "Iteration 3990: loss 1.0259883403778076\n",
      "Iteration 3991: loss 1.1345183849334717\n",
      "Iteration 3992: loss 1.3525598049163818\n",
      "Iteration 3993: loss 0.608191728591919\n",
      "Iteration 3994: loss 1.2982609272003174\n",
      "Iteration 3995: loss 1.3024002313613892\n",
      "Iteration 3996: loss 1.2625467777252197\n",
      "Iteration 3997: loss 1.202466607093811\n",
      "Iteration 3998: loss 1.0832304954528809\n",
      "Iteration 3999: loss 0.9735574126243591\n",
      "Iteration 4000: loss 1.2329884767532349\n",
      "Iteration 4001: loss 1.0882289409637451\n",
      "Iteration 4002: loss 1.0807658433914185\n",
      "Iteration 4003: loss 1.416770339012146\n",
      "Iteration 4004: loss 1.2638113498687744\n",
      "Iteration 4005: loss 1.0980578660964966\n",
      "Iteration 4006: loss 0.949686586856842\n",
      "Iteration 4007: loss 1.3200368881225586\n",
      "Iteration 4008: loss 0.8189871311187744\n",
      "Iteration 4009: loss 1.5461845397949219\n",
      "Iteration 4010: loss 0.7449958920478821\n",
      "Iteration 4011: loss 1.31593656539917\n",
      "Iteration 4012: loss 1.4345240592956543\n",
      "Iteration 4013: loss 0.8957412242889404\n",
      "Iteration 4014: loss 0.5909517407417297\n",
      "Iteration 4015: loss 0.8021104335784912\n",
      "Iteration 4016: loss 1.3219937086105347\n",
      "Iteration 4017: loss 1.213505506515503\n",
      "Iteration 4018: loss 1.535523772239685\n",
      "Iteration 4019: loss 0.8707099556922913\n",
      "Iteration 4020: loss 1.0263503789901733\n",
      "Iteration 4021: loss 0.9182450771331787\n",
      "Iteration 4022: loss 1.2660953998565674\n",
      "Iteration 4023: loss 1.3372193574905396\n",
      "Iteration 4024: loss 0.8791038393974304\n",
      "Iteration 4025: loss 0.982966423034668\n",
      "Iteration 4026: loss 0.8641496896743774\n",
      "Iteration 4027: loss 0.9471070766448975\n",
      "Iteration 4028: loss 1.0029340982437134\n",
      "Iteration 4029: loss 0.7936162948608398\n",
      "Iteration 4030: loss 1.0450658798217773\n",
      "Iteration 4031: loss 0.7266244888305664\n",
      "Iteration 4032: loss 1.0202239751815796\n",
      "Iteration 4033: loss 0.809539794921875\n",
      "Iteration 4034: loss 1.0812649726867676\n",
      "Iteration 4035: loss 0.9005126953125\n",
      "Iteration 4036: loss 0.9358772039413452\n",
      "Iteration 4037: loss 0.5795557498931885\n",
      "Iteration 4038: loss 1.5601775646209717\n",
      "Iteration 4039: loss 1.3710293769836426\n",
      "Iteration 4040: loss 0.8179194331169128\n",
      "Iteration 4041: loss 0.8036256432533264\n",
      "Iteration 4042: loss 0.824400782585144\n",
      "Iteration 4043: loss 1.0589240789413452\n",
      "Iteration 4044: loss 1.0637149810791016\n",
      "Iteration 4045: loss 0.9589664340019226\n",
      "Iteration 4046: loss 1.108673095703125\n",
      "Iteration 4047: loss 1.123801589012146\n",
      "Iteration 4048: loss 0.7105642557144165\n",
      "Iteration 4049: loss 0.7006254196166992\n",
      "Iteration 4050: loss 0.832692563533783\n",
      "Iteration 4051: loss 0.8751280903816223\n",
      "Iteration 4052: loss 1.289015293121338\n",
      "Iteration 4053: loss 1.2512296438217163\n",
      "Iteration 4054: loss 1.1233370304107666\n",
      "Iteration 4055: loss 1.308884859085083\n",
      "Iteration 4056: loss 1.0539333820343018\n",
      "Iteration 4057: loss 0.7705808281898499\n",
      "Iteration 4058: loss 1.0778796672821045\n",
      "Iteration 4059: loss 1.2900463342666626\n",
      "Iteration 4060: loss 0.6903112530708313\n",
      "Iteration 4061: loss 0.8568041324615479\n",
      "Iteration 4062: loss 0.8607519268989563\n",
      "Iteration 4063: loss 0.9633520245552063\n",
      "Iteration 4064: loss 1.0033444166183472\n",
      "Iteration 4065: loss 0.6321181058883667\n",
      "Iteration 4066: loss 1.0679489374160767\n",
      "Iteration 4067: loss 1.1650382280349731\n",
      "Iteration 4068: loss 1.033615231513977\n",
      "Iteration 4069: loss 0.7665380835533142\n",
      "Iteration 4070: loss 0.7632539868354797\n",
      "Iteration 4071: loss 0.5355839729309082\n",
      "Iteration 4072: loss 0.6565738320350647\n",
      "Iteration 4073: loss 1.0948280096054077\n",
      "Iteration 4074: loss 0.910967230796814\n",
      "Iteration 4075: loss 1.3656575679779053\n",
      "Iteration 4076: loss 1.1529393196105957\n",
      "Iteration 4077: loss 0.9130440354347229\n",
      "Iteration 4078: loss 0.761889636516571\n",
      "Iteration 4079: loss 1.3359893560409546\n",
      "Iteration 4080: loss 0.9398782849311829\n",
      "Iteration 4081: loss 0.824290931224823\n",
      "Iteration 4082: loss 1.6297656297683716\n",
      "Iteration 4083: loss 1.455424427986145\n",
      "Iteration 4084: loss 1.0949288606643677\n",
      "Iteration 4085: loss 1.3882427215576172\n",
      "Iteration 4086: loss 1.292463779449463\n",
      "Iteration 4087: loss 1.211411952972412\n",
      "Iteration 4088: loss 0.9250227212905884\n",
      "Iteration 4089: loss 0.8442819118499756\n",
      "Iteration 4090: loss 0.8066992163658142\n",
      "Iteration 4091: loss 0.5971856713294983\n",
      "Iteration 4092: loss 0.8289304375648499\n",
      "Iteration 4093: loss 0.7836587429046631\n",
      "Iteration 4094: loss 1.1661567687988281\n",
      "Iteration 4095: loss 1.3600060939788818\n",
      "Iteration 4096: loss 1.4258952140808105\n",
      "Iteration 4097: loss 1.1346399784088135\n",
      "Iteration 4098: loss 2.1617767810821533\n",
      "Iteration 4099: loss 1.0079659223556519\n",
      "Iteration 4100: loss 1.6923052072525024\n",
      "Iteration 4101: loss 1.4678658246994019\n",
      "Iteration 4102: loss 1.791297435760498\n",
      "Iteration 4103: loss 1.774418592453003\n",
      "Iteration 4104: loss 1.8005919456481934\n",
      "Iteration 4105: loss 1.947214961051941\n",
      "Iteration 4106: loss 1.4435588121414185\n",
      "Iteration 4107: loss 1.9321801662445068\n",
      "Iteration 4108: loss 1.911460280418396\n",
      "Iteration 4109: loss 2.0986733436584473\n",
      "Iteration 4110: loss 2.0130889415740967\n",
      "Iteration 4111: loss 2.4535129070281982\n",
      "Iteration 4112: loss 1.2460224628448486\n",
      "Iteration 4113: loss 1.7332967519760132\n",
      "Iteration 4114: loss 1.4766770601272583\n",
      "Iteration 4115: loss 1.902290940284729\n",
      "Iteration 4116: loss 1.8183242082595825\n",
      "Iteration 4117: loss 2.2949905395507812\n",
      "Iteration 4118: loss 2.076383352279663\n",
      "Iteration 4119: loss 1.7930210828781128\n",
      "Iteration 4120: loss 1.964882731437683\n",
      "Iteration 4121: loss 1.9272594451904297\n",
      "Iteration 4122: loss 1.7676175832748413\n",
      "Iteration 4123: loss 1.0773788690567017\n",
      "Iteration 4124: loss 1.6960405111312866\n",
      "Iteration 4125: loss 1.4257092475891113\n",
      "Iteration 4126: loss 1.6734343767166138\n",
      "Iteration 4127: loss 1.7119359970092773\n",
      "Iteration 4128: loss 1.042106032371521\n",
      "Iteration 4129: loss 1.0112383365631104\n",
      "Iteration 4130: loss 1.2800235748291016\n",
      "Iteration 4131: loss 1.202250599861145\n",
      "Iteration 4132: loss 1.0162124633789062\n",
      "Iteration 4133: loss 0.9883241057395935\n",
      "Iteration 4134: loss 1.2275359630584717\n",
      "Iteration 4135: loss 0.8342552185058594\n",
      "Iteration 4136: loss 0.9295299053192139\n",
      "Iteration 4137: loss 1.420255422592163\n",
      "Iteration 4138: loss 1.4591201543807983\n",
      "Iteration 4139: loss 1.4310113191604614\n",
      "Iteration 4140: loss 0.9699007868766785\n",
      "Iteration 4141: loss 1.10233736038208\n",
      "Iteration 4142: loss 1.0445836782455444\n",
      "Iteration 4143: loss 0.8763129711151123\n",
      "Iteration 4144: loss 1.0271366834640503\n",
      "Iteration 4145: loss 0.9674737453460693\n",
      "Iteration 4146: loss 0.9328733086585999\n",
      "Iteration 4147: loss 1.0317391157150269\n",
      "Iteration 4148: loss 0.8805134296417236\n",
      "Iteration 4149: loss 0.7060041427612305\n",
      "Iteration 4150: loss 1.285425066947937\n",
      "Iteration 4151: loss 0.9478686451911926\n",
      "Iteration 4152: loss 1.4085172414779663\n",
      "Iteration 4153: loss 1.6471775770187378\n",
      "Iteration 4154: loss 1.3105204105377197\n",
      "Iteration 4155: loss 1.4804948568344116\n",
      "Iteration 4156: loss 1.2318998575210571\n",
      "Iteration 4157: loss 2.08176326751709\n",
      "Iteration 4158: loss 1.410726547241211\n",
      "Iteration 4159: loss 1.2978936433792114\n",
      "Iteration 4160: loss 0.9070591330528259\n",
      "Iteration 4161: loss 1.0995936393737793\n",
      "Iteration 4162: loss 1.3477636575698853\n",
      "Iteration 4163: loss 0.7970811128616333\n",
      "Iteration 4164: loss 1.4138888120651245\n",
      "Iteration 4165: loss 1.4736477136611938\n",
      "Iteration 4166: loss 0.92107754945755\n",
      "Iteration 4167: loss 1.0439343452453613\n",
      "Iteration 4168: loss 1.0450619459152222\n",
      "Iteration 4169: loss 1.1481469869613647\n",
      "Iteration 4170: loss 0.8091139197349548\n",
      "Iteration 4171: loss 1.108925461769104\n",
      "Iteration 4172: loss 0.9558209180831909\n",
      "Iteration 4173: loss 0.8890771269798279\n",
      "Iteration 4174: loss 0.4825607240200043\n",
      "Iteration 4175: loss 0.8758850693702698\n",
      "Iteration 4176: loss 1.0076828002929688\n",
      "Iteration 4177: loss 0.6924769282341003\n",
      "Iteration 4178: loss 0.8654414415359497\n",
      "Iteration 4179: loss 0.943007230758667\n",
      "Iteration 4180: loss 1.2140542268753052\n",
      "Iteration 4181: loss 0.7501371502876282\n",
      "Iteration 4182: loss 0.8894549608230591\n",
      "Iteration 4183: loss 0.9989153742790222\n",
      "Iteration 4184: loss 0.9107626676559448\n",
      "Iteration 4185: loss 0.6340373158454895\n",
      "Iteration 4186: loss 0.8749768137931824\n",
      "Iteration 4187: loss 0.615812361240387\n",
      "Iteration 4188: loss 0.708585262298584\n",
      "Iteration 4189: loss 0.6969109773635864\n",
      "Iteration 4190: loss 0.8560563325881958\n",
      "Iteration 4191: loss 0.9035982489585876\n",
      "Iteration 4192: loss 0.6572222113609314\n",
      "Iteration 4193: loss 0.6586047410964966\n",
      "Iteration 4194: loss 1.1822469234466553\n",
      "Iteration 4195: loss 1.07328462600708\n",
      "Iteration 4196: loss 1.1044301986694336\n",
      "Iteration 4197: loss 1.4305455684661865\n",
      "Iteration 4198: loss 1.350821614265442\n",
      "Iteration 4199: loss 1.271178126335144\n",
      "Iteration 4200: loss 1.3508105278015137\n",
      "Iteration 4201: loss 1.6918435096740723\n",
      "Iteration 4202: loss 1.543943166732788\n",
      "Iteration 4203: loss 1.4817156791687012\n",
      "Iteration 4204: loss 1.2993654012680054\n",
      "Iteration 4205: loss 1.2188465595245361\n",
      "Iteration 4206: loss 1.410463571548462\n",
      "Iteration 4207: loss 1.1424288749694824\n",
      "Iteration 4208: loss 0.8448484539985657\n",
      "Iteration 4209: loss 1.0297034978866577\n",
      "Iteration 4210: loss 1.396856665611267\n",
      "Iteration 4211: loss 1.140476107597351\n",
      "Iteration 4212: loss 1.2427637577056885\n",
      "Iteration 4213: loss 1.3375842571258545\n",
      "Iteration 4214: loss 1.2382745742797852\n",
      "Iteration 4215: loss 0.9211254715919495\n",
      "Iteration 4216: loss 1.1245280504226685\n",
      "Iteration 4217: loss 0.907315194606781\n",
      "Iteration 4218: loss 0.725687563419342\n",
      "Iteration 4219: loss 1.0674241781234741\n",
      "Iteration 4220: loss 1.1578749418258667\n",
      "Iteration 4221: loss 1.0281989574432373\n",
      "Iteration 4222: loss 1.010796070098877\n",
      "Iteration 4223: loss 0.7497583031654358\n",
      "Iteration 4224: loss 1.0259994268417358\n",
      "Iteration 4225: loss 1.5103133916854858\n",
      "Iteration 4226: loss 1.218263864517212\n",
      "Iteration 4227: loss 1.0051766633987427\n",
      "Iteration 4228: loss 1.1995209455490112\n",
      "Iteration 4229: loss 1.0111809968948364\n",
      "Iteration 4230: loss 0.8052682280540466\n",
      "Iteration 4231: loss 1.1463215351104736\n",
      "Iteration 4232: loss 0.8405141830444336\n",
      "Iteration 4233: loss 0.8802402019500732\n",
      "Iteration 4234: loss 0.8693457841873169\n",
      "Iteration 4235: loss 1.8769177198410034\n",
      "Iteration 4236: loss 0.5699092745780945\n",
      "Iteration 4237: loss 0.9108833074569702\n",
      "Iteration 4238: loss 1.4411795139312744\n",
      "Iteration 4239: loss 1.433512568473816\n",
      "Iteration 4240: loss 1.2264127731323242\n",
      "Iteration 4241: loss 1.2143734693527222\n",
      "Iteration 4242: loss 1.5296789407730103\n",
      "Iteration 4243: loss 1.3863381147384644\n",
      "Iteration 4244: loss 1.32706618309021\n",
      "Iteration 4245: loss 0.8819139003753662\n",
      "Iteration 4246: loss 1.2403297424316406\n",
      "Iteration 4247: loss 0.7919332981109619\n",
      "Iteration 4248: loss 1.09492027759552\n",
      "Iteration 4249: loss 1.2073432207107544\n",
      "Iteration 4250: loss 0.7980503439903259\n",
      "Iteration 4251: loss 1.2742984294891357\n",
      "Iteration 4252: loss 0.9294145703315735\n",
      "Iteration 4253: loss 1.0021852254867554\n",
      "Iteration 4254: loss 1.1017107963562012\n",
      "Iteration 4255: loss 0.9512683749198914\n",
      "Iteration 4256: loss 1.0287671089172363\n",
      "Iteration 4257: loss 0.7381097078323364\n",
      "Iteration 4258: loss 1.0219740867614746\n",
      "Iteration 4259: loss 1.0212152004241943\n",
      "Iteration 4260: loss 0.9119453430175781\n",
      "Iteration 4261: loss 1.0172475576400757\n",
      "Iteration 4262: loss 0.7283096313476562\n",
      "Iteration 4263: loss 1.0982129573822021\n",
      "Iteration 4264: loss 1.158207654953003\n",
      "Iteration 4265: loss 0.695501983165741\n",
      "Iteration 4266: loss 1.0475326776504517\n",
      "Iteration 4267: loss 0.8406016230583191\n",
      "Iteration 4268: loss 0.8346301317214966\n",
      "Iteration 4269: loss 1.266481876373291\n",
      "Iteration 4270: loss 1.0911482572555542\n",
      "Iteration 4271: loss 1.8819043636322021\n",
      "Iteration 4272: loss 1.0398629903793335\n",
      "Iteration 4273: loss 1.4804949760437012\n",
      "Iteration 4274: loss 0.9421432018280029\n",
      "Iteration 4275: loss 0.8276946544647217\n",
      "Iteration 4276: loss 0.8994505405426025\n",
      "Iteration 4277: loss 0.7720947265625\n",
      "Iteration 4278: loss 0.8353179693222046\n",
      "Iteration 4279: loss 0.7357671856880188\n",
      "Iteration 4280: loss 0.48328647017478943\n",
      "Iteration 4281: loss 0.8327250480651855\n",
      "Iteration 4282: loss 0.8292863368988037\n",
      "Iteration 4283: loss 1.2215042114257812\n",
      "Iteration 4284: loss 0.8295710682868958\n",
      "Iteration 4285: loss 0.7387630343437195\n",
      "Iteration 4286: loss 0.9550083875656128\n",
      "Iteration 4287: loss 0.604385256767273\n",
      "Iteration 4288: loss 1.0238124132156372\n",
      "Iteration 4289: loss 0.5365380644798279\n",
      "Iteration 4290: loss 0.8676027655601501\n",
      "Iteration 4291: loss 0.4805488884449005\n",
      "Iteration 4292: loss 1.0036808252334595\n",
      "Iteration 4293: loss 0.8572981357574463\n",
      "Iteration 4294: loss 0.4853493273258209\n",
      "Iteration 4295: loss 0.9987278580665588\n",
      "Iteration 4296: loss 0.5006675720214844\n",
      "Iteration 4297: loss 0.8751932978630066\n",
      "Iteration 4298: loss 0.7661959528923035\n",
      "Iteration 4299: loss 0.8229431509971619\n",
      "Iteration 4300: loss 1.2660233974456787\n",
      "Iteration 4301: loss 0.9547330737113953\n",
      "Iteration 4302: loss 0.8692147135734558\n",
      "Iteration 4303: loss 1.1537363529205322\n",
      "Iteration 4304: loss 0.9377068877220154\n",
      "Iteration 4305: loss 1.128066897392273\n",
      "Iteration 4306: loss 0.9222552180290222\n",
      "Iteration 4307: loss 1.3754128217697144\n",
      "Iteration 4308: loss 0.9918385744094849\n",
      "Iteration 4309: loss 1.2208372354507446\n",
      "Iteration 4310: loss 1.166445255279541\n",
      "Iteration 4311: loss 0.9683155417442322\n",
      "Iteration 4312: loss 0.5736008882522583\n",
      "Iteration 4313: loss 0.6346893906593323\n",
      "Iteration 4314: loss 1.1965467929840088\n",
      "Iteration 4315: loss 0.6607010364532471\n",
      "Iteration 4316: loss 1.4696018695831299\n",
      "Iteration 4317: loss 0.6782230734825134\n",
      "Iteration 4318: loss 0.8889853358268738\n",
      "Iteration 4319: loss 0.8795464634895325\n",
      "Iteration 4320: loss 0.9576850533485413\n",
      "Iteration 4321: loss 1.051782250404358\n",
      "Iteration 4322: loss 1.3008054494857788\n",
      "Iteration 4323: loss 1.1450538635253906\n",
      "Iteration 4324: loss 1.4593881368637085\n",
      "Iteration 4325: loss 1.0744832754135132\n",
      "Iteration 4326: loss 1.0810911655426025\n",
      "Iteration 4327: loss 0.8211706876754761\n",
      "Iteration 4328: loss 1.0863523483276367\n",
      "Iteration 4329: loss 1.3477771282196045\n",
      "Iteration 4330: loss 1.5538005828857422\n",
      "Iteration 4331: loss 0.6308989524841309\n",
      "Iteration 4332: loss 1.085028052330017\n",
      "Iteration 4333: loss 1.055616021156311\n",
      "Iteration 4334: loss 1.2616263628005981\n",
      "Iteration 4335: loss 0.6196070909500122\n",
      "Iteration 4336: loss 1.354730486869812\n",
      "Iteration 4337: loss 1.1678013801574707\n",
      "Iteration 4338: loss 1.013716220855713\n",
      "Iteration 4339: loss 0.9075756669044495\n",
      "Iteration 4340: loss 1.0198500156402588\n",
      "Iteration 4341: loss 0.8663206100463867\n",
      "Iteration 4342: loss 0.9711971879005432\n",
      "Iteration 4343: loss 0.5536549091339111\n",
      "Iteration 4344: loss 0.7860536575317383\n",
      "Iteration 4345: loss 1.1977002620697021\n",
      "Iteration 4346: loss 1.019753336906433\n",
      "Iteration 4347: loss 0.9450237154960632\n",
      "Iteration 4348: loss 0.7280881404876709\n",
      "Iteration 4349: loss 0.7970399260520935\n",
      "Iteration 4350: loss 0.9498656392097473\n",
      "Iteration 4351: loss 1.0288304090499878\n",
      "Iteration 4352: loss 0.9738182425498962\n",
      "Iteration 4353: loss 1.6405397653579712\n",
      "Iteration 4354: loss 1.4396013021469116\n",
      "Iteration 4355: loss 1.6688851118087769\n",
      "Iteration 4356: loss 1.487196445465088\n",
      "Iteration 4357: loss 1.2331218719482422\n",
      "Iteration 4358: loss 1.2729747295379639\n",
      "Iteration 4359: loss 1.8548091650009155\n",
      "Iteration 4360: loss 1.1742024421691895\n",
      "Iteration 4361: loss 1.8019319772720337\n",
      "Iteration 4362: loss 1.5560966730117798\n",
      "Iteration 4363: loss 1.3369450569152832\n",
      "Iteration 4364: loss 1.6403484344482422\n",
      "Iteration 4365: loss 1.5216988325119019\n",
      "Iteration 4366: loss 1.0825554132461548\n",
      "Iteration 4367: loss 1.159284234046936\n",
      "Iteration 4368: loss 0.7692282199859619\n",
      "Iteration 4369: loss 0.7205632328987122\n",
      "Iteration 4370: loss 0.8428791761398315\n",
      "Iteration 4371: loss 1.0868550539016724\n",
      "Iteration 4372: loss 0.9072260856628418\n",
      "Iteration 4373: loss 0.718708872795105\n",
      "Iteration 4374: loss 1.1722261905670166\n",
      "Iteration 4375: loss 2.0246880054473877\n",
      "Iteration 4376: loss 1.6286572217941284\n",
      "Iteration 4377: loss 1.2983078956604004\n",
      "Iteration 4378: loss 1.4712258577346802\n",
      "Iteration 4379: loss 0.9951955080032349\n",
      "Iteration 4380: loss 1.0494384765625\n",
      "Iteration 4381: loss 0.7500613331794739\n",
      "Iteration 4382: loss 0.7915314435958862\n",
      "Iteration 4383: loss 1.0130577087402344\n",
      "Iteration 4384: loss 1.0819332599639893\n",
      "Iteration 4385: loss 0.5384694337844849\n",
      "Iteration 4386: loss 1.0322636365890503\n",
      "Iteration 4387: loss 0.8309447765350342\n",
      "Iteration 4388: loss 1.1168975830078125\n",
      "Iteration 4389: loss 0.9128347635269165\n",
      "Iteration 4390: loss 1.1346336603164673\n",
      "Iteration 4391: loss 0.9256564378738403\n",
      "Iteration 4392: loss 1.1391783952713013\n",
      "Iteration 4393: loss 0.8070473074913025\n",
      "Iteration 4394: loss 1.027023434638977\n",
      "Iteration 4395: loss 0.48640376329421997\n",
      "Iteration 4396: loss 1.1017286777496338\n",
      "Iteration 4397: loss 1.0756208896636963\n",
      "Iteration 4398: loss 1.1426984071731567\n",
      "Iteration 4399: loss 1.0508251190185547\n",
      "Iteration 4400: loss 0.6950684785842896\n",
      "Iteration 4401: loss 0.8117260336875916\n",
      "Iteration 4402: loss 1.3154748678207397\n",
      "Iteration 4403: loss 0.8786036372184753\n",
      "Iteration 4404: loss 0.6463204026222229\n",
      "Iteration 4405: loss 0.7498763799667358\n",
      "Iteration 4406: loss 0.8110502362251282\n",
      "Iteration 4407: loss 0.873790442943573\n",
      "Iteration 4408: loss 1.0213443040847778\n",
      "Iteration 4409: loss 0.668015718460083\n",
      "Iteration 4410: loss 1.0795767307281494\n",
      "Iteration 4411: loss 0.8947773575782776\n",
      "Iteration 4412: loss 0.8530776500701904\n",
      "Iteration 4413: loss 0.947191059589386\n",
      "Iteration 4414: loss 0.6556761264801025\n",
      "Iteration 4415: loss 1.0969572067260742\n",
      "Iteration 4416: loss 0.8414826989173889\n",
      "Iteration 4417: loss 1.0666167736053467\n",
      "Iteration 4418: loss 0.7614016532897949\n",
      "Iteration 4419: loss 1.0065809488296509\n",
      "Iteration 4420: loss 1.127888560295105\n",
      "Iteration 4421: loss 0.8232935070991516\n",
      "Iteration 4422: loss 1.1596908569335938\n",
      "Iteration 4423: loss 0.7400497198104858\n",
      "Iteration 4424: loss 0.9321402311325073\n",
      "Iteration 4425: loss 0.9216115474700928\n",
      "Iteration 4426: loss 1.194089651107788\n",
      "Iteration 4427: loss 0.9567357897758484\n",
      "Iteration 4428: loss 0.8580134510993958\n",
      "Iteration 4429: loss 0.7461125254631042\n",
      "Iteration 4430: loss 1.0267555713653564\n",
      "Iteration 4431: loss 0.8364461064338684\n",
      "Iteration 4432: loss 0.7745361924171448\n",
      "Iteration 4433: loss 0.6715014576911926\n",
      "Iteration 4434: loss 1.1633707284927368\n",
      "Iteration 4435: loss 1.2502162456512451\n",
      "Iteration 4436: loss 0.9870539903640747\n",
      "Iteration 4437: loss 1.1464629173278809\n",
      "Iteration 4438: loss 2.0536386966705322\n",
      "Iteration 4439: loss 1.8524879217147827\n",
      "Iteration 4440: loss 1.0644022226333618\n",
      "Iteration 4441: loss 2.0956437587738037\n",
      "Iteration 4442: loss 1.592018961906433\n",
      "Iteration 4443: loss 1.7332977056503296\n",
      "Iteration 4444: loss 1.40031099319458\n",
      "Iteration 4445: loss 0.9668343663215637\n",
      "Iteration 4446: loss 1.8029049634933472\n",
      "Iteration 4447: loss 0.5399837493896484\n",
      "Iteration 4448: loss 2.1527929306030273\n",
      "Iteration 4449: loss 1.1734133958816528\n",
      "Iteration 4450: loss 1.1619327068328857\n",
      "Iteration 4451: loss 1.6732711791992188\n",
      "Iteration 4452: loss 1.1495985984802246\n",
      "Iteration 4453: loss 1.4500137567520142\n",
      "Iteration 4454: loss 0.9210261106491089\n",
      "Iteration 4455: loss 1.6489280462265015\n",
      "Iteration 4456: loss 1.0785319805145264\n",
      "Iteration 4457: loss 1.176456332206726\n",
      "Iteration 4458: loss 0.7534671425819397\n",
      "Iteration 4459: loss 1.018173336982727\n",
      "Iteration 4460: loss 1.0204499959945679\n",
      "Iteration 4461: loss 1.0415860414505005\n",
      "Iteration 4462: loss 0.883673369884491\n",
      "Iteration 4463: loss 1.233115315437317\n",
      "Iteration 4464: loss 2.259242296218872\n",
      "Iteration 4465: loss 2.4821629524230957\n",
      "Iteration 4466: loss 1.41766357421875\n",
      "Iteration 4467: loss 1.2639938592910767\n",
      "Iteration 4468: loss 1.375052809715271\n",
      "Iteration 4469: loss 1.4477558135986328\n",
      "Iteration 4470: loss 1.8650951385498047\n",
      "Iteration 4471: loss 1.2897512912750244\n",
      "Iteration 4472: loss 1.2517650127410889\n",
      "Iteration 4473: loss 1.5209259986877441\n",
      "Iteration 4474: loss 1.1078529357910156\n",
      "Iteration 4475: loss 1.648685097694397\n",
      "Iteration 4476: loss 1.2695155143737793\n",
      "Iteration 4477: loss 1.375410795211792\n",
      "Iteration 4478: loss 1.334190845489502\n",
      "Iteration 4479: loss 1.3302475214004517\n",
      "Iteration 4480: loss 1.7238671779632568\n",
      "Iteration 4481: loss 1.4490489959716797\n",
      "Iteration 4482: loss 1.4105218648910522\n",
      "Iteration 4483: loss 1.5127853155136108\n",
      "Iteration 4484: loss 1.7324117422103882\n",
      "Iteration 4485: loss 1.2917580604553223\n",
      "Iteration 4486: loss 1.701724648475647\n",
      "Iteration 4487: loss 1.0103384256362915\n",
      "Iteration 4488: loss 1.2417125701904297\n",
      "Iteration 4489: loss 1.4736801385879517\n",
      "Iteration 4490: loss 1.5172042846679688\n",
      "Iteration 4491: loss 0.9626430869102478\n",
      "Iteration 4492: loss 1.2738842964172363\n",
      "Iteration 4493: loss 1.1394370794296265\n",
      "Iteration 4494: loss 1.1662003993988037\n",
      "Iteration 4495: loss 1.4145236015319824\n",
      "Iteration 4496: loss 0.9958959817886353\n",
      "Iteration 4497: loss 1.07111656665802\n",
      "Iteration 4498: loss 1.2981339693069458\n",
      "Iteration 4499: loss 1.4600698947906494\n",
      "Iteration 4500: loss 1.5552507638931274\n",
      "Iteration 4501: loss 1.643912672996521\n",
      "Iteration 4502: loss 1.333528757095337\n",
      "Iteration 4503: loss 1.7031402587890625\n",
      "Iteration 4504: loss 1.3501676321029663\n",
      "Iteration 4505: loss 1.1627483367919922\n",
      "Iteration 4506: loss 1.0202922821044922\n",
      "Iteration 4507: loss 1.0450807809829712\n",
      "Iteration 4508: loss 1.0907232761383057\n",
      "Iteration 4509: loss 1.2114124298095703\n",
      "Iteration 4510: loss 1.0243597030639648\n",
      "Iteration 4511: loss 1.051649570465088\n",
      "Iteration 4512: loss 0.8898643255233765\n",
      "Iteration 4513: loss 0.9151647686958313\n",
      "Iteration 4514: loss 1.0553748607635498\n",
      "Iteration 4515: loss 0.6600410342216492\n",
      "Iteration 4516: loss 0.9486223459243774\n",
      "Iteration 4517: loss 0.8459569215774536\n",
      "Iteration 4518: loss 1.0694291591644287\n",
      "Iteration 4519: loss 0.9476559162139893\n",
      "Iteration 4520: loss 0.9386318922042847\n",
      "Iteration 4521: loss 0.7322524785995483\n",
      "Iteration 4522: loss 0.8565019965171814\n",
      "Iteration 4523: loss 0.7587631344795227\n",
      "Iteration 4524: loss 1.1304644346237183\n",
      "Iteration 4525: loss 1.1225249767303467\n",
      "Iteration 4526: loss 0.8416087627410889\n",
      "Iteration 4527: loss 0.6681451201438904\n",
      "Iteration 4528: loss 0.9614190459251404\n",
      "Iteration 4529: loss 0.7760053873062134\n",
      "Iteration 4530: loss 0.7140488028526306\n",
      "Iteration 4531: loss 0.8014222383499146\n",
      "Iteration 4532: loss 1.0515985488891602\n",
      "Iteration 4533: loss 0.8598693609237671\n",
      "Iteration 4534: loss 0.666589617729187\n",
      "Iteration 4535: loss 1.2224622964859009\n",
      "Iteration 4536: loss 0.9331181049346924\n",
      "Iteration 4537: loss 0.47133123874664307\n",
      "Iteration 4538: loss 0.7961477637290955\n",
      "Iteration 4539: loss 0.9853430986404419\n",
      "Iteration 4540: loss 0.4367058575153351\n",
      "Iteration 4541: loss 0.5267009735107422\n",
      "Iteration 4542: loss 0.9377173185348511\n",
      "Iteration 4543: loss 1.4215928316116333\n",
      "Iteration 4544: loss 1.3782719373703003\n",
      "Iteration 4545: loss 0.6158564686775208\n",
      "Iteration 4546: loss 1.1257760524749756\n",
      "Iteration 4547: loss 1.1879197359085083\n",
      "Iteration 4548: loss 0.3858090043067932\n",
      "Iteration 4549: loss 0.5578507781028748\n",
      "Iteration 4550: loss 0.9405052065849304\n",
      "Iteration 4551: loss 0.8112891316413879\n",
      "Iteration 4552: loss 0.6051781177520752\n",
      "Iteration 4553: loss 0.8417841196060181\n",
      "Iteration 4554: loss 0.7347988486289978\n",
      "Iteration 4555: loss 0.9445419907569885\n",
      "Iteration 4556: loss 0.7457866668701172\n",
      "Iteration 4557: loss 0.9318482279777527\n",
      "Iteration 4558: loss 0.9986816048622131\n",
      "Iteration 4559: loss 0.7049914598464966\n",
      "Iteration 4560: loss 0.6110013723373413\n",
      "Iteration 4561: loss 0.9606626629829407\n",
      "Iteration 4562: loss 0.952979564666748\n",
      "Iteration 4563: loss 0.9510752558708191\n",
      "Iteration 4564: loss 1.5499972105026245\n",
      "Iteration 4565: loss 2.081373453140259\n",
      "Iteration 4566: loss 1.9938546419143677\n",
      "Iteration 4567: loss 2.3352251052856445\n",
      "Iteration 4568: loss 2.5190234184265137\n",
      "Iteration 4569: loss 2.3130645751953125\n",
      "Iteration 4570: loss 2.7932116985321045\n",
      "Iteration 4571: loss 2.882537841796875\n",
      "Iteration 4572: loss 3.0330681800842285\n",
      "Iteration 4573: loss 2.9369146823883057\n",
      "Iteration 4574: loss 2.859327554702759\n",
      "Iteration 4575: loss 2.997532844543457\n",
      "Iteration 4576: loss 2.974027633666992\n",
      "Iteration 4577: loss 2.8764684200286865\n",
      "Iteration 4578: loss 1.7692590951919556\n",
      "Iteration 4579: loss 2.559659719467163\n",
      "Iteration 4580: loss 1.6344667673110962\n",
      "Iteration 4581: loss 2.5727548599243164\n",
      "Iteration 4582: loss 1.8920977115631104\n",
      "Iteration 4583: loss 2.098146915435791\n",
      "Iteration 4584: loss 2.0089333057403564\n",
      "Iteration 4585: loss 2.145303249359131\n",
      "Iteration 4586: loss 1.9428378343582153\n",
      "Iteration 4587: loss 2.0471954345703125\n",
      "Iteration 4588: loss 1.5786272287368774\n",
      "Iteration 4589: loss 1.7988696098327637\n",
      "Iteration 4590: loss 2.1449639797210693\n",
      "Iteration 4591: loss 2.198000431060791\n",
      "Iteration 4592: loss 2.2637481689453125\n",
      "Iteration 4593: loss 2.136235475540161\n",
      "Iteration 4594: loss 1.0248228311538696\n",
      "Iteration 4595: loss 1.0877516269683838\n",
      "Iteration 4596: loss 1.5171688795089722\n",
      "Iteration 4597: loss 1.816227674484253\n",
      "Iteration 4598: loss 1.4678698778152466\n",
      "Iteration 4599: loss 1.4574111700057983\n",
      "Iteration 4600: loss 1.1825788021087646\n",
      "Iteration 4601: loss 1.6351051330566406\n",
      "Iteration 4602: loss 1.3281447887420654\n",
      "Iteration 4603: loss 1.1754231452941895\n",
      "Iteration 4604: loss 1.1807734966278076\n",
      "Iteration 4605: loss 1.137848973274231\n",
      "Iteration 4606: loss 1.3985286951065063\n",
      "Iteration 4607: loss 1.2979785203933716\n",
      "Iteration 4608: loss 1.2734707593917847\n",
      "Iteration 4609: loss 1.231274962425232\n",
      "Iteration 4610: loss 1.2596709728240967\n",
      "Iteration 4611: loss 1.3825109004974365\n",
      "Iteration 4612: loss 1.329404592514038\n",
      "Iteration 4613: loss 1.5427498817443848\n",
      "Iteration 4614: loss 1.1800528764724731\n",
      "Iteration 4615: loss 1.1162067651748657\n",
      "Iteration 4616: loss 1.0963791608810425\n",
      "Iteration 4617: loss 1.0471136569976807\n",
      "Iteration 4618: loss 1.1997489929199219\n",
      "Iteration 4619: loss 1.2983123064041138\n",
      "Iteration 4620: loss 0.7060766220092773\n",
      "Iteration 4621: loss 1.4609748125076294\n",
      "Iteration 4622: loss 0.9160884022712708\n",
      "Iteration 4623: loss 1.6202443838119507\n",
      "Iteration 4624: loss 1.0838305950164795\n",
      "Iteration 4625: loss 0.8862842917442322\n",
      "Iteration 4626: loss 1.3070465326309204\n",
      "Iteration 4627: loss 1.0532081127166748\n",
      "Iteration 4628: loss 0.8533989787101746\n",
      "Iteration 4629: loss 1.4435055255889893\n",
      "Iteration 4630: loss 0.8697611689567566\n",
      "Iteration 4631: loss 1.3612643480300903\n",
      "Iteration 4632: loss 0.6354642510414124\n",
      "Iteration 4633: loss 1.2720565795898438\n",
      "Iteration 4634: loss 1.3153318166732788\n",
      "Iteration 4635: loss 1.0216164588928223\n",
      "Iteration 4636: loss 1.1159210205078125\n",
      "Iteration 4637: loss 1.294153094291687\n",
      "Iteration 4638: loss 1.5867666006088257\n",
      "Iteration 4639: loss 1.7913341522216797\n",
      "Iteration 4640: loss 1.5210295915603638\n",
      "Iteration 4641: loss 1.2968169450759888\n",
      "Iteration 4642: loss 1.8924163579940796\n",
      "Iteration 4643: loss 1.3423690795898438\n",
      "Iteration 4644: loss 1.3838658332824707\n",
      "Iteration 4645: loss 1.6597025394439697\n",
      "Iteration 4646: loss 1.5150216817855835\n",
      "Iteration 4647: loss 1.1839094161987305\n",
      "Iteration 4648: loss 1.3687849044799805\n",
      "Iteration 4649: loss 1.1911171674728394\n",
      "Iteration 4650: loss 1.6157125234603882\n",
      "Iteration 4651: loss 2.2040605545043945\n",
      "Iteration 4652: loss 1.7694106101989746\n",
      "Iteration 4653: loss 2.0200870037078857\n",
      "Iteration 4654: loss 2.0506672859191895\n",
      "Iteration 4655: loss 1.0929913520812988\n",
      "Iteration 4656: loss 1.793616771697998\n",
      "Iteration 4657: loss 1.2920260429382324\n",
      "Iteration 4658: loss 1.5449035167694092\n",
      "Iteration 4659: loss 1.0815094709396362\n",
      "Iteration 4660: loss 0.9361853003501892\n",
      "Iteration 4661: loss 1.810597538948059\n",
      "Iteration 4662: loss 2.227545738220215\n",
      "Iteration 4663: loss 2.0990207195281982\n",
      "Iteration 4664: loss 2.2857348918914795\n",
      "Iteration 4665: loss 2.4598922729492188\n",
      "Iteration 4666: loss 3.03251576423645\n",
      "Iteration 4667: loss 1.5678337812423706\n",
      "Iteration 4668: loss 2.5543010234832764\n",
      "Iteration 4669: loss 2.2759768962860107\n",
      "Iteration 4670: loss 1.7058711051940918\n",
      "Iteration 4671: loss 2.772059917449951\n",
      "Iteration 4672: loss 2.068622350692749\n",
      "Iteration 4673: loss 1.5294225215911865\n",
      "Iteration 4674: loss 1.9173504114151\n",
      "Iteration 4675: loss 1.7992304563522339\n",
      "Iteration 4676: loss 1.7388216257095337\n",
      "Iteration 4677: loss 1.8170151710510254\n",
      "Iteration 4678: loss 1.482580542564392\n",
      "Iteration 4679: loss 1.4230577945709229\n",
      "Iteration 4680: loss 1.6049847602844238\n",
      "Iteration 4681: loss 1.5827950239181519\n",
      "Iteration 4682: loss 1.634109616279602\n",
      "Iteration 4683: loss 1.56748628616333\n",
      "Iteration 4684: loss 1.60336172580719\n",
      "Iteration 4685: loss 1.8316657543182373\n",
      "Iteration 4686: loss 1.2556376457214355\n",
      "Iteration 4687: loss 1.4242624044418335\n",
      "Iteration 4688: loss 1.6824449300765991\n",
      "Iteration 4689: loss 1.8255947828292847\n",
      "Iteration 4690: loss 1.1816250085830688\n",
      "Iteration 4691: loss 1.5403294563293457\n",
      "Iteration 4692: loss 1.1269387006759644\n",
      "Iteration 4693: loss 1.4712574481964111\n",
      "Iteration 4694: loss 1.156265139579773\n",
      "Iteration 4695: loss 1.932056188583374\n",
      "Iteration 4696: loss 1.462851881980896\n",
      "Iteration 4697: loss 1.6273144483566284\n",
      "Iteration 4698: loss 1.6425853967666626\n",
      "Iteration 4699: loss 2.017939329147339\n",
      "Iteration 4700: loss 1.9027913808822632\n",
      "Iteration 4701: loss 1.511465072631836\n",
      "Iteration 4702: loss 1.18362557888031\n",
      "Iteration 4703: loss 1.356589674949646\n",
      "Iteration 4704: loss 1.217496395111084\n",
      "Iteration 4705: loss 1.4676213264465332\n",
      "Iteration 4706: loss 1.600874900817871\n",
      "Iteration 4707: loss 1.723393201828003\n",
      "Iteration 4708: loss 1.7209185361862183\n",
      "Iteration 4709: loss 1.5623397827148438\n",
      "Iteration 4710: loss 1.8682605028152466\n",
      "Iteration 4711: loss 1.7139229774475098\n",
      "Iteration 4712: loss 1.60125732421875\n",
      "Iteration 4713: loss 1.5600157976150513\n",
      "Iteration 4714: loss 1.7065093517303467\n",
      "Iteration 4715: loss 1.468313217163086\n",
      "Iteration 4716: loss 1.4555459022521973\n",
      "Iteration 4717: loss 1.8387850522994995\n",
      "Iteration 4718: loss 1.6569769382476807\n",
      "Iteration 4719: loss 1.1731215715408325\n",
      "Iteration 4720: loss 1.4437469244003296\n",
      "Iteration 4721: loss 1.4324947595596313\n",
      "Iteration 4722: loss 1.4299263954162598\n",
      "Iteration 4723: loss 1.0831795930862427\n",
      "Iteration 4724: loss 1.198569893836975\n",
      "Iteration 4725: loss 1.506181001663208\n",
      "Iteration 4726: loss 1.0611531734466553\n",
      "Iteration 4727: loss 1.2207965850830078\n",
      "Iteration 4728: loss 1.19495689868927\n",
      "Iteration 4729: loss 1.1024302244186401\n",
      "Iteration 4730: loss 1.4448167085647583\n",
      "Iteration 4731: loss 1.1536717414855957\n",
      "Iteration 4732: loss 0.8212502598762512\n",
      "Iteration 4733: loss 1.1419364213943481\n",
      "Iteration 4734: loss 1.131104826927185\n",
      "Iteration 4735: loss 1.1757338047027588\n",
      "Iteration 4736: loss 1.349440336227417\n",
      "Iteration 4737: loss 1.6083998680114746\n",
      "Iteration 4738: loss 1.946338176727295\n",
      "Iteration 4739: loss 1.562967300415039\n",
      "Iteration 4740: loss 0.6911790370941162\n",
      "Iteration 4741: loss 1.3304227590560913\n",
      "Iteration 4742: loss 0.9138137102127075\n",
      "Iteration 4743: loss 1.2113409042358398\n",
      "Iteration 4744: loss 1.2919018268585205\n",
      "Iteration 4745: loss 0.915301501750946\n",
      "Iteration 4746: loss 1.5661413669586182\n",
      "Iteration 4747: loss 1.1040977239608765\n",
      "Iteration 4748: loss 1.6382516622543335\n",
      "Iteration 4749: loss 0.9163140654563904\n",
      "Iteration 4750: loss 1.1691813468933105\n",
      "Iteration 4751: loss 1.3373208045959473\n",
      "Iteration 4752: loss 1.2558495998382568\n",
      "Iteration 4753: loss 1.107189416885376\n",
      "Iteration 4754: loss 0.9989635944366455\n",
      "Iteration 4755: loss 1.2316572666168213\n",
      "Iteration 4756: loss 1.351083755493164\n",
      "Iteration 4757: loss 1.2105191946029663\n",
      "Iteration 4758: loss 0.9474536180496216\n",
      "Iteration 4759: loss 0.9164177775382996\n",
      "Iteration 4760: loss 1.3929791450500488\n",
      "Iteration 4761: loss 0.9187591075897217\n",
      "Iteration 4762: loss 1.3320367336273193\n",
      "Iteration 4763: loss 0.968500554561615\n",
      "Iteration 4764: loss 1.8917917013168335\n",
      "Iteration 4765: loss 1.2954506874084473\n",
      "Iteration 4766: loss 1.365189790725708\n",
      "Iteration 4767: loss 1.3085507154464722\n",
      "Iteration 4768: loss 1.1010905504226685\n",
      "Iteration 4769: loss 1.1564680337905884\n",
      "Iteration 4770: loss 1.090425729751587\n",
      "Iteration 4771: loss 0.946817934513092\n",
      "Iteration 4772: loss 0.7739362716674805\n",
      "Iteration 4773: loss 0.843522310256958\n",
      "Iteration 4774: loss 0.7989120483398438\n",
      "Iteration 4775: loss 1.00093412399292\n",
      "Iteration 4776: loss 0.7936601638793945\n",
      "Iteration 4777: loss 0.7852733731269836\n",
      "Iteration 4778: loss 1.260871410369873\n",
      "Iteration 4779: loss 1.271092176437378\n",
      "Iteration 4780: loss 1.1755092144012451\n",
      "Iteration 4781: loss 0.5948814153671265\n",
      "Iteration 4782: loss 0.782113790512085\n",
      "Iteration 4783: loss 1.1371262073516846\n",
      "Iteration 4784: loss 0.7516656517982483\n",
      "Iteration 4785: loss 0.7335501909255981\n",
      "Iteration 4786: loss 0.9833608269691467\n",
      "Iteration 4787: loss 1.120942234992981\n",
      "Iteration 4788: loss 0.7397674322128296\n",
      "Iteration 4789: loss 1.3225195407867432\n",
      "Iteration 4790: loss 0.9637026786804199\n",
      "Iteration 4791: loss 0.9072616100311279\n",
      "Iteration 4792: loss 0.9089179039001465\n",
      "Iteration 4793: loss 0.9975014925003052\n",
      "Iteration 4794: loss 0.8516004681587219\n",
      "Iteration 4795: loss 1.249335765838623\n",
      "Iteration 4796: loss 0.6938136219978333\n",
      "Iteration 4797: loss 0.84122633934021\n",
      "Iteration 4798: loss 0.8367640376091003\n",
      "Iteration 4799: loss 0.6768071055412292\n",
      "Iteration 4800: loss 0.5720977187156677\n",
      "Iteration 4801: loss 0.642626166343689\n",
      "Iteration 4802: loss 0.9506827592849731\n",
      "Iteration 4803: loss 0.9153671264648438\n",
      "Iteration 4804: loss 0.659626305103302\n",
      "Iteration 4805: loss 0.9919775724411011\n",
      "Iteration 4806: loss 0.5308934450149536\n",
      "Iteration 4807: loss 0.9972732067108154\n",
      "Iteration 4808: loss 0.7786766886711121\n",
      "Iteration 4809: loss 0.558234453201294\n",
      "Iteration 4810: loss 0.7462818622589111\n",
      "Iteration 4811: loss 0.640005350112915\n",
      "Iteration 4812: loss 1.0218018293380737\n",
      "Iteration 4813: loss 0.6969032883644104\n",
      "Iteration 4814: loss 0.7620218992233276\n",
      "Iteration 4815: loss 0.8984960913658142\n",
      "Iteration 4816: loss 0.581547737121582\n",
      "Iteration 4817: loss 0.6251166462898254\n",
      "Iteration 4818: loss 0.586018979549408\n",
      "Iteration 4819: loss 0.8036732077598572\n",
      "Iteration 4820: loss 0.6322607398033142\n",
      "Iteration 4821: loss 0.6717094779014587\n",
      "Iteration 4822: loss 0.6097829341888428\n",
      "Iteration 4823: loss 0.9487121105194092\n",
      "Iteration 4824: loss 0.5179749727249146\n",
      "Iteration 4825: loss 0.6033607125282288\n",
      "Iteration 4826: loss 0.8378562927246094\n",
      "Iteration 4827: loss 0.6884969472885132\n",
      "Iteration 4828: loss 0.683327317237854\n",
      "Iteration 4829: loss 0.3922838568687439\n",
      "Iteration 4830: loss 0.7026819586753845\n",
      "Iteration 4831: loss 0.7639620304107666\n",
      "Iteration 4832: loss 0.7087731957435608\n",
      "Iteration 4833: loss 0.8715713024139404\n",
      "Iteration 4834: loss 0.7937136888504028\n",
      "Iteration 4835: loss 0.5706250071525574\n",
      "Iteration 4836: loss 0.7852490544319153\n",
      "Iteration 4837: loss 0.611594557762146\n",
      "Iteration 4838: loss 0.801207959651947\n",
      "Iteration 4839: loss 1.2503726482391357\n",
      "Iteration 4840: loss 0.665979266166687\n",
      "Iteration 4841: loss 0.6567257642745972\n",
      "Iteration 4842: loss 1.3765331506729126\n",
      "Iteration 4843: loss 1.0304137468338013\n",
      "Iteration 4844: loss 1.1784882545471191\n",
      "Iteration 4845: loss 1.097875952720642\n",
      "Iteration 4846: loss 0.6907435655593872\n",
      "Iteration 4847: loss 0.8504002094268799\n",
      "Iteration 4848: loss 0.9503510594367981\n",
      "Iteration 4849: loss 1.29172682762146\n",
      "Iteration 4850: loss 0.708909809589386\n",
      "Iteration 4851: loss 1.2491737604141235\n",
      "Iteration 4852: loss 1.1839849948883057\n",
      "Iteration 4853: loss 1.3209141492843628\n",
      "Iteration 4854: loss 0.7970947623252869\n",
      "Iteration 4855: loss 0.9905132055282593\n",
      "Iteration 4856: loss 0.743659496307373\n",
      "Iteration 4857: loss 1.5883145332336426\n",
      "Iteration 4858: loss 1.075926423072815\n",
      "Iteration 4859: loss 0.904487669467926\n",
      "Iteration 4860: loss 0.6093543767929077\n",
      "Iteration 4861: loss 0.8079162240028381\n",
      "Iteration 4862: loss 0.891616702079773\n",
      "Iteration 4863: loss 0.6931438446044922\n",
      "Iteration 4864: loss 0.8097327947616577\n",
      "Iteration 4865: loss 0.42949771881103516\n",
      "Iteration 4866: loss 0.7657569050788879\n",
      "Iteration 4867: loss 0.7990147471427917\n",
      "Iteration 4868: loss 0.996123731136322\n",
      "Iteration 4869: loss 0.7882121801376343\n",
      "Iteration 4870: loss 0.6862136721611023\n",
      "Iteration 4871: loss 0.9805192947387695\n",
      "Iteration 4872: loss 0.9984614849090576\n",
      "Iteration 4873: loss 0.7387387156486511\n",
      "Iteration 4874: loss 1.1074968576431274\n",
      "Iteration 4875: loss 1.3329647779464722\n",
      "Iteration 4876: loss 1.450888991355896\n",
      "Iteration 4877: loss 2.7024407386779785\n",
      "Iteration 4878: loss 2.661203145980835\n",
      "Iteration 4879: loss 3.553760528564453\n",
      "Iteration 4880: loss 3.6758978366851807\n",
      "Iteration 4881: loss 4.209136486053467\n",
      "Iteration 4882: loss 3.4271011352539062\n",
      "Iteration 4883: loss 2.292921304702759\n",
      "Iteration 4884: loss 2.0208740234375\n",
      "Iteration 4885: loss 1.5545116662979126\n",
      "Iteration 4886: loss 2.011554718017578\n",
      "Iteration 4887: loss 1.1395572423934937\n",
      "Iteration 4888: loss 1.3771684169769287\n",
      "Iteration 4889: loss 1.5106099843978882\n",
      "Iteration 4890: loss 1.4539436101913452\n",
      "Iteration 4891: loss 1.6381628513336182\n",
      "Iteration 4892: loss 1.1307677030563354\n",
      "Iteration 4893: loss 1.4566198587417603\n",
      "Iteration 4894: loss 1.1305381059646606\n",
      "Iteration 4895: loss 1.3565956354141235\n",
      "Iteration 4896: loss 1.0681836605072021\n",
      "Iteration 4897: loss 1.1564632654190063\n",
      "Iteration 4898: loss 0.8013435006141663\n",
      "Iteration 4899: loss 1.3444091081619263\n",
      "Iteration 4900: loss 1.132164716720581\n",
      "Iteration 4901: loss 1.0272051095962524\n",
      "Iteration 4902: loss 1.0328428745269775\n",
      "Iteration 4903: loss 1.168949842453003\n",
      "Iteration 4904: loss 1.3240673542022705\n",
      "Iteration 4905: loss 1.5124634504318237\n",
      "Iteration 4906: loss 1.5217324495315552\n",
      "Iteration 4907: loss 1.1954400539398193\n",
      "Iteration 4908: loss 0.8151450753211975\n",
      "Iteration 4909: loss 0.9327837824821472\n",
      "Iteration 4910: loss 1.0670549869537354\n",
      "Iteration 4911: loss 1.3083680868148804\n",
      "Iteration 4912: loss 1.0163358449935913\n",
      "Iteration 4913: loss 1.2255065441131592\n",
      "Iteration 4914: loss 1.146221399307251\n",
      "Iteration 4915: loss 1.1727620363235474\n",
      "Iteration 4916: loss 1.2219502925872803\n",
      "Iteration 4917: loss 0.9073835015296936\n",
      "Iteration 4918: loss 1.338972806930542\n",
      "Iteration 4919: loss 0.8559329509735107\n",
      "Iteration 4920: loss 0.9137181043624878\n",
      "Iteration 4921: loss 1.2019855976104736\n",
      "Iteration 4922: loss 1.1101527214050293\n",
      "Iteration 4923: loss 0.9233538508415222\n",
      "Iteration 4924: loss 1.1114898920059204\n",
      "Iteration 4925: loss 0.9489142894744873\n",
      "Iteration 4926: loss 0.913127601146698\n",
      "Iteration 4927: loss 1.093080997467041\n",
      "Iteration 4928: loss 1.0918872356414795\n",
      "Iteration 4929: loss 1.2380034923553467\n",
      "Iteration 4930: loss 1.005731463432312\n",
      "Iteration 4931: loss 1.0711520910263062\n",
      "Iteration 4932: loss 0.8765406608581543\n",
      "Iteration 4933: loss 0.9968229532241821\n",
      "Iteration 4934: loss 0.930461049079895\n",
      "Iteration 4935: loss 1.014461874961853\n",
      "Iteration 4936: loss 0.6880995631217957\n",
      "Iteration 4937: loss 0.6629067659378052\n",
      "Iteration 4938: loss 0.783105731010437\n",
      "Iteration 4939: loss 1.078091025352478\n",
      "Iteration 4940: loss 1.0941424369812012\n",
      "Iteration 4941: loss 1.2430000305175781\n",
      "Iteration 4942: loss 0.9873046875\n",
      "Iteration 4943: loss 0.6380350589752197\n",
      "Iteration 4944: loss 1.389796257019043\n",
      "Iteration 4945: loss 0.9073571562767029\n",
      "Iteration 4946: loss 0.8222211003303528\n",
      "Iteration 4947: loss 0.31929904222488403\n",
      "Iteration 4948: loss 1.0591269731521606\n",
      "Iteration 4949: loss 1.0289503335952759\n",
      "Iteration 4950: loss 1.1106868982315063\n",
      "Iteration 4951: loss 0.9193739295005798\n",
      "Iteration 4952: loss 0.7700207233428955\n",
      "Iteration 4953: loss 0.7913996577262878\n",
      "Iteration 4954: loss 0.36027923226356506\n",
      "Iteration 4955: loss 0.975020170211792\n",
      "Iteration 4956: loss 0.688928484916687\n",
      "Iteration 4957: loss 0.5922061204910278\n",
      "Iteration 4958: loss 0.9301086068153381\n",
      "Iteration 4959: loss 1.3369756937026978\n",
      "Iteration 4960: loss 0.888080894947052\n",
      "Iteration 4961: loss 0.8187922239303589\n",
      "Iteration 4962: loss 0.7679308652877808\n",
      "Iteration 4963: loss 0.7321895956993103\n",
      "Iteration 4964: loss 0.7218031883239746\n",
      "Iteration 4965: loss 0.8386918306350708\n",
      "Iteration 4966: loss 0.6930637955665588\n",
      "Iteration 4967: loss 0.8006399273872375\n",
      "Iteration 4968: loss 0.531431257724762\n",
      "Iteration 4969: loss 0.9669795632362366\n",
      "Iteration 4970: loss 0.6202524900436401\n",
      "Iteration 4971: loss 0.7228928804397583\n",
      "Iteration 4972: loss 0.42037397623062134\n",
      "Iteration 4973: loss 0.5627152919769287\n",
      "Iteration 4974: loss 0.9420836567878723\n",
      "Iteration 4975: loss 0.5216131210327148\n",
      "Iteration 4976: loss 0.7204307317733765\n",
      "Iteration 4977: loss 0.5184178948402405\n",
      "Iteration 4978: loss 0.9108274579048157\n",
      "Iteration 4979: loss 1.0078368186950684\n",
      "Iteration 4980: loss 0.7632424235343933\n",
      "Iteration 4981: loss 1.1853597164154053\n",
      "Iteration 4982: loss 0.7736361026763916\n",
      "Iteration 4983: loss 1.4020953178405762\n",
      "Iteration 4984: loss 1.1621880531311035\n",
      "Iteration 4985: loss 1.4481407403945923\n",
      "Iteration 4986: loss 1.3578232526779175\n",
      "Iteration 4987: loss 1.2583844661712646\n",
      "Iteration 4988: loss 1.7039992809295654\n",
      "Iteration 4989: loss 1.2714927196502686\n",
      "Iteration 4990: loss 0.7279630899429321\n",
      "Iteration 4991: loss 0.663462221622467\n",
      "Iteration 4992: loss 1.276617407798767\n",
      "Iteration 4993: loss 0.9375218749046326\n",
      "Iteration 4994: loss 0.9634220600128174\n",
      "Iteration 4995: loss 1.4244414567947388\n",
      "Iteration 4996: loss 1.2756304740905762\n",
      "Iteration 4997: loss 1.4509503841400146\n",
      "Iteration 4998: loss 0.6756181120872498\n",
      "Iteration 4999: loss 1.086068034172058\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "cached_fn = f\"{dataset}.nf\"\n",
    "\n",
    "if os.path.exists(cached_fn):\n",
    "    with open(cached_fn, \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "    encoder.to(device)\n",
    "else:\n",
    "    for j in range(5000):\n",
    "        theta, x = generate_data(mb_size, return_theta=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -1*encoder.log_prob(theta.to(device), x.to(device)).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Iteration {}: loss {}'.format(j, loss.item()))\n",
    "    \n",
    "    with open(cached_fn, \"wb\") as f:\n",
    "        pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3cb41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cal_scores = []\n",
    "for calibration_theta_pt, calibration_x_pt in zip(calibration_theta, calibration_x):\n",
    "    log_prob = encoder.log_prob(calibration_theta_pt.view(1,-1).to(device), calibration_x_pt.view(1,-1).to(device)).detach()\n",
    "    prob = log_prob.cpu().exp().numpy()\n",
    "    cal_scores.append(1 / prob)\n",
    "cal_scores = np.array(cal_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e767721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the true posterior density\n",
    "def plot(j, x, theta, encoder, **kwargs):\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    # Plot exact density\n",
    "    nrows = 4\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(24,24))\n",
    "\n",
    "    discretization = .1\n",
    "    vals = torch.arange(-5., 5., discretization)\n",
    "    eval_pts = torch.cartesian_prod(vals, vals)\n",
    "    lps = encoder.log_prob(eval_pts.to(device), x[j].view(1,-1).repeat(eval_pts.shape[0],1).to(device)).detach()\n",
    "    X, Y = torch.meshgrid(vals, vals)\n",
    "    Z = lps.view(X.shape)\n",
    "    ax[0,1].plot(theta[j][0], theta[j][1], marker=\"x\", color=\"r\")\n",
    "    ax[0,1].pcolormesh(X.cpu().numpy(), Y.cpu().numpy(), Z.cpu().exp().numpy())\n",
    "    ax[0,1].set_title('Approximate Posterior Flow')\n",
    "\n",
    "    remaining_spots = nrows * ncols - 2\n",
    "    for k in range(remaining_spots):\n",
    "        coverage_guarantee = 0.1 * (k + 1)\n",
    "        qhat = np.quantile(cal_scores, q = coverage_guarantee)\n",
    "        prob_min = 1 / qhat\n",
    "\n",
    "        graphic_idx = k + 2\n",
    "        row_idx = graphic_idx // ncols\n",
    "        col_idx = graphic_idx - row_idx * ncols\n",
    "\n",
    "        prediction_interval = (Z.cpu().exp().numpy() > prob_min).astype(\"bool\")\n",
    "        \n",
    "        # find corresponding indices of matrix for lookup: have to invert y convention, since down is positive in index\n",
    "        ax[row_idx,col_idx].plot(theta[j][0], theta[j][1], marker=\"x\", color=\"r\")\n",
    "        ax[row_idx,col_idx].pcolormesh(X.cpu().numpy(), Y.cpu().numpy(), prediction_interval)\n",
    "        ax[row_idx,col_idx].set_title(f'Conformalized Posterior: q={coverage_guarantee}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955455ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected features = 3, got 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot(j\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, x\u001b[39m=\u001b[39;49mtest_x, theta\u001b[39m=\u001b[39;49mtest_theta, encoder\u001b[39m=\u001b[39;49mencoder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(j, x, theta, encoder, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m-\u001b[39m\u001b[39m5.\u001b[39m, \u001b[39m5.\u001b[39m, discretization)\n\u001b[1;32m     12\u001b[0m eval_pts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcartesian_prod(vals, vals)\n\u001b[0;32m---> 13\u001b[0m lps \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mlog_prob(eval_pts\u001b[39m.\u001b[39;49mto(device), x[j]\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mrepeat(eval_pts\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device))\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     14\u001b[0m X, Y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmeshgrid(vals, vals)\n\u001b[1;32m     15\u001b[0m Z \u001b[39m=\u001b[39m lps\u001b[39m.\u001b[39mview(X\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/nflows/distributions/base.py:40\u001b[0m, in \u001b[0;36mDistribution.log_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m context\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     37\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNumber of input items must be equal to number of context items.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         )\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_prob(inputs, context)\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/nflows/flows/base.py:39\u001b[0m, in \u001b[0;36mFlow._log_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_log_prob\u001b[39m(\u001b[39mself\u001b[39m, inputs, context):\n\u001b[1;32m     38\u001b[0m     embedded_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_net(context)\n\u001b[0;32m---> 39\u001b[0m     noise, logabsdet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inputs, context\u001b[39m=\u001b[39;49membedded_context)\n\u001b[1;32m     40\u001b[0m     log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution\u001b[39m.\u001b[39mlog_prob(noise, context\u001b[39m=\u001b[39membedded_context)\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m log_prob \u001b[39m+\u001b[39m logabsdet\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/nflows/transforms/base.py:56\u001b[0m, in \u001b[0;36mCompositeTransform.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     funcs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transforms\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cascade(inputs, funcs, context)\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/nflows/transforms/base.py:50\u001b[0m, in \u001b[0;36mCompositeTransform._cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m total_logabsdet \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mnew_zeros(batch_size)\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m funcs:\n\u001b[0;32m---> 50\u001b[0m     outputs, logabsdet \u001b[39m=\u001b[39m func(outputs, context)\n\u001b[1;32m     51\u001b[0m     total_logabsdet \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m logabsdet\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m outputs, total_logabsdet\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chig/lib/python3.8/site-packages/nflows/transforms/coupling.py:76\u001b[0m, in \u001b[0;36mCouplingTransform.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInputs must be a 2D or a 4D tensor.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected features = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, inputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     80\u001b[0m identity_split \u001b[39m=\u001b[39m inputs[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midentity_features, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m     81\u001b[0m transform_split \u001b[39m=\u001b[39m inputs[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_features, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected features = 3, got 2."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x2400 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(j=20, x=test_x, theta=test_theta, encoder=encoder, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ff4c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_coverage(coverage_trials = 1000, num_coverage_pts = 20):\n",
    "    empirical_coverages = np.zeros(num_coverage_pts)\n",
    "    variational_coverages = np.zeros(num_coverage_pts)\n",
    "    desired_coverages = [(1 / num_coverage_pts) * (k + 1) for k in range(num_coverage_pts)]\n",
    "    quantiles = np.array([1 / np.quantile(cal_scores, q = coverage) for coverage in desired_coverages])\n",
    "    \n",
    "    for j in range(coverage_trials):\n",
    "        predicted_lps = encoder.log_prob(test_theta[j].view(1,-1).to(device), test_x[j].view(1,-1).to(device)).detach()\n",
    "        predicted_prob = predicted_lps.cpu().exp().numpy()\n",
    "        empirical_coverages += predicted_prob > quantiles\n",
    "\n",
    "        empirical_theta_dist = encoder.sample(10_000, test_x[j].view(1,-1).to(device)).cpu().detach().numpy()[0]\n",
    "        for k, desired_coverage in enumerate(desired_coverages):\n",
    "            alpha = (1 - desired_coverage) / 2\n",
    "            lower_quantile = np.quantile(empirical_theta_dist, q = alpha, axis=0)\n",
    "            upper_quantile = np.quantile(empirical_theta_dist, q = 1 - alpha, axis=0)\n",
    "            variational_coverages[k] += int(\n",
    "                np.all(lower_quantile <= test_theta[j].cpu().detach().numpy()) and \n",
    "                np.all(test_theta[j].cpu().detach().numpy() <= upper_quantile)\n",
    "            )\n",
    "\n",
    "    return empirical_coverages / coverage_trials, variational_coverages / coverage_trials, desired_coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efab282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yppatel/anaconda3/envs/chig/lib/python3.8/site-packages/nflows/transforms/lu.py:80: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2191.)\n",
      "  outputs, _ = torch.triangular_solve(\n"
     ]
    }
   ],
   "source": [
    "empirical_coverages, variational_coverages, desired_coverages = assess_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29ba4831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Desired vs. Empirical Coverage')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(desired_coverages, empirical_coverages, label=\"Empirical\")\n",
    "plt.plot(variational_coverages, empirical_coverages, label=\"Variational\")\n",
    "plt.plot(desired_coverages, desired_coverages, label=\"Desired\")\n",
    "plt.legend()\n",
    "plt.title(\"Desired vs. Empirical Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac521a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "025fdf49e09ee838b0c05e971129fbc14df70fae1b22b06a04398c8d66c2f675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
